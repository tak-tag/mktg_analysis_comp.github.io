[["index.html", "マーケティング・データ分析補足資料 本ウェブサイトについて", " マーケティング・データ分析補足資料 田頭拓己 神戸大学大学院経営学研究科 2025-11-06 本ウェブサイトについて 本ウェブサイトは、マーケティング・リサーチにて広く用いられる基礎的な分析手法について、R (R studio) を用いた実行方法を紹介することを目的としている。また、本ウェブサイトは以下の図書の補足サイトとして、作成している。 田頭拓己（2025）「マーケティングデータ分析: 実務とリサーチをつなぐ」，有斐閣. そのうえで、ここではとくに分析用のコードと、図書内で省略した一部の分析手法の紹介を行っている。なお、上記の図書では分析コードだけでなく、紹介した分析手法の理論的な説明や、マーケティング・経営学領域での研究の進め方についても紙幅を割いているため、それらの内容に関心がある場合にはぜひ図書を手にとってみてほしい。 "],["rusage.html", "R と R studioに慣れる ", " R と R studioに慣れる "],["ch2intro.html", "本章の概要", " 本章の概要 本章では、統計分析ソフトウェアR（あーる）と R Studio の概要および使い方について説明する。本書では特に、R Studio と呼ばれるアプリケーションを活用した分析を想定する。さらに本章ではR Studio で活用できるプロジェクト機能を活用することを強く推奨する。本章の内容を参考に、R Studioを用いたデータ分析を行うための実行環境を整えてほしい。 R は統計、データ解析、統計グラフ作成のためのオープンソースソフトウェアである。なお、Rを用いる際には、多くの場合) R studio、Jupyter notebookや、Rコマンダーのようなユーザーインターフェイスが利用される。そしてRを使用する際には (Rコマンダーを使わないかぎりは) 基本的にソースコードを入力し計算、分析を行う。Rを用いる際に最もよく使われる環境（アプリケーション）のひとつがR studioであり、本書においても基本的にはR studioを用いることを前提とするが、R studioをデスクトップにインストールし利用する場合には、Rそのものもインストールしておく必要があることに注意が必要である。R studioは現在、Positとも呼ばれており、以下のサイトからダウンロードが可能である（https://posit.co/download/rstudio-desktop/）1。このリンクにアクセスし、自身の環境に適したRStudio Desktop ソフトウェアの実行ファイルやディスクイメージファイルをダウンロードしてほしい。そしてファイルをダウンロードしたら、それを開くことで表示されるインストーラーの手順に従い、RStudioをダウンロードしてほしい。 また本書では、Posit Cloudという、アカウント登録を行うことでブラウザ上でR studioを利用できる環境も紹介する。R studio Desktop版 の利用においては、データの所在地（ディレクトリ）設定などによってエラーが生じることが多々あり、個別のPC環境に合わせて対応、設定を行う必要がある。そのため、まとまった人数に対応する必要がある講義で R studio を活用する場合や、R studio のインストールに苦労する初学者においてはまずPosit cloud と呼ばれるクラウド版を利用し、本書で紹介するような分析作業になれることを勧める。しかし、Posit cloud のフリーアカウントには、利用可能な時間やデータ容量に制限があるため、自身の研究や仕事等でデータ処理や分析を行う場合にはR studio デスクトップ版（通常はR studio IDEのフリーバージョンで十分）をインストールし、利用してほしい。 なお、Posit社のウェブサイトの構成は時期によって変化する可能性がある。もし、RStudio Desktopのダウンロードサイトが見当たらないときにはPosit ウェブサイト内で “Rstudio” と検索してほしい↩︎ "],["desktop.html", "R studio デスクトップ版の利用", " R studio デスクトップ版の利用 R studioは、Rを利用するためのアプリケーションである。R単体で使うよりも便利な機能が搭載されており、R studioを使うことでプログラミング作業を容易にすることが可能になる。最も大きな特徴としては、Rでの操作、分析を実行するための「コンソール画面」と、実行したい操作、分析のコードを記述しておく「Rスクリプト」と呼ばれるテキストファイルを一つの画面内に同時に表示できることである。そのため、Rに実行してほしいコマンドをテキストデータのように記述、修正し書き溜めておける一方で、その実行もスムーズに行え、結果も同画面内で確認することができる。 R studio はR をより便利に使うためのツールであるため、利用者はまず、R と R studioの両方をインストールする必要がある。Rは以下のサイト（https://ftp.yz.yamagata-u.ac.jp/pub/cran/）等からダウンロードし、インストールすることが可能である。当該ウェブサイトにアクセスすると、Windows用、Mac用、Linux用のソフトウェアが選択できるため、ユーザー自身の環境に適したバージョンを選択してほしい。Windowsユーザーの場合は、“install R for the first time” と書かれているリンクから自身の適した条件を選択し、実行ファイルをダウンロードしてほしい。Macユーザーの場合には、自身の使っているMacのバージョンに適したパッケージファイルを選択し、ダウンロードしてほしい。インストーラーやパッケージファイルをダウンロードした後はその後のソフトウェアインストール手順指示に従い、Rをインストールすること。 R studio のインストールは、以下のリンクから 行うことができる（https://posit.co/download/rstudio-desktop/）。なお下図は、MacOSを使ったデバイスの場合のインストール画面を表している。RStudioのインストールにあたっては、このリンクにアクセスし、自身の環境に適したRStudio Desktop ソフトウェアの実行ファイルやディスクイメージファイルをダウンロードしてほしい。なお、特別な事情がない限り、無料版で十分分析が可能である。そしてファイルをダウンロードしたら、それを開くことで表示されるインストーラーの手順に従い、RStudioをダウンロードしてほしい。 Rstudio Desktop インストール画面（Mac例） なお、WindowsでのRおよびR studioのインストールには注意が必要である。特に、Rを用いる講義を受け持っていると、Windowsユーザーを中心に新たなパッケージのインストールができないなどのトラブルが頻発する。これらの問題点を調べると、(1) 文字コードによる文字化けの問題、(2) ユーザーアカウントのホームディレクトリ名に日本語（全角）が利用されていること、(3) Rのライブラリが(勝手に) One drive 上に作成されることが原因であることが多かった。これに対して、 R の version 4.20以降からは、UTF-8の文字コードに対応したり、デフォルトでのRのインストール場所の変更（One drive上でない）が行われたりと、問題の改善が図られている。また、自身のホームディレクトリの名前が全角文字であるときは、ホームディレクトリ以外のローカルディレクトリを設定したほうが良い。この点に関する対応には、三重大学の奥村先生によって以下のウェブサイトに説明が記載されているので、詳しくはそちらを参照してほしい（https://okumuralab.org/~okumura/stat/R-win.html）。 "],["project.html", "プロジェクト機能について", " プロジェクト機能について R および R studio のインストールが完了したら、アプリケーションを起動する。 アプリケーションを起動すると、デフォルトでは以下のような R Studio環境画面が表示される。 新しいRstudio 画面 Rstudioは、上記の図のような画面構成をしている。Rstudioの画面を構成する主なウィンドウはペインと呼ばれ、(1) RスクリプトでRコードの入力・編集に用いる”Source”、(2) Rの命令を直接入力し結果も表示される”Console”がなどが主な要素としてある。また、その他利用しているデータ情報、パッケージ、履歴など様々なタブが存在する。Rstudioの初回起動時にはSourceのペインは収納されているため、 Rスクリプトファイルを作成する必要がある。Rstudioは基本的に4分割画面で表示され、各ペインの配置については、Tools \\(\\rightarrow\\) Global option \\(\\rightarrow\\) Pane Layoutより変更が可能になる。Rstudioを操作する上で、基本的に重要となる情報は、(1) Source、(2) Console、(3) データやプロットに関する環境情報の3点であるので、以下のような配置がおすすめである。 左上 or 下: Source 左下 or 上: History (ただし、さほど重要ではないので畳んだ状態にしておく) 右上 or 下: Console 右下 or 上: 複数タブをまとめ 配置の目的はあくまで、必要な情報を同一画面上に表示することであるため、自身のやりやすい配置を考えてアレンジしてほしい。 R studio を利用する際には、「プロジェクト」機能を使うことを勧める。プロジェクトは、互いに関連し合ったファイルの集まりを指す。Rを通じた分析では、たくさんのファイルを扱うことになる。例えば、複数のRスクリプトやデータセット、加工したデータセットの保存、分析結果、出力された図表などがある。これらのファイルを手作業で一括管理することは困難である。むしろそのような管理作業に認知的な負担を費やしたくないというのが分析者の本音である。プロジェクト機能を使うことにより、作業ディレクトリとファイルの保存先をひとまとまりに指定できるため、ファイル管理の手間がなくなる。 新しいプロジェクトを作成するシンプルな方法が、Fileから作成する方法である。具体的には、File -&gt; New Project -&gt; New Directory -&gt; Create New Project -&gt;Directory nameの指定 -&gt; プロジェクトの設置場所（ディレクト）の指定、という手順で作成する。 R をデスクトップ上で利用する際には、基本的には自身のPC内にあるデータの所在地（ディレクトリ）を特定することでデータの操作や分析を行う。これに対してプロジェクト機能を利用することでそのプロジェクトを実行している際に参照するワーキングディレクトリを固定することが可能になる。この機能によってR studioを通じたデータ処理や分析作業が容易になり、不要なトラブルを避けることが可能になるため、デスクトップでR studioを使う場合には可能な限りプロジェクト機能を利用してほしい。 "],["basics.html", "Rの基本操作", " Rの基本操作 ここでは、Rを使用する上での基本的な操作方法を紹介する。Rはコマンド（命令）をconsoleを通じて実行することで動かすことができる。例えば四則演算であれば、以下のように命令し、計算が実行できる。 1 + 2 ## [1] 3 5 - 10 ## [1] -5 3 * 8 ## [1] 24 1/2 ## [1] 0.5 基本的に一つのコマンドは1行に書き、数字、演算記号、スペースは半角で入力する。以下は、べき乗、平方根、自然対数を計算するためのコマンドで計算できる。 2^3 ## [1] 8 sqrt(2) ## [1] 1.414214 log(2) ## [1] 0.6931472 Rは、ベクトルや行列の計算も可能である。c() という関数を用いると、ベクトルを作成できる。例えば、c(1, 3, 5) というコマンドによって(1, 3, 5)というベクトルが作成できる。作成したベクトルを使って以下のような計算も可能である。 c(1, 3, 5) + 1 ## [1] 2 4 6 ベクトルは、連続した数字の列を生成するための演算子である : を用いても作成することができる。例えば、1から100の整数を要素とするベクトルは以下のように作成することが可能である。 1:100 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 また、ベクトルの要素は文字列でも構わない。 cities &lt;- c(&quot;Tokyo&quot;,&quot;Osaka&quot;,&quot;Kobe&quot;) cities ## [1] &quot;Tokyo&quot; &quot;Osaka&quot; &quot;Kobe&quot; 上記の計算方法に加え、Rが持つ重要な特徴に、オブジェクトの定義がある。Rでは、任意の行列、ベクトル、数値などに名前をつけ定義したうえで、それを用いた計算を行うことができる。なお、Console上で以下のように定義（実行）したオブジェクトはenvironmentタブ内に表示されるため、各自確認をしてほしい。なお、定義したオブジェクトの確認・出力も簡単に行えるが、大文字と小文字は区別されるため、注意が必要である。 a &lt;- 1 b &lt;- 2 a ## [1] 1 A ## Error: object &#39;A&#39; not found また、定義したオブジェクトを用いた計算も実行できるため、各自以下の計算を実行し、結果を確認してほしい。 a + b a / b a ^ b なお、先程のベクトル操作と組み合わせ、ベクトル名 [i] とすることで、ベクトルの i 番目の要素にアクセスすることができる。例えば、以下のaとbというベクトルから特定の要素を取り出すことを考える。なおこの場合、同時に複数の要素を取り出すこともできる。 a &lt;- seq(10, 100, length = 10) b &lt;- 10:1 aの2番目の要素 a[2] ## [1] 20 bの2番目の要素 b[2] ## [1] 9 aの3-5番目の要素 a[3:5] ## [1] 30 40 50 aの1,3,5番目の要素 a[c(1,3,5)] ## [1] 10 30 50 分析で繰り返し必要になる機能がRで使えないときは、function()関数を使って、新たな関数を作成できる。例えば、最大値と最小値を並べて表示したい場合を考える。そのために、ここでは ‘mm’ という新たな(オブジェクトxの最小値と最大値で構成されるベクトルを返す)関数を作ってみる。 mm &lt;- function(x){ c(min(x), max(x)) } そして上記の関数を利用して、以下のオブジェクト a, b の最小値と最大値を出力する。 a &lt;- c(1, 5, 100, 2, -8, 7) b &lt;- c(1, 6, 8, 0, 120) mm(a) ## [1] -8 100 mm(b) ## [1] 0 120 しかし、すべての関数を自作するのは難しい。Rでは様々な計算を実行するための関数が用意されており、多くのマーケティング研究においては既存の関数を用いることで対応が可能である。実は上記のmm関数の中で使っている “min”や”max”も、それぞれ最小値と最大値を返す関数である。他にも例えば、meanや median があり、これらはそれぞれ平均値と中央値を計算するための関数である。 関数の利用においては例えば、f()のように関数名 “f” のあとにカッコをつけて表記する。()の中には、引数（arguments）を用い、計算に必要な情報を指定することが必要となる。例えば、seq() という関数を用いて、2以上20以下の偶数の数列(sequence)を作ることが可能である。seq(from = x, to = y, by = z)は等差数列を作るための関数であり、第一の引数で最初の数、第二の引数で最後の数、第三の引数で間隔を指定することで数列を作成できる。以下で提示される二通りの表記ではどちらも同じ結果を返す。 seq(from = 2, to = 20, by = 2) ## [1] 2 4 6 8 10 12 14 16 18 20 seq(2, 20, 2) ## [1] 2 4 6 8 10 12 14 16 18 20 特定の関数に対する引数を確認したい場合は ‘?関数名’とconsoleに命令することで確認が可能になる。例えば、seq関数について知りたければ、’?seq’ で確認できる。 Rに元から含まれている関数以外にも他者が開発してくれた関数も存在する。そしていくつかの関数をまとめたpackagesが多数存在する。これまで世界中の開発者たちが作成したパッケージを公開してくれている。パッケージは何らかの目的や課題を達成することを目的に構成されたコードライブラリであり、それらをインストールし、各セッションごとに起動することで利用できる。 CRAN（The Comprehensive R Archive Network、Rに関するコードとドキュメントを配布するサーバーネットワーク）で公開されているパッケージは、install.packages()でインストール可能である。また、Rstudio の場合、ペインからpackagesタブ\\(\\rightarrow\\)Install\\(\\rightarrow\\)パッケージ名の入力という手順でもインストールが可能である。そしてインストール済パッケージは、library() によって起動することで、活用可能にある。ここで注意しておきたいのは、library()によるパッケージの起動はセッションごとに実行しないといけないという点である。まずは以下の通り、本講義で用いる “tidyverse” パッケージをインストールし起動してみる。“tidyverse”は、複数のパッケージから構成されているパッケージであり、データの整形・分析を行うために役立つ複数のパッケージをまとめてインストール・起動できる。 install.packages(&quot;tidyverse&quot;) library(&quot;tidyverse&quot;) "],["rscript.html", "R スクリプトのすゝめ", " R スクリプトのすゝめ Rstudio環境で作業を行う際には、Consoleに直接コマンドを入力するのではなく、‘.R’ という拡張子のファイルを使って、「Rスクリプト」を作成することを勧める。RスクリプトはRでの分析に対応しRコマンドの集まりとして記されるファイルであり、SourceエディタからRスクリプトに記載されたコマンドを、Consoleを通じて実行する。 Rスクリプトを用いることの重要性は、コマンドの修正可能性と、分析の再現性という二点から理解できる。まず修正可能性として、そもそもコマンドを書くうえでは大小様々な誤りが付き物である。このような間違いに対応し適宜修正を加えていくためには、実行するコマンドを一つ一つ console に直接記載するのではなく、Rスクリプトとして分析過程を記録し、その内容に基づき記載、修正を加えることが好ましい。 第二に再現性においては、Rスクリプトとしてデータ整形・分析のプロセスを文書ファイルとして保存しておくことで研究者自身もしくは第三者が分析を再現することが可能になる。これは研究プロセスの客観性を高め、分析結果の信頼性を高めるために非常に重要な要素である。 これらに加え、Rスクリプトの利用は研究者個人の研究遂行上の利点もある。自身が行った研究であっても分析の細部に関しては時間経過とともに忘れてしまうものである。その際に、Rスクリプトによる分析プロセスの追跡可能性が役に立つ。また、同様の分析を再度別データで実施する場合も、既存のRスクリプトを応用することで効率的に分析が可能になる。これらに加え、共同研究において他の研究者と分析プロセスを共有する場合にもRスクリプトが役に立つ。 Rスクリプトを書く 先述のRスクリプトの利点を活かすために、Rスクリプトの作成においては、いつ作成されたなんのためのファイルなのか、そしてファイル内に記載されているコマンドがどのような意図によるものなのかがわかるように書くべきである。そのための重要になるのがコメント機能である。一つの行の中で#記号よりも後ろの部分はコメントとして処理（コメントアウト）される。コメントアウト機能とは、プログラムコードにおいて、分析や実行から除外するための指示である。この機能を使い、コメントで自然言語による説明を加えることで、コマンドの説明や意図等、自分や他人がスクリプトを見返して内容を理解できるようにする。例えば、Xという変数の平均値を求める場合、以下のようにRスクリプトを書くようにする。 #Xの平均値を求める。 mean(X) Rスクリプトは、Rstudioの左上にある+ボタンから新規作成可能である。ここでは試しに、新規Rスクリプトを作成し、“mktg01.R” という名前で保存してほしい。保存したRスクリプトはファイルから開くことができる。“mktg01.R”ファイルを作成したら、試しに以下のコマンドを書き込み、実行してほしい。Rスクリプトからコマンドを実行する際には、コマンド記入後、Rスクリプト上で実行したい行にカーソルを合わせた状態でcommand (control) + Return (Enter)を入力する。もう一度同じキーを押すと、2行目のコマンドが実行される。これらを実行することで、Rコンソール上に、下記と同じ結果が出ていることを確認してほしい。なお、コマンドを記入の際には、こまめに command (control) + s により保存することを心がけるようにしてほしい。 a &lt;- 9 sqrt(a) ## [1] 3 また、Rスクリプトを作成する際には、ファイルの冒頭に以下の説明を書き込む習慣をつけると後々見返すときに便利である。 ファイル名 目的 作成者 作成日 最新更新日 例えば、上記の内容とコマンドを含めた “mktg01.R” ファイルは、以下のようになる。 Rスクリプト例 "],["ch2summary.html", "本章のまとめ", " 本章のまとめ Rはデータの管理、分析、図表の作成を行うことができる統計分析プログラミング言語である。 Rを動かすには、コマンドと呼ばれる命令をコンソールを通じて実行する。 コマンドは基本的にRスクリプトに書き込んでからcommand (control) + Return (Enter)で実行する。 Rスクリプトにはコマンドだけでなくコメントを使った説明も追加する。 分析には既存の関数やパッケージを使うことが多い。 "],["positcloud.html", "コラム：Posit cloudを使ってみる", " コラム：Posit cloudを使ってみる R studioをより手軽に利用できるサービスとしてPosit Cloudがある。Posit Cloudはブラウザを通じてR studio環境を利用できるサービスであり、アカウント登録をするだけでよく、コンピュータへのRおよびR studioのダウンロードとインストールが不要である。 Posit Cloudの利用方法はとても簡単である。大まかな利用までの流れは以下のとおりである。 以下のリンク（https://posit.co/）からサイトへアクセスし、ProductsタブからPosit Cloudを選択する。 その後、進んだ画面で “Get Started” \\(\\rightarrow\\) （特別な理由がなければ）Free planを選択し “Sign up” \\(\\rightarrow\\) 好きな方法でアカウントを作成する。 登録が完了すると、自身のアカウントのホーム画面へ移動する。新しいR studio セッションを開始するためには、画面右上の New projectボタンを押し、“New Rstudio Project” を選択する。 New projectのセットアップが完了すると、Studio環境画面が表示される。 新しいRstudio 画面 このような手順でデスクトップ版と同様のR studio 画面にブラウザからアクセスすることができる。これにより、（容量の制限はあるものの）デスクトップ版と同様の分析を実行することができるため、大人数講義や、RやR studioのインストールに不安のある方はこちらのサービスを利用してほしい。 "],["ch2reference.html", "参考文献", " 参考文献 浅野雅彦・矢内勇生 (2018) 「Rによる計量政治学」, オーム社. ランダージャレド（2015）「みんなのR 第2版」，高柳新市・牧山幸史・蓑田高志訳，マイナビ. "],["handling.html", "データ整理と要約・可視化 ", " データ整理と要約・可視化 "],["Ch3intro.html", "本章の概要", " 本章の概要 本章では、Rを用いたデータの処理と記述的な分析について紹介する。マーケティング領域では、様々なタイプのデータを扱うが、どのようなデータであってもデータを取り込み、分析可能な形に処理した後、データの特徴について確認することが必要になる。最終的に高度な統計分析を行うことを想定していたとしても、自身の獲得したデータの特徴を確認することは非常に重要である。そのため、本章ではデータの読み込みやデータ処理といった、分析の前に必要な技術的過程を紹介する。 データセットの構築が完了したあとは、分析を行うのだが、本章ではRを通じて実行可能な基本的なデータ分析手法を紹介する。Rには、様々な計算を実行するための関数が用意されており（例、mean, median, sqrt 等）、これらを使えば、分析者はシンプルなコマンドで分析が可能になる。関数のは f(argument) のように関数名 f のあとにカッコをつけて表記することで利用する事ができる。なお、argument は日本では引数とよばれ、計算に必要な情報の指定である。関数の利用において作業者は具体的な関数名とそれに対応する引数を指定する必要がある。例えば、データ（列ベクトル）x の平均値を計算したい場合には、以下のようなコマンドで実行できる。 mean(x) ただし、 na.rm = TRUE はデータに欠損値がある場合に、それを無視して（欠損値でない観測値のみで）計算を行うための引数である。 Rの作業として本章では主に、1. データの読み込み（csv, excel, etc.）、2. dplyrの利用とデータ整形、3. パイプ演算子を用いた複数処理の実行、について学ぶ。なお、これらの作業は、統計的な分析を実行する前のデータ前処理としても広く使われるものなので、データ分析をしたいと考える人達にとってはとても重要なスキルになる。 分析可能な形にデータを処理した後は、データの特徴を確認することが必要になる。具体的には、記述統計や図示化を用いて、特定の変数の分布や変数間の関係について確認を行うことが重要である。この過程により、調査の背景にある実情を把握できるとともに、入手したデータ（のコーディングなど）にエラーがないかを確認することにもつながる。本章では、関数を用いた基本的な記述統計の計算はもちろん、先述のパイプ演算子を用いて、ある特徴を持つ観測における記述統計の計算などを簡単に行う方法も紹介する。 また、二変数間の関係を捉えるための基本的な指標である相関係数についても説明する。そこでは、相関係数の意味や係数の解釈における注意点についても紹介した後、データを可視化することの重要性も合わせて説明する。データの可視化においては主に、ggplot2 というパッケージを用いた方法を紹介する。本章では主にRパッケージ内に含まれているデータ例を用いて、可視化の方法を紹介するため、読者においてはぜひ自身の関心のあるデータを用いて実行してみてほしい。 "],["readData.html", "データの読み込み", " データの読み込み 本節で用いるパッケージをまだインストールしていない読者は、以下のコマンドを用いてインストールしてほしい。また、インストールを完了したら、library()関数によって各パッケージを起動し、作業に備えてほしい。 install.packages(c(&quot;tidyverse&quot;,&quot;readr&quot;,&quot;readxl&quot;)) library(tidyverse) library(readr) library(readxl) ここからは、データセットを用いた情報の取り込みとデータ処理作業を行っていく。多くの場合、R外部で作成されたデータを取り込み利用するのだが、あるソフトウェアで作成・保存されたデータセットが他の環境で利用できるとは限らないという点に注意が必要である。そのため、ソフト特性に依存しない汎用的な形式を使うことが好まれることも多い。汎用性の高いファイル形式の代表的な例がCSV (comma separated values) である。以下は、mktData.csvという架空のファイルをdfというオブジェクト名で取り込むための、見本コードである。ここで用いる関数は、readrというパッケージのread_csv() という関数である。なお、以下のコードは、実在しない ’mktData.csv’というデータセットを引数に利用した見本コードであるため、このコードをそのまま実行してもエラーを返すだけであることに注意をしてほしい。実際には、自身が利用するファイル名を指定してファイルを読み込むことになる。なお、以下のコードの2行目は、データの1行目に変数名（列名）が含まれていない場合の引数の指定方法である。また、下記コードで利用している :: は用いるパッケージを指示するための演算子である。これにより、library() を用いなくても、指定したパッケージ内の関数を利用することができる。 df1 &lt;- readr::read_csv(&quot;mktData.csv&quot;) df2 &lt;- readr::read_csv(&quot;mktData.csv&quot;, col_name = FALSE) なお、R studioデスクトップ版を利用している場合には、ファイルが格納されているディレクトリ名も指定する必要がある。Rにおいては様々なファイルを入力・出力することになるため、利用するディレクトリが一貫していないとそれだけで作業が煩雑になる。そのため、??章 で紹介している「プロジェクト機能」必ずを活用するようにほしい。 ここで用いるデータは、有斐閣ウェブサイトを通じて配布している。そのため、読者においては各自のコンピュータにダウンロードし、分析用に活用してほしい。ここではまず、分析に利用するデータを格納するディレクトリを作成するコードを紹介する。以下のコードは、プロジェクトを作成しそのためのディレクトリを指定していることを前提にしている。 具体的は、以下の通りdir.create() を使ってproject内に新たに data というディレクトリ（フォルダ）を作成する。 dir.create(&quot;data&quot;) 新たなディレクトリを作成したら、そこに、ウェブサイトよりダウンロードしたデータを入れてほしい。ここではまず “2022idpos.csv”というデータを用いる。データが無事 data ディレクトリに含まれたら、以下のコマンドによってそのデータファイルをR の作業スペースに読み込み、それに “idpos” というオブジェクト名を定義する。なお、ここで分析者はディレクトリを指定することも必要になる。また、コード内の na は、欠損値がどのように保存されているかを指定するための引数であり、もし欠損値が空欄であればnaによる指定は必要ない。 idpos &lt;- readr::read_csv(&quot;data/2022idpos.csv&quot;, na = &quot;.&quot;) 問題なくデータを読み込むことができたら、そのデータの冒頭数行を head() 関数によって表示する。head() 関数の結果によって、このデータセットのうち、4つの変数（列）について6つの観測（行）が表示されるはずである。なお、R studio 画面内の Environment タブからこのidposデータが3000行、5列のデータセットであることを確認できる。 head(idpos) ## # A tibble: 6 × 4 ## id date spent coupon ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019/9/25 14326 1 ## 2 32 2019/9/10 10232 1 ## 3 30 2019/9/9 6881 1 ## 4 29 2019/9/4 6365 0 ## 5 46 2019/9/10 7595 1 ## 6 44 2019/9/14 7858 0 また、読み込んだデータ特徴の確認は他の関数でも実行できる。例えば、name() 関数を使えば、データ内の変数名 (列名) を確認できるし、tidyverseに含まれる glimpse() 関数によってもデータの冒頭数行を含むいくつかの情報を返してくれる。 names(idpos) ## [1] &quot;id&quot; &quot;date&quot; &quot;spent&quot; &quot;coupon&quot; glimpse(idpos) ## Rows: 3,000 ## Columns: 4 ## $ id &lt;dbl&gt; 12, 32, 30, 29, 46, 44, 44, 32, 3, 34, 36, 3, 42, 18, 38, 4, 19… ## $ date &lt;chr&gt; &quot;2019/9/25&quot;, &quot;2019/9/10&quot;, &quot;2019/9/9&quot;, &quot;2019/9/4&quot;, &quot;2019/9/10&quot;, … ## $ spent &lt;dbl&gt; 14326, 10232, 6881, 6365, 7595, 7858, 9405, 1821, 8375, 1828, 6… ## $ coupon &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, … なお、このidposデータは、POS (Point of sales) という小売店レジでの取引データとロイヤルティプログラムなどの会員IDを含むID-POSと呼ばれるデータを想定して簡略化し作成した、演習用人工データである。データには、小売店舗での取引日（date）、金額（spent）、クーポン利用の有無 (coupon)、性別 (gender) が含まれている。本来のPOSデータは、より詳細な日時や具体的な製品単品レベルの取引品目など、より詳細な情報が含まれているはずだが、ここでは簡単化のためにこのようなデータにしている。また、もしスプレッドシート形式で表示したい場合には View() 関数をconsoleに直接入力することでそれが可能になる。例えば、idposデータを用いて以下のようなコードを入力することで、Sourceウィンドウに新しいタブができ、そこにデータセットが表示される。 View(idpos) "],["normalization.html", "データの整形", " データの整形 tidyverse の活用 データの整形には、tidyverseパッケージ群に含まれるdplyrというパッケージを用いる。作業では、tidyverseをインストール・起動しておけばdplyrも利用できるため、特に心配する必要はない。dplyr には、いくつもの便利な関数がふくまれているが、本節では主に以下の関数および機能を紹介する。 summarize(): データセットを関数内で定義した統計量等を用いて要約された新しいデータセットに変換するための関数 mutate(): データセットに新しい変数を追加するための関数 filter(): データセットから、関数内で指示する特定のレコード（行）を残す（フィルタリング）するための関数 select(): データセットから、関数内で指示する特定の列を抽出するための関数 arrange(): データセットから、関数内で指示する順番で並べ替えるための関数 パイプ演算子 %&gt;%: 前（左）の関数の出力を次（右）の関数の入力として渡すため演算子 summarize は、ある変数の平均値や標準偏差などの記述統計量を計算することができる関数である。例えば、dataというデータセットに含まれる var_name という変数の平均値を計算し、それを M という変数名として定義する場合、以下のコマンドを用いる（以下のコマンドは見本コードである）。 summarize(data, M = mean(var_name)) mutate は、データセットに引数内で指定した定義の変数（列）を追加する関数である。例えば、以下の見本コードにような指示によって、data というデータセットに対し、definition で定義した変数をnew_varとして追加することができる。実際の分析でdefinitionを定義する場合には、様々な関数や論理式を利用する事が多い。例えば、“new_var = var1/100” という定義を用いれば、var1を1/100倍した値をnew_varとして定義することになる。また、“new_var = var1 – mean(var1)”という定義を用いれば、var1の観測値からvar1の平均値を引いた値をnew_varとしている。なお、このような操作化を一般的に「中心化」と呼ぶ。 mutate(data, new_var = definition) また、mutate関数の利用においては、条件分岐を用いた変数の作成を行うこともある。そのように、研究者がある変数の値に応じて異なる値を変数を作成するときには、mutate内で、ifelse()関数を用いる事が多い。ifelse() 内の第一引数は条件、第二引数は条件が満たされたときの処理、第三引数は条件が満たされないときの処理をそれぞれ表す。なお、特定の条件の指定には “==” （同値）, “&gt;=”（以上）, “&lt;=”（以下） を使う。具体的には、var1 が2ならば1をとり、それ以外であれば０をとるという条件でnew_varを作成するという指示は、以下のようになる（以下は見本コードである）。 mutate(data, new_var = ifelse(var1 == 2, 1, 0)) filter関数は、データから特定の条件に合致する行だけ取り出す場合に用いる関数である。例えば、男性（gender == “male”）のサンプル情報のみ抽出したい場合には以下のような指示になる。 filter(data, gender == &quot;male&quot;) なお、特定の条件以外のものを指定したいときは、 という論理式 “!=” (not equal) を使う。男性以外の行を選ぶための指示は、以下の通りになる。 filter(data, gender != &quot;male&quot;) select関数は、特定の変数（列）を選んで新たなデータフレームを作成することができる関数である。例えば、dataというデータセットから、var1、var2、var3 という変数（列）を抽出して、data2というdataframeとして定義するには、以下のような指示になる。 data2&lt;- select(data, var1, var2, var3) 反対に、取り除きたい変数を指定するときには、以下のように “-” を使う。 data2&lt;- select(data, -var1) 列の指定方法には、いくつかのやり方が存在する。並んでいる列をまとめて指定するときは:（コロン）を使う。例えば、var1からvar5までの列をまとめて抽出し、それをdata2として定義するのは以下のようにできる。 data2&lt;- select(data, var1:var5) また、tidyverseのstarts_with()（ends_with()）を使うことで、変数名の冒頭（末尾）が特定の文字列から始まる変数を指定するようなことも可能である。例えば、“v” という文字から始まる変数を取り出すための指示は、いかのようになる。 data3&lt;- select(data, starts_with(&quot;v&quot;)) arrangeは、データの並べかえを可能にする関数である。例えば、以下ではvar1の値が小さい順（昇順）に並べ替えるような指示を示す。一方で、降順にする場合は、desc(var1)と引数を指定する必要がある2。 data2 &lt;- arrange(data, var1) data2 &lt;- arrange(data, desc(var1)) また、tidyverse環境において、変数名を変更することも、rename() 関数で可能になる。以下の指示によって、var_name という変数を new_name に変更する事ができる。 data3 &lt;- rename(data2, new_name = var_name) dplyr を活用すると、パイプ演算子（%&gt;%）が使える（ショートカット: command (control) + Shift + m）。パイプ演算子は、左側の処理結果を演算子右側の関数の第一引数として利用するための指示である。たとえば、以下のコマンドではまず \\(\\small 10-6\\) が計算され、その結果である “4” が sqrt() の引数として利用される（sqrt(4) は 2）。 (10-6) %&gt;% sqrt() ## [1] 2 パイプ演算子は、複数のデータ操作処理を連続して行う際に便利である。例えば、顧客の情報を含むデータセット(data)から、男性に該当する情報のみを抽出し、var1(例、購買額)についてのランキングを作成したうえでいくつかの変数を含んだデータセット（new_data）を作成する場合を考える。その際に実行すべき作業とそれらに対応する関数は以下のように示すことができる。 男性の情報だけ抜き出す(filter) Var1の値について降順に並べ替える(arrange) 第一位から最下位までの順位を割り当てた ranking 変数を作る(mutate) var1 , var2, var3, var4, rankingだけ残し(select) new_dataとして定義する 上記の作業を一気に行うためのコードをパイプ演算子を使わずに書くと以下の様になる（以下は見本コード）。 new_data &lt;- select(  mutate(   arrange(    filter(data, gender == &quot;male&quot;),    desc(var1)),    ranking = 1:n()),   var1, var2, var3, var4, ranking) パイプ演算子を使わない場合、先に実行する処理が内側に来ており、一見して何を行っているのか理解するのが難しい。一方でパイプ演算子を使い、左側の処理結果を演算子右側の関数の第一引数として利用すると、以下のように書き換えることができる。 new_data &lt;- data %&gt;% filter(gender == &quot;male&quot;)%&gt;% arrange(desc(var1)) %&gt;% mutate(ranking = 1:n()) %&gt;% select(var1, var2, var3, var4, ranking) パイプ演算子の利用により、各関数の処理を一つの行で示せる。また、処理の順番通りに関数を記載することが可能なので、コードの記述容易性と可読性の両方が高まる。また、パイプ演算子による操作は次の関数の第一引数以外に反映されることも可能である。第一引数以外の引数に左側の処理結果を反映させる際には、該当する箇所に “.” （ドット）を使う。たとえば、\\(\\small 10-2\\)の計算結果を用いて2から8までの偶数で構成されるベクトルを返すためのコードは以下のように書くことができる。 (10-2) %&gt;% seq(from = 2, to = ., by = 2) ## [1] 2 4 6 8 データの整形・処理作業が終わったら、そのデータを自身のコンピュータ内のストレージに保存したいと考えるかもしれない。Rでは、外部への書き出しという形でデータを保存することが可能である。例えば、df という名前のデータフレームをnew_dataというファイル名で、dataというディレクトリにcsv形式を用いて保存するためには、以下のようなコードを用いる（以下は見本コード）。また、csv以外にもファイル形式は選択可能であり、例えばRのデータ形式(.Rds)で保存する場合には、“#Rds” 以降のコードを用いる。 readr::write_csv(df, path = &quot;data/new_data.csv&quot;) #Rds readr::write_rds(df, path = &quot;data/new_data.Rds&quot;) 企業データの処理 これまでに学んだデータ処理の手法を実行するために、本節では、 MktRes_firmdata.xlsxデータを用いる。このデータをwebサイトより data ディレクトリにダウンロードし、以下の要領で読み込んでほしい。 firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) このデータは、小売・サービス分野の企業約160社（企業数は年によって異なる）に関する2010年から2019年までの財務データである（計1440件）。このデータは、日本生産性本部における顧客満足度調査の対象になっている企業リストを作成し、その企業の中から金融領域の企業や、データを入手できなかった一部の企業を教育的意図から排除したものである。したがって、日本の小売・サービス分野において全国的に知名度のある代表的な企業の財務データ（の一部）だと考えられる。 なお、本データには以下の変数が含まれており、データ内の単位は従業員数（人）を除き百万円である。 fyear: 決算年 legalname: 企業名 ind_en: 日経業種名（英文） parent:親会社名（もしあれば） fiscal_month: 決算月 current_liability: 流動負債 ltloans: 長期借入金 total_liability: 負債合計 current_assets: 流動資産 ppent: 有形固定資産 total_assets: 資産合計 net_assets_per_capital: 純資産合計／資本合計 sales: 売上高 sga: 販売費及び一般管理費 operating_profit: 営業利益 net_profit: 当期純利益 pnet_profit: 親会社株主に帰属する当期純利益（連結）／当期利益（単独） re: 利益剰余金 adv: 広告・宣伝費 labor_cost: 人件費 rd: 研究開発費 other_sg: その他販売費及び一般管理費 emp: 期末従業員数 temp: 平均臨時従業員数 tempratio: temp/(emp+temp) indgrowth: 産業成長率 adint: 広告集中率（adv/sales） rdint: 研究集中率（rd/sales） mkexp: (sga - rd) / sales op: operating_profit / sales roa: pnet_profit / total_assets 本データセットは、複数年にわたる複数サンプルからのデータであり、一般的にこのような構造のデータをパネルデータという。パネルデータの分析の概要は ?? 節で紹介している。 ここではこのデータを用いて、以下の作業を行う。 2018年度のデータのみを抽出する。 企業名、年、売上高、人件費、期末従業員数、平均臨時従業員数のみの変数を含むデータセットにする。 労働単価（人件費/（期末従業員数+平均臨時従業員数））変数を作成する。 労働単価の高い順に並び替えてトップ10企業を出力する。 firm2018_check &lt;- firmdata %&gt;% filter(fyear == 2018) %&gt;% select(legalname, fyear, sales, labor_cost, emp, temp) %&gt;% mutate(wage = labor_cost/(temp+emp), na.rm=TRUE) %&gt;% arrange(desc(wage)) head(firm2018_check, n = 10) ## # A tibble: 10 × 8 ## legalname fyear sales labor_cost emp temp wage na.rm ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 株式会社リクルート 2018 2310756 388583 45856 2449 8.04 TRUE ## 2 株式会社 大丸松坂屋百貨店 2018 459840 62692 6695 3581 6.10 TRUE ## 3 株式会社 帝国ホテル 2018 58426 17307 1940 998 5.89 TRUE ## 4 株式会社 髙島屋 2018 912848 83779 7761 8849 5.04 TRUE ## 5 株式会社コメリ 2018 346862 43991 4646 4777 4.67 TRUE ## 6 株式会社オートバックスセブン 2018 213840 22139 4171 747 4.50 TRUE ## 7 株式会社ロイヤルホテル 2018 40884 13115 2049 894 4.46 TRUE ## 8 オルビス株式会社 2018 248574 28555 4181 2330 4.39 TRUE ## 9 株式会社ファンケル 2018 122496 15103 1381 2213 4.20 TRUE ## 10 近畿日本ツーリスト株式会社 2018 411821 38186 6956 2189 4.18 TRUE このように、データの中から研究課題と整合的な情報を抽出したり、変数を作成したりすることができる。ただし、研究者にとって都合の良い結果を得るために恣意的に用いるデータを制限、操作することは、研究不正となる。そのため、実際の研究では、どのようなデータ・情報を用いるかについて事前に計画しておく必要がある。 “desc”は、descending orderの略であり、降順を表す。↩︎ "],["summary.html", "データの要約と可視化", " データの要約と可視化 ここからは、記述統計や可視化によってデータを要約する方法について説明する。記述統計では、統計量と呼ばれる指標を用いてデータの特徴を数値から把握する。一方で可視化においては、図表を作成することでデータの特徴を視覚的に理解することを目的とする。実証的なマーケティング研究においては、データを用いた仮説の検証という方法が主流かもしれないが、仮説検証に用いるデータはどのようなものなのかを要約し、それを（論文やレポートの）読者へ伝えるプロセスは必要である。記述統計やデータの可視化は、このプロセスにおいて機能する方法である。 記述統計 記述統計の利用においては、データのタイプ別に利用すべき統計量が異なることに注意が必要である。データには量的変数とカテゴリ（を示す質的）変数があるが、量的変数は数値で測定できるものであり、その計算結果を解釈することも可能である。一方でカテゴリ変数は、各観測個体が属している状態やグループを表す指標であり、それを計算してもそこから含意を得るのが難しい。Rのような統計ソフトは非常に素直なので、たとえカテゴリ変数であってもそこに数値が入力されていれば、記述統計に必要な計算を実行し、結果を返してくれる。しかしながら研究においてはそれらの結果を適切に解釈する必要があり、自身が用いている変数のタイプに応じた分析を実行する必要がある。 その上で本節ではまずひとつの量的変数の情報を要約するための記述統計を紹介する。一つの数値によってデータ全体を代表させるような数値を代表値と呼ぶ。代表値は主に、データの中心を示す指標と考えられる。本節ではデータの中心を表す指標として中央値 (median) と平均値 (mean) を紹介する。中央値は、データのすべての観測値において、その値より小さな観測値の数と大きな観測値の数が等しくなるような真ん中の値を表す。そのため、（1, 3, 2, 5, 4）というデータにおける中央値は3である。これは、このデータを、1, 2, 3, 4, 5 と並べ替えると、3よりより小さな観測値の数と大きな観測値の数が等しくなっていることから確認できる3。 d &lt;- c(1, 3, 2, 5, 4) median(d) ## [1] 3 d2 &lt;- c(1, 3, 2, 5, 4, 6) median(d2) ## [1] 3.5 平均値（算術平均と呼ばれる）は、最もよく使われる代表値の一つである。平均値は、n個のデータ、\\(\\small x_1,x_2,...,x_n\\) に対して以下のように定義される。 \\[\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\] 観測値と平均値の差（\\(x_i - \\bar{x}\\)）は偏差と呼ばれ、偏差の和はゼロである（\\(\\sum_ix_i - \\bar{x}=0\\)）という性質を持つ。つまり、平均値を中心として、データの正の方向へのばらつきと負の方向へのばらつきが釣り合いが取れているということが伺える。この点が、平均値がデータの中心を表す代表値として用いられるひとつの理由である。また、平均値にはいくつかの好ましい統計的性質があるのだが、それについては後述する。Rにおいては、mean() 関数を用いることで分析が可能である。例えば、9人の生徒に対して行われた数学(x)と国語(y)のテスト(10 点満点)の結果が、それぞれ以下の通りであったとしよう。 数学: (3,3,5,5,5,5,5,7,7) 国語: (2,3,3,5,5,5,7,7,8) このときの平均値は以下のように求まる。 math &lt;- c(3,3,5,5,5,5,5,7,7) jpn &lt;- c(2,3,3,5,5,5,7,7,8) mean(math) ## [1] 5 mean(jpn) ## [1] 5 計算の結果、どちらも平均値は5であった。データの中心を表す代表値の値が等しかったため、これら2科目のテスト結果は同じ分布を持つと判断して良いのだろうか。自明かもしれないが、そのような解釈は不適切である。具体的には、データの「ばらつき」についても確認する必要がある。分布のばらつきは、平均値からの離れ方(平均値からの偏差) によって判断される事が多く、これが大きなデータが多い場合は、よりデータは散らばっ て分布していると解釈される。一方でデータが平均の近くに集まって分布している場合、ばらつきが小さいと捉えられる。この分布のばらつきは主に、分散や標準偏差という指標で測られる。 分散 (Variance, \\(S^2\\)で定義する) は以下のように、平均からの偏差の二乗の和をデータ数で割ったものだと定義される。平均からの偏差の和を計算すると、正の方向へのズレとマイナス方向へのずれがあるので、互いに相殺しあって合計は 0 になる。そこで、偏差の二乗和を用いることでデータ全体がどの程度平均からばらついているかを把握する。 \\[S^2 = \\frac{1}{n}\\sum_i^n (x_i-\\bar{x})^2\\] しかしながら、分散は元の値を二乗しているのでもとのデータと単位が異なる。そのため、分散の正の平方根 (\\(\\sqrt{\\cdot}\\)) を取った値を標準偏差と呼び、この標準偏差を用いることも多い4。なお、Rでは var() と sd() によって分散と標準偏差をそれぞれ求める。ただし、Rの関数による計算では \\(s^2=\\frac{1}{n-1}\\sum_i^n (x_i-\\bar{x})^2\\) で定義される「不偏標本分散」および「不偏標準誤差」という指標を用いる。これらの指標は、不偏性（??節参照）という統計的に好ましい性質を持っているため、分析ソフトではこちらの計算方法が用いられる。そのため、Rを用いた分散の計算値が n で割った際の手計算値と異なることがあるのでその点には注意が必要である。 var(math) ## [1] 2 var(jpn) ## [1] 4.25 先程の数学と国語のテスト結果データを用いて分散を計算すると、国語の方が分散が大きいことがわかる。つまり、両テストとも平均値は同じであるものの、国語のほうがそのスコアのばらつきが大きいことがわかる。このように、代表値とともにデータのばらつきに関する情報も踏まえてデータの特徴を把握することが好ましい。 観察されたデータと標準偏差を用いて、特定の観測結果がデータ内において「相対的に」どのような位置にいるのかを捉えることも可能になる。具体的には、任意の量的変数 \\(x_1,...,x_n\\) に対して、標準化されたスコア \\(z_1,..,z_n\\) は以下のように定義できる。 \\[ z_i=\\frac{(x_i-\\bar{x})}{\\sqrt{(S^2)}} \\] ただし、 \\(S^2\\) は変数 \\(x\\) の分散である（不偏標本分散を用いることもある）。上記定義の通り、標準化スコアは観測値の平均からの偏差を標準偏差で割っており、ある観測が平均値から標準偏差何個分ズレているかを示していると解釈できる。なお、標準化スコアは、平均が0、分散が1になることも知られている。 一方でデータの観測数（ \\(n\\) ）が偶数である場合、\\(\\small n/2\\) 番目と、\\(\\small (n/2)+1\\) 番目が中央となるため、n個のデータの観測値を、\\(x_1,x_2,...,x_n\\) とすると、これらふたつの値の平均値（ \\(\\small \\frac{x_{\\frac{n}{2}}+x_{ \\frac{n}{2}+1}}{2}\\) ）が中央値となる。Rにおいてはmedian() 関数によって以下のように計算することができる。↩︎ 偏差の二乗和のかわりに偏差の絶対値を用いた平均偏差という指標も存在する。しかしながら、分散や標準偏差のほうが好ましい統計的性質を持つことから、二乗和が用いられることが多い。↩︎ "],["categoricalVar.html", "カテゴリ変数の要約", " カテゴリ変数の要約 一方でカテゴリ変数は、代表値や分散によって含意を得るのではなく、頻度のカウント（集計）や、クロス集計を用いることが多い。これにより、各カテゴリにどれぐらいの観測数があるのかを確認することが可能になる。カテゴリ変数の内容（出現頻度）の確認には、table() 関数を用いる。また、with()関数を用いて同様の結果を得ることも可能である。ここでは、先ほど用いた firmdata から2018年度の情報を抽出し、日経業種に基づく産業の違いから、どのカテゴリの企業がどれだけデータ内にいるのかを確認する。なお、tidyverseを起動していない場合には、必要に応じて library(tidyverse) を事前に指示してほしい。 firm2018 &lt;- firmdata %&gt;% filter(fyear == 2018) table(firm2018$ind_en) ## ## Air Transportation Amusement Services Bakery Products ## 8 4 1 ## Communication Services Cosmetics &amp; Toilet Goods Department Stores ## 2 3 7 ## Foods, NEC Home &amp; Pre-Fabs Hotels ## 1 2 5 ## Miscellaneous Services Miscellaneous Wholesales Motor Vehicles ## 27 2 4 ## Musical Instrument Railroad (Major) Railroad (Minor) ## 1 27 2 ## Real Estate - Sales Retail Stores, NEC Supermarket Chains ## 1 35 14 ## Trucking ## 1 with(firm2018, table(ind_en)) また、table関数にて2つのカテゴリ変数を指定することで、両変数に対応するカテゴリの出現頻度を返してくれる。このような表のことをクロス集計表とよぶ。例えば、同データにおける広告集中的な企業を把握するため、広告集中度が中央値よりも高ければ1、それ以外であれば0を取るダミー変数（??節参照）を作成し、各産業カテゴリとの関係を確認する。 firm2018 &lt;- firm2018 %&gt;% mutate(ad_dummy = ifelse(adint &gt; median(adint),1, 0)) with(firm2018, table(ind_en,ad_dummy)) ## ad_dummy ## ind_en 0 1 ## Air Transportation 4 4 ## Amusement Services 4 0 ## Bakery Products 0 1 ## Communication Services 1 1 ## Cosmetics &amp; Toilet Goods 0 3 ## Department Stores 0 7 ## Foods, NEC 0 1 ## Home &amp; Pre-Fabs 0 2 ## Hotels 5 0 ## Miscellaneous Services 17 10 ## Miscellaneous Wholesales 1 1 ## Motor Vehicles 0 4 ## Musical Instrument 0 1 ## Railroad (Major) 27 0 ## Railroad (Minor) 2 0 ## Real Estate - Sales 0 1 ## Retail Stores, NEC 11 24 ## Supermarket Chains 2 12 ## Trucking 1 0 上の表では、各行に産業名が記載されており、その右隣に、広告集中度が低い（ad_dummy=0）企業数が、さらにその右隣には広告集中度が高い（ad_dummy=1）企業数がそれぞれ記載されている。これらのデータを確認すると、鉄道会社やアミューズメント、ホテル、トラック運送業において広告集中度が高い企業が少ないことがわかる。それ以外では産業内でも広告集中度の高い企業と低い企業とが比較的バラけている。 特定のカテゴリに着目して、カテゴリ変数についての集計を行うことも可能である。例えば、広告集中度が高い企業における産業のばらつきを調べたいときには、filter() 関数を用いれば良い。 firm2018 %&gt;% filter(ad_dummy == 1) %&gt;% with(table(ind_en)) ## ind_en ## Air Transportation Bakery Products Communication Services ## 4 1 1 ## Cosmetics &amp; Toilet Goods Department Stores Foods, NEC ## 3 7 1 ## Home &amp; Pre-Fabs Miscellaneous Services Miscellaneous Wholesales ## 2 10 1 ## Motor Vehicles Musical Instrument Real Estate - Sales ## 4 1 1 ## Retail Stores, NEC Supermarket Chains ## 24 12 カテゴリ変数と量的変数の関係を調べることも、グループ別に量的変数の要約を行う形で可能である。具体的には、 group_by() 関数を用いる。group_by() は関数内で指定した変数を用いてデータをグループ化し、グループごとの集計や処理を可能にする関数である。例えば、売上高と広告集中度の平均と標準偏差を産業ごとに確認することは、以下のような指示で可能になる。 firm2018 %&gt;% group_by(ind_en) %&gt;% summarize(obs = n(), sales_m = mean(sales), sales_sd = sd(sales), adint_m = mean(adint), adint_sd = sd(adint)) ## # A tibble: 19 × 6 ## ind_en obs sales_m sales_sd adint_m adint_sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Air Transportation 8 1772786. 305240. 0.00311 0.00333 ## 2 Amusement Services 4 298138. 263017. 0 0 ## 3 Bakery Products 1 1059442 NA 0.0122 NA ## 4 Communication Services 2 547088. 172736. 0.0231 0.0327 ## 5 Cosmetics &amp; Toilet Goods 3 140669. 100063. 0.108 0.0498 ## 6 Department Stores 7 843248. 348819. 0.0195 0.00535 ## 7 Foods, NEC 1 504153 NA 0.0229 NA ## 8 Home &amp; Pre-Fabs 2 4143505 0 0.00857 0 ## 9 Hotels 5 62135. 58060. 0 0 ## 10 Miscellaneous Services 27 311867. 456037. 0.0114 0.0204 ## 11 Miscellaneous Wholesales 2 176520 52778. 0.0203 0.0287 ## 12 Motor Vehicles 4 5279122. 4233188. 0.0254 0.00404 ## 13 Musical Instrument 1 434373 NA 0.0443 NA ## 14 Railroad (Major) 27 1302921. 1037834. 0 0 ## 15 Railroad (Minor) 2 260502 0 0 0 ## 16 Real Estate - Sales 1 1861195 NA 0.0114 NA ## 17 Retail Stores, NEC 35 571019. 547247. 0.0243 0.0272 ## 18 Supermarket Chains 14 4335164. 3511347. 0.0147 0.00782 ## 19 Trucking 1 1118094 NA 0 NA このように、カテゴリごとの量的変数の要約も実行可能である。なお、標準偏差が NA となっている箇所は、観測数が 1 であり、標準偏差を計算できない状況を表している。 "],["visualization.html", "データの可視化", " データの可視化 本サイトでのデータの可視化では、主にtidyverse内に含まれる ggplot2 というパッケージを用いる。データの可視化では、円グラフ、折れ線グラフ、帯グラフなどの様々なグラフを用いて視覚化されることも多いだろう。しかしなが本節では、主にヒストグラム、箱ひげ図、バイオリンプロットをRでの実行例とともに紹介する。これらの図は、量的変数の分布を視覚的に示すことについて優れた可視化の方法だと言える。ここでは、ggplot2に内包されている diamonds データを用いて可視化を学ぶ（tidyverseを起動することで自動的に ggplot2も起動されるため、このタイミングでtidyverseを起動していない場合には、必要に応じて library(tidyverse) によってパッケージを起動してほしい）。diamonds データについては以下のように確認できる。 head(diamonds) ## # A tibble: 6 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 なお、Macのデスクトップ版でggplot2等を使うと日本語が文字化けするので、Macユーザーは別途以下のコマンドを実行する必要がある。 #For mac users theme_set(theme_gray(base_size = 10, base_family = &quot;HiraMinProN-W3&quot;)) ここではまず、ggplot2の ggplot() 関数を用いて図示化のためのオブジェクトを作成する。この関数では、以下の引数を指定する。 data: 可視化に用いるデータフレームの指定 mapping: データから抽出する変数と画面に表示される図との関係の指定 mapping内で、aes() 関数（aesthetics）で視覚化に用いる変数とプロット要素間の接続を図ることも多い。 aes() 関数は、データ内の変数がをどのように視覚的情報（美的特性）にマッピングするか（例えば、x軸とy軸の変数はなにか）を特定するための関数である。こらの引数により、ggplot関数で作成された図示化オブジェクトには、着目するデータと変数が特定される。 続いて、ggplot()で作られたオブジェクトに対して、geom (geometry) に関する情報を 追加し、グラフィックの層(layer)を加えることで図を作成する。このプロセスでは、geom_point() による散布図や、geom_histogram() によるヒストグラムなど、具体的な図表のタイプに対応する関数を利用することで、図を作成できる。また、geomに関する関数以降に labs() というラベルに関する関数を追加することで、図に必要な情報を加筆することが可能になる。 ggplot2を用いたデータ可視化の例として、まずはヒストグラムを描画する。ヒストグラムはデータの分布を離散的に示すものであり、連続変数を階級で分けて各階級の頻度を図示化する。ヒストグラムは一つの変数を扱った図なので、mapping引数ではひとつの変数を指定する。その上で作成した図示化オブジェクトに geom_histogram() を追加することでヒストグラムを描画する。以下では、ダイアモンドの価格の観測頻度についての可視化例を紹介する。以下の図では、価格の程度を離散的に区切り、その区切られた各範囲の価格を取る観測がデータ内にどれだけ存在するかを示している。 p1 &lt;- ggplot(diamonds, mapping = aes(x = price)) p1 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム1: ダイアモンド価格&quot;) なお、縦軸を確率密度(density)に変えるときは、geom_density()を用いる。その際、fillという引数を設定すると、密度を範囲に色を塗ることができる (なお、“p1” というオブジェクトは再利用できるので、再びggplot()によって指定する必要はない)。 p1 + geom_density(fill = &quot;black&quot;, alpha = 0.5) + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム2: ダイアモンド価格（geom_density）&quot;) 次に、箱ひげ図の作り方を紹介する。箱ひげ図は、四分位数と四分位範囲等を図示化したものである。四分位数はデータを4等分する区切りの値であり、第一四分位はQ1、第二四分位はQ2、第三四分位はQ3、最大値はQ4で示される。四分位範囲はQ3-Q1の範囲で示されるものである。ここでは、Cutの質（Fair, Good, Very Good, Premium, Ideal）ごとに価格の分布を比べるため、複数の箱ひげ図を並べる例を提示する。 p2 &lt;- ggplot(diamonds, mapping = aes(x = cut, y = price)) p2 + geom_boxplot() + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;箱ひげ図1: ダイアモンド価格&quot;) 箱ひげ図を作成すると、ひげの上下に点が表示されることがある（上図では上部が太線のように見えている）。これは、外れ値の候補として全体の分布から離れて存在する観測値が示されている。ここで示される外れ値の候補は、Q1よりも四分位範囲\\(\\times 1.5\\) 以上小さい、ないしは、Q3よりも四分位範囲\\(\\times 1.5\\) 以上大きいかで特定される。外れ値がある場合、入力ミスなどのエラーではないか、異質な観測値でないか、を検討、確認することが必要になる。 次に紹介する図のタイプはバイオリンプロットである。バイオリンプロットは、箱ひげ図よりももう少し詳しくデータの分布を確認できる図である。ggplot2では、geom_violin() を用いる。例えば、先程の箱ひげ図をバイオリンプロットで示すと、以下のようになる。以下の図は、バイオリンプロット内に箱ひげ図を示すことでよりわかりやすい図を作成するように工夫している。 p2 + geom_violin() + geom_boxplot(fill = &quot;gray&quot;, width = 0.1) + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;バイオリンプロット: ダイアモンド価格&quot;) バイオリンプロットで横に広がっているところは、ヒストグラムで言う山が高いところを意味しており、そこに多くのデータが集まっていることを示している。 "],["cor.html", "二変数間の関係の要約", " 二変数間の関係の要約 ここまでの内容は（カテゴリ変数に関する一部の説明を除き）、一つの変数に関する要約と可視化を扱っていた。しかし、データ分析では二つの異なる変数間の関係を捉えたいと考えることも多い。二変数間の関係を数量的に要約するための指標の代表例が共分散や相関係数である。データ数をnとする変数xとyの共分散（\\(S_{xy}\\)）は、以下のように定義される。なお、Rで共分散を求める際には cov() 関数を用いる。 \\(S_{xy}=\\frac{1}{n}\\sum_i^n (x_i-\\bar{x})(y_i-\\bar{y})\\) また、\\(S^2_x\\)と\\(S^2_y\\)をそれぞれxとyの分散としたとき、相関係数（\\(\\rho_{xy}\\)5）は以下のように定義される。Rで相関係数を求める際には cor() 関数を用いる。 \\(\\rho_{xy}=\\frac{S_{xy}}{\\sqrt{S_x^2}\\cdot \\sqrt{S_y^2}}\\) 共分散は、二つのデータ間の共変動を示す指標であるものの、この数値を持って我々研究者が二変数の関係について（例えばその強弱などを）解釈するのは困難である。そこで、二変数間の関係を数値的に解釈する場合には、一般的に相関係数を用いる。相関係数は、-1 から 1 までの値を取り、正の値を取る場合は正の相関、負の値を取る場合は負の相関を示している。また、相関係数が正（負）の値かつ1に近いほど強い正（負）の相関であることが知られている。ただし、相関係数で表される二変数間の関係は、どれだけ線形関係に近いかである。言い換えると、相関が高いとはデータがどれだけ直線上に集まって分布しているかを示しており、グラフ等で示される線形関係の傾きについては何も回答することができないという点に注意が必要である。 例えば、以下のようなデータセットを考える。 X &lt;- tibble(x1 = c(-3, -1, 0, 2, 5), y1 = c(16, 12, 10, 6, 0), y2 = c(8, 6, 5, 3, 0)) X ## # A tibble: 5 × 3 ## x1 y1 y2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -3 16 8 ## 2 -1 12 6 ## 3 0 10 5 ## 4 2 6 3 ## 5 5 0 0 ただし、tibble はより大規模なデータ操作が容易になる特性を持った、tidyverse 上で用いられるデータフレームの形式である。tibble() 関数を用いることで、オブジェクトとなるデータフレームをtibbleとして定義することができる。 以下の計算で示すように、このデータセットにおける x1 と y1 の相関係数は -1 であり、両者の関係を図で示すと、すべてのデータが直線上（\\(y=-2x+10\\)）に並ぶことがわかる。なお、$ はデータフレーム内の変数名を指定するための記号である（データ$変数）。一方で、x1 と y2 との相関係数も -1 であるものの、両者の線形関係は \\(y=-x+5\\)である。このことからも、相関係数が線形関数の傾きや切片についての情報は何も持たないことがわかる。なお、geom_point() は散布図を描くための関数であり、geom_smooth() は引数method で指定する方法で、データに関する近似線を描画するための関数である。ここでは、method = lm として、最小二乗法で求めた直線を描画している。 cor(X$x1, X$y1) ## [1] -1 ggplot(data = X, mapping = aes(x = x1, y = y1)) + geom_point() + geom_smooth(method = lm) また我々は、二変数間の相関係数がゼロであることが、両者が無関係であることを意味しないことにも注意をしなければならない。例えば、以下のようなデータセットにおけるA と B の相関は 0 になる。 ## # A tibble: 5 × 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -2 4 ## 2 -1 1 ## 3 0 0 ## 4 1 1 ## 5 2 4 cor(AB$A, AB$B) ## [1] 0 しかしながら、両者の関係を描画すると、\\(y = x^2\\) という二次関数の関係にあることがわかる。つまり、相関係数がゼロだからといって、二つの変数間に関係がないと結論づける事はできず、相関ではなく異なる複数の分析アプローチによって関係を特定していくことが必要になる。 ggplot(data = AB, mapping = aes(x = A, y = B)) + geom_point() + geom_smooth(method = lm, formula = y ~ x + I(x^2), se = FALSE) 二変数間の関係についての可視化もggplot2にて対応できる。具体的には、geom_point()という関数を用いるのだが、mappingに対する引数として、x と y 二つの変数を指定することが必要になる。ダイアモンドの価格は、カラット数に大きく依存すると考えられる。そこで、以下のようにカラット数と価格との間の共分散と相関係数を計算する。 cov(diamonds$carat,diamonds$price) ## [1] 1742.765 cor(diamonds$carat,diamonds$price) ## [1] 0.9215913 これらの変数間の相関係数は約0.92であり、高い正の相関関係であることが確認された。続いて、これらの変数の関係を可視化する。二変数間の関係を端的に可視化する方法が散布図である。散布図は、一方の変数を横軸に、もう一方の変数を縦軸に取り、各データのそれぞれの値の組み合わせをプロットしたものである。 p3 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price)) p3 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図1: カラット：価格&quot;) 研究目的によっては、二つの変数間の関係をカテゴリごとに比較したい場合もあるだろう。例えば、我々はカラットと価格の関係は、カットの質によって変わるのか、という問いに関心があるとしよう。その場合には、(1) 同一図内にてカテゴリごとに色分けする方法と、(2) カテゴリごとに分割して図示化する方法がある。それぞれのggplot2での実行方法は、以下のとおりである。 Mapping = aes() 内に、 color = categ_varと指定することで、categ_var変数のカテゴリに基づき色分けする。 facet_grid() や facet_wrap() を用いる。 まず、(1) の図内での色分け方法は、以下のようなコマンドで実行できる。 p4 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price, color = cut)) p4 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, color = &quot;カット&quot;, title = &quot;散布図 2: カット別、カラット：価格&quot;) このように、mapping = aes() 内にて色付けに関する引数を設定することで散布図内の観測値を色分けできる。ただし、ここで重要なのは、color =という引数では、カテゴリ変数を指定すべきであり、色そのもの（例えば、redやblue）を指定するものではないということである。しかしながら、散布図 2のように多くのカテゴリが含まれる場合にはこの可視化の方法適さないかもしれない。そこで、以下の方法を紹介する。facet_wrap() を用いた図の作成では、散布図 2のように color 引数を指定する必要はなく、p3 を再利用できる。facet_wrap()は、関数内で指定した変数（~cut）に基づいて図を分けて描画するように指示ができる。geom_point() で散布図作成の指示を与えたあとに、facet_wrap() のレイヤーを重ねる指示を与えれば、散布図 3が作成される。 p3 + geom_point() + facet_wrap(~cut) + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図 3: カット別、カラット：価格&quot;) 散布図 3をみると、基本的にはカラット数と価格には正の相関があるものの、カットの質が低い（例、Fair）場合にはばらつきが大きいことがうかがえる。 これまでに学んだdplyrによるデータ処理方法をパイプ演算子でつなげることで、特定の群のみを対象にした図示化も容易になる。ここでは例として、1.00カラット以上と未満とで分けて、それぞれのヒストグラムを作成してみる。 p5 &lt;- diamonds %&gt;% filter(carat &gt;= 1.0) %&gt;% ggplot(mapping = aes(x = price)) p5 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット以上&quot;) p6 &lt;- diamonds %&gt;% filter(carat &lt; 1.0) %&gt;% ggplot(mapping = aes(x = price)) p6 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット未満&quot;) Rで図を作成したら保存（出力）したいと考えることも多いだろう。日本語を使っていない図はggsaveを使い簡単に保存できる。具体的には、まず、作成した図そのもの（図示化のためのggplot() オブジェクトではない）をオブジェクトとして定義（例、plot1）する。ggsaveの使用例は以下の様になる (以下は見本コード)。 ggsave(filename = &quot;plot1.pdf&quot;, plot = plot1, width = 10, height = 5, units = &quot;cm&quot;) 日本語を含む頭の場合、quartz() を用いた以下の手順を経て図を保存する。 1. quartz()で作図デバイスを起動する。 2. 作図デバイスを開いたまま、Rstudio内で図を表示する。 3. dev.off()という指示で作図デバイスを閉じることで図が保存される。 また、Rstudio内のplotタブから、クリック-バイ-クリックで実行することも可能である（Export -&gt; Save as Image/ Save as PDF -&gt; Directory -&gt; File name）。 なお、\\(\\rho\\)はローと読む↩︎ "],["ch3reference.html", "参考文献", " 参考文献 倉田博史・星野崇宏（2011）「入門統計解析」、新世社. 高橋将宜・渡辺美智子 (2017). 「欠測データ処理」, 共立出版. 松村優哉・湯谷啓明・紀ノ定保礼・前田和寛（2021）「改訂2版 RユーザのためのRStudio[実践]入門〜tidyverseによるモダンな分析フローの世界」，技術評論社. Healy, Kieran (2018) Data Visualization: A Practical Introduction, Princeton University Press. "],["test.html", "基礎統計学復習 ", " 基礎統計学復習 "],["ch4Intro.html", "本章の概要", " 本章の概要 本章では、基礎的な統計学と、統計的推定・検定について説明する。前章では、データを集めて分析を行うことで、ある変数の平均値などの統計量を計算できることを学んだ。しかし、その計算された数値にどのような意味があるのかを解釈するのが難しい場合もある。例えば、ある変数の平均値を異なるグループ（例えば、性別）それぞれで計算したとする。このとき、ほとんどの場合においてグループ間で同じ値を取ることはないと考えられる。しかしながら、この違う値が誤差の範囲なのか、意味のある（誤差を考慮しても無視できないほど大きな）差なのかについて検討することは重要である。このような目的を達成するために利用されるのが、、統計的な分析（区間推定や検定）である。 Rを用いて、統計的な分析（区間推定や検定）を実行すること自体はさほど難しくない。基本的な分析に必要な関数は基本パッケージに搭載されており、コードの書き方（引数の設定など）もネット上で検索すれば容易に知ることができる。しかしながら、自身もしくは他者が実行した分析をきちんと理解するためには、基礎的な統計学の内容を理解している必要がある。本章では、基礎統計学に関する説明を提示する。なお、本書で紹介する内容はあくまで簡易的な内容であるため、統計学を未習の場合は基礎統計学の図書を用いて学習することを強く推奨する。また、章末に統計学や計量経済学の学習に役立つ参考文献を提示しているので、各自の学習に役立ててほしい。 本章で扱う内容は主に、標本から得た情報に基づき母集団の性質について推測するアプローチを想定したものである。具体的には、以下の内容を含める： 確率モデルと期待値・分散 統計的推測と点推定・区間推定 統計的仮説検定 平均値に関する検定・分散分析 検定力分析とサンプルサイズ 本章の後半では様々な検定方法を紹介するが、本書ではこれらの分析手法について記憶するよりも、マーケティング領域の学生において特に誤解の多い統計的分析に関する基本的な性質について理解してほしいと考えている。第一に、確率変数によって定義された統計量もまた確率変数であるという点である。データを収集し、分析を実行し統計量を計算すると、自身が観察した値が唯一の値であると考えてしまうかもしれない。しかし、母集団とは確率分布であり、標本はその確率分布に従う確率変数、データはその確率変数の実現値だと解釈できる（倉田・星野、2024）。そのため、自身が観察した推定値もまた確率変数の実現値であるということを理解することが大切である。 第二に、信頼区間についてである。統計的な分析では、信頼区間の計算を行うことも多い。しかしながら、信頼区間は誤解のもとで解釈される場合も多いため、注意が必要である。信頼区間は、未知パラメータ（母平均等）を一定の確率（信頼水準）で含む区間を示す。信頼区間には確率を割り当て、例えば95%信頼区間のようなものを計算するのだが、よくある誤った解釈として「母集団の期待値（未知パラメータ）は95%の確率でxx以上、yy以下という区間に含まれる」というものである。基礎的な統計学の枠組みでは、未知パラメータは確率変数ではない。そのため、未知パラメータに確率を割り当てるような表現は誤りである。確率変数であるのは、推定された区間の両端である。 第三に、統計的仮説検定についてである。統計的仮説検定ではまず、母集団の統計的特徴に関する記述である「帰無仮説」と「対立仮説」を定める必要がある。特に帰無仮説は検定における分析や考察の基準となり、帰無仮説を棄却するか否かによって仮説検定の判断を行う。通常、「棄却する」という言葉は帰無仮説に対してのみ用いるため、論文の中で利用している理論仮説や作業仮説に対しては使わない。また、統計的仮説検定では、帰無仮説が真であるにもかかわらず帰無仮説を棄却してしまう第一種の誤りと、帰無仮説が真ではないにもかかわらず、帰無仮説を採択してしまう第二種の誤りとが存在する。第一種の誤りを起こす確率は有意水準という。統計的仮説検定では、事前に設定した有意水準の分だけ（慣習として5%や1%といった基準が採用される）第一種の誤りを起こす確率を許容したうえで帰無仮説が正しいか否かを判断する。一方で、「帰無仮説を棄却できない」という結果を得た際には注意が必要である。具体的には、帰無仮説を棄却できない（統計的に有意でない）からと言って、帰無仮説が正しいと結論づけることはできない。そのため例えば、統計的に有意でない結果をもとに「〇〇は××に影響がない（もしくは、差がない）ことが明らかになった」という解釈を行うことは適切ではない。 第四にp値についての解釈である。Rや他のソフトウェアで統計的検定を実行すると “p-value”（p値）という値を得る。p値にも誤解が蔓延しており、「値が小さければ仮説が真であることを示す指標」や「小さいほど結果の重要性を示す指標」といった解釈を行うべきではない（Baker, 2016）。p値は、帰無仮説が正しいと仮定したときに，手元のデータから計算した検定統計量以上に極端な値を取る確率だと定義できる（豊田, 2017）。そしてp値が有意水準よりも小さい場合には、帰無仮説が間違っていたという判断を下すというのが、p値に基づく帰無仮説棄却の判断である。 本章では、上記の注意点について理解するための統計的な原理原則と、それに対応するRを用いた統計的分析手法を提示する。 "],["probability.html", "確率モデル、期待値と分散", " 確率モデル、期待値と分散 確率と離散確率変数 伝統的なデータ分析では、標本から得た情報に基づき母集団の性質について推測する。このとき、母集団は確率分布、標本はその確率分布に従う確率変数、データはその確率変数の実現値だと考える（倉田・星野、2024）。そのうえで確率とは、起こりうる事象の集合内において、各事象の起こりやすさの度合いを0以上1以下の実数で表したものである。より詳細な定義として、標本空間を \\(\\small \\Omega\\)、任意の事象 A に対して実数 P(A) が定まっていて、以下の三つを満たすとき、P(A)は事象 A の確率という： 確率は非負であり、以下を満たす： \\[0\\leq P(A)\\leq 1\\] 全事象を \\(\\Omega\\)、空事象を \\(\\emptyset\\) とするとこれらの確率は以下の様に示される： \\[P(\\Omega)=1,~~ P(\\emptyset)=0\\] 事象 \\(A_1,A_2,...\\) が互いに排反ならば、これらのうち少なくとも１つが起こる事象 \\(A_1\\cup A_2\\cup ...\\) の確率は以下となる： \\[ P(A_1\\cup A_2\\cup ...)=P(A_1)+P(A_2)+... \\] 確率変数とは、ある標本空間上で定義される取りうる各事象に対してそれぞれ一定の確率と対応関係のあるような変数である。例えば、細工のないサイコロを投げるとき、出た目の値を \\(x\\) とすると、\\(x\\) は1から6までの整数を取りうる変数だと言い換えることができる。この場合標本空間は、取りうる出目に対応した6個の標本点からなる。またこれらの標本点には、それぞれ対応する確率が以下のように付与されている。 Table 1: サイコロの確率分布 x 1 2 3 4 5 6 確率 1/6 1/6 1/6 1/6 1/6 1/6 このように、確率変数の取る値に対応して確率が付与されるルール（ \\(x\\) の関数としての確率 \\(P(x)\\) ）を確率分布や確率分布関数という。 確率変数は主に、離散確率変数と連続確率変数に分けることができる。離散確率変数は、サイコロのように、取りうる値が離散的な確率変数である。一方で、連続確率変数は、ある範囲の中で連続的にどんな値も取りうる確率変数である。離散確率変数では、サイコロの表で示されているように、取りうる特定の値に対応する確率を確率分布に基づき計算できる。 連続確率変数 一方で連続確率変数の場合、取りうる値の数が無限に存在する。例えば、-1 から 1 までの区間を取りうる値の範囲とする連続確率変数があったとする。この変数は例えば、0.90という値を取りうるが、同様に、0.91 や 0.900001 といった値も取りうる。このように、連続確率変数が取りうる値の数は無限に存在するため、取りうる特定の値に対応する確率は0になる。もし取りうる各値に確率が付与されていると、確率の合計が無限大になってしまうという問題に直面する。そのため、連続確率変数の場合、取りうる区間に対して確率が付与される。これを踏まえて連続確率変数を捉え直すと、「連続確率変数はその取りうる任意の区間に対して一定の確率が対応するような変数」だといえる。 また、連続確率変数における取りうる区間の起こりやすさには「確率密度」が対応することで計算可能になる。言い換えると、確率変数 \\(x\\) の値に確率密度がどのように対応するのかという関係は、\\(f(x)\\) という確率密度関数（probability density function: PDF）として示される。PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) が区間 [a, b] を取る確率 \\(P(a\\leq x \\leq b)\\) は、以下の積分計算で求められる。 \\[ P(a\\leq x \\leq b)=\\int^b_a f(x) dx \\] 以下の図はPDFの例であり、図内の曲線はPDFを、灰色に塗られている面積はある区間の確率を示している。なお、上記の式で示されている関係から、PDFを特定（仮定）することで、ある確率に対応する区間の両端 a, b を求めることも可能である。以降の節で紹介する統計的分析では、この関係を用いて分析することもあるが、詳しくは後述する。 連続確率変数例 連続確率変数を用いた具体的な確率計算例を紹介するために、ここでは一様分布（uniform distribution）を用いる。区間 [a, b] を持つ一様分布に従う確率変数 \\(x\\) のPDFは以下のように示される。 \\[ f(x) = \\begin{cases} \\frac{1}{b-a} &amp; a\\leq x \\leq b\\\\ 0 &amp; otherwise \\end{cases} \\] 具体的な計算を実行するために、ここでは、-1から3までの区間を持つ一様分布を考える。この一様分布のPDFは、\\(\\small f(x)=\\frac{1}{4}~for~-1\\leq x\\leq 3\\)（その他の区間の確率は0）となる。このとき、\\(x\\) が区間 [0, 2] を取る確率は、以下のように求められる。 \\[ P(0\\leq x \\leq 2) = \\int^2_0 \\frac{1}{4} dx=\\left[\\frac{x}{4}\\right]^2_0=\\frac{1}{2}-0=\\frac{1}{2} \\] 期待値 期待値とは確率の考え方を含む理論的な平均値（\\(\\mu\\)）といえる。確率分布 \\(P(x)\\) を持つ離散確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\sum_x x \\cdot P(x)=\\mu \\] 一方、PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\int_{-\\infty}^\\infty x \\cdot f(x) dx=\\mu \\] この定義に基づき、先程の区間 [-1, 3] を持つ一様分布の期待値を以下のように求める。 \\[ E(x)= \\int^3_{-1} \\frac{x}{4} dx=\\left[\\frac{x^2}{8}\\right]_{-1}^3=\\frac{9-1}{8}=1 \\] 期待値 \\(E(x)\\) は一般的に、\\(a\\) を定数、\\(g(x)\\) と \\(h(x)\\) を \\(x\\) の関数とするとき、以下が成り立つ： \\(E(a) = a\\) \\(E[a\\cdot g(x)]=a\\cdot E[g(x)]\\) \\(E[g(x)+h(x)]=E[g(x)]+E[h(x)]\\) これらの性質により、確率変数 \\(x\\) の分散（\\(\\sigma^2\\)）は以下のように求まる。つまり、分散は二乗の期待値から期待値の二乗を引くことで計算できる。 \\[ \\sigma^2=E\\left[\\bigl(x-E(x)\\bigr)^2\\right]=E\\left[(x-\\mu)^2\\right]=E(x^2)-E(x)^2 \\] "],["inference.html", "統計的推測", " 統計的推測 前節で述べた通り、我々は研究の対象となる集団全体ではなく、その一部から情報を取得し分析を行う。このとき、その集団全体を母集団、母集団から抽出した一部を標本と呼ぶ。統計的な分析においては、確率分布を用いて母集団をモデル化し、標本をその確率分布に従う確率変数とみなすことで母集団と標本の関係を捉える。そのため、データ分析は標本を対象とするものの、分析者の関心は、母集団の特徴である母数（parameter）についてであることが多い。母集団の平均（\\(\\mu\\)）や分散（\\(\\sigma^2\\)）は母数の代表例である。しかし、母数は通常未知であり直接知ることはできないため、標本の情報を用いて母集団の特徴について推測する。このプロセスを統計的推測と呼ぶ。統計的推測を行うためには、原則として母集団からの無作為標本抽出（random sampling）が必要になる。統計的推測では、互いに独立に同一の分布に従う（Identically Independently Distributed: IID）ような標本が好ましく、無作為標本は、IIDを満たすことが知られている。 統計的推測においては、「推定」「統計量」「推定量」「推定値」などの似たような言葉が利用されるが、これらはそれぞれ異なる意味を持つ。推定とは、標本の情報に基づき母数について把握しようとする作業そのものを示す。一方で、標本として得られるデータに基づき計算できる値（計算式で示されることも多い）を一般的に統計量というのだが、その中でも特に推定に用いる統計量を推定量という。そして推定値は、推定量についてデータから求めた実際の計算値を表す。 また、推定には「点推定」と「区間推定」がある。点推定とは、未知の母数について１つの数値に基づいて推定する方法である。例えば、標本平均は母平均（\\(\\mu\\)）を点推定するための推定量である。一方で区間推定は、未知の母数を一定の確率で含む区間を推定する方法である。これは、点推定では捉えきれない統計的誤差を考慮して区間を推定する方法であり、母平均の信頼区間の測定は区間推定の代表例である。 "],["pestimate.html", "点推定", " 点推定 点推定は特定の推定量によって母数を捉えようとするが、どのような推定量を用いるべきなのだろうか。本節では、不偏性（unbiasedness）、一致性（consistency）、効率性（efficiency）という統計的に重要な推定量の性質について説明する。なお、以下の説明では、未知パラメータ \\(\\small \\theta\\)（シータ）に対する推定量 \\(\\small \\hat{\\theta}\\)（シータハット）を考える。不偏性とは、推定量の「期待値」が未知パラメータの真の値に等しいという性質であり、以下のように示すことができる。 \\[ E(\\hat{\\theta})=\\theta \\] つまり、実際の推定量の実現値がどうかは置いておいて、期待値の下では推定量が未知パラメータを示していることを表すものであり、サンプルサイズに関係のない推定量の性質である。そして、不偏性を満たす推定量のことを不偏推定量（unbiased estimator）という。なお、上記の定義から、統計的なバイアス（B）は、以下のように定義できる。 \\[ B=E(\\hat{\\theta})-\\theta \\] 第二に一致性とは、サンプルサイズが十分に大きいとき、推定量が未知パラメータの真の値と等しくなる確率が1に近づくという性質である。この性質について詳しく論じるには、漸近理論を学ぶ必要があるため、本書では詳細を省略するが、サンプルサイズを大きくすると未知パラメータの真の値に近づくような推定量を示した性質だと解釈できる。なお、任意の\\(\\small \\epsilon &gt;0\\)（\\(\\small \\epsilon\\): イプシロン）に対して以下のような性質を持つ推定量を一致推定量という。 \\[ \\lim_{n\\rightarrow \\infty} P\\left(|\\hat{\\theta}-\\theta|\\leq \\epsilon \\right)=1 \\] 第三に効率性は、推定量の分散の小ささを示している。分散の小さい推定量の方が、期待値から離れた値を取りにくく、好ましい推定量と考えられる。複数の不偏推定量や一致推定量がある場合、効率性をもとに好ましい推定量を考える。 なお、代表的な推定量である標本平均は母集団期待値の推定量として好ましい性質（不偏性と一致性）も持っている。以下では、期待値 \\(\\mu\\)、分散 \\(\\sigma^2\\) の確率分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\)を考える（つまり、\\(\\small E(X)=\\mu\\), \\(\\small Var(X)=\\sigma^2\\)）。このとき、標本平均（\\(\\small \\bar{X}\\)）の不偏性は以下のように示すことができる。 \\[ E(\\bar{X})= \\left[\\frac{1}{n}(X_1+X_2+...+X_n)\\right] = \\frac{1}{n}~\\left[E(X_1)+E(X_2)+...+E(X_n)\\right] = \\frac{1}{n}\\cdot n\\mu=\\mu. \\] また標本平均の分散については、以下となることが知られている（計算は省略）。 \\[ Var(\\bar{X})=\\frac{\\sigma^2}{n} \\] 上記と同様の無作為標本による標本平均の一致性については、任意の \\(\\small \\epsilon&gt;0\\) に対していかが成り立つことが知られている。 \\[ \\lim_{n\\rightarrow \\infty}P(|\\bar{X}-\\mu|\\leq \\epsilon)=1 \\] 言い換えると、サンプルサイズが増えることで標本平均 \\(\\small\\bar{X}\\) は母集団の真の平均 \\(\\small \\mu\\) と等しくなる確率が1に近づく。なお、標本平均がもつこの特性は「大数の法則（Law of Large Number）」として知られている。 また、標本平均はその分布の収束に関しても重要な特性を持っている。ここで、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) を持つ確率分布に従う母集団からのn個の無作為標本 \\(\\small X_1,.., X_n\\) を考える。サンプルサイズが十分に大きい場合、 \\(\\small \\bar{X}\\sim N(\\mu,\\sigma^2/n)\\)（\\(\\small \\bar{X}\\) が平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布に従う）となることが知られている。この性質を「中心極限定理（Central Limit Theorem）」というが、詳細な証明や定義は省略するので、文末の参考文献を参照してほしい（例えば、岩田, 1996; 倉田・星野, 2024; 宮川, 2002）。また、この定理を以下のような\\(\\small \\bar{X}\\) を標準化した確率変数に応用することも可能である。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] このとき、中心極限定理よりサンプルサイズが十分に大きい場合には Z の分布関数は標準正規分布（N(0,1)）の分布関数に収束する。詳細については割愛するが、サンプルサイズが十分に大きい場合、「標本平均」や「標本平均を標準化した確率変数」の確率分布が正規分布や標準正規分布に近似できるという定理は、統計的な推定や検定において重要なものである。 また、母集団分散の推定量としては、不偏標本分散が使われる事が多い。上記と同じ無作為標本に対し、標本分散 \\(S^2\\) と、不偏標本分散 \\(s^2\\) は以下のように定義される。 \\[ S^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X}) \\] \\[ s^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X}) \\] そして、それぞれの推定量の期待値は以下のようになることが知られている（計算省略）。そのため、母集団分散の推定量として、不偏標本分散（\\(s^2\\)）が用いられる。 \\[ E(S^2)=\\frac{n-1}{n}\\sigma^2 \\] \\[ E(s^2)=\\sigma^2 \\] "],["estimates.html", "推定量もまた確率変数", " 推定量もまた確率変数 本節では、推定値と母数との関係を標本平均（\\(\\small \\bar{X}\\)）を使って説明する。ある母集団からランダムサンプルを収集し、標本平均を計算することを考える。ここで計算された数値は真の母平均を捉えた唯一の値なのだろうか。 結論としては、点推定の推定値は母数そのものではなく、ひとつのある実現値でしかないことに注意が必要である。この理由は、「確率変数から計算される推定量もまた確率変数である」という事実から理解することができる。 例えば、我々が神戸大学経営学部生の一ヶ月あたりの平均収入（仕送りは除く）に関心があるとする。このとき（実現可能性は置いておいて）、経営学部全体を母集団とする無作為標本を100件収集し、標本平均を計算した結果 \\(\\small \\bar{X}=\\) 0 だったとする。もしこのような極端な結果を得た場合、多くの人が「標本平均の実現値は必ずしも真の母平均そのものではない」という説明に納得がいくだろう。同様の調査（100件のランダムサンプリング）をもう一度行い平均収入を計算し直すと、おそらく0とは異なる推定値を得る可能性が高い。仮に、\\(\\small\\bar{X}=\\) 50,000 だった場合、その結果をどのように解釈するだろうか。仕送りを除く大学生の月当たり収入の平均が5万円だという結果はなんとも尤もらしいと感じるかもしれない。しかしながら、たとえ尤もらしい結果を得たとしても、それはひとつの分析結果であり、真の母平均を示す唯一の値ではない。その点については、平均収入が0という結果も5万円という結果も同様に注意が必要である。 確率変数から計算される推定量もまた確率変数であるという点をさらに直感的に経験するために、細工のない6面サイコロを（バーチャルに）振ってもらう。なお、コード内では、出力結果を少し見やすくするために、knitr というパッケージを利用している。同じように作業を行いたい場合には、以下のようにインストールしてほしい。 install.packages(&quot;knitr&quot;) 標本平均についての議論を行う前に、理論的な期待値を求める。6面サイコロの出目の期待値 \\(\\mu\\) は以下の通りである。 \\[ \\mu = 1\\cdot \\frac{1}{6}+2\\cdot \\frac{1}{6}+...+6\\cdot \\frac{1}{6}=3.5 \\] ここで、以下のようなコマンドを用いてR内でサイコロを振ってみてほしい（実際にサイコロを振ってもらっても構わないが、面倒くさい）。sample() 関数は指定した標本空間からランダムに size の数だけサンプルを抽出するための関数である。replace = TRUE は、復元を認めるという指示になっている。なお、ここでは、乱数の再現性確保のために、set.seed()を用いている。set.seed()のカッコの中に数値を入力することで、その数値を使う場合には常に同じ乱数を発生させることが可能になり、乱数の固定と再現性の確保ができる。そのため、読者においては、set.seed()内に異なる値を入れてもらって構わない。 set.seed(442)# 乱数の再現性確保のための指示。関数内の数字に特に意味はないため各自別の値を使っても良い。 die &lt;- 1:6 d &lt;- sample(die,size=1,replace = TRUE) d ## [1] 6 本書内では以上で示されている通り、6という出目を得た。上記のコマンドを実施した各自がそれぞれ異なる値を得ているだろう。ここで得た6という数字は、サイコロの出目という確率変数の実現値（\\(n=1\\)）である。そのため、本データの標本平均も6であり、真の期待値とは異なる。ただし、読者によっては1件の標本による標本平均という表現を直感的に理解しにくいかもしれない。そのため、以下のように サイコロを10回振る試行を3回実施し、各サンプリング結果に基づき標本平均を以下のように計算する。 set.seed(352) d1 &lt;- sample(die,size=10,replace = TRUE) d2 &lt;- sample(die,size=10,replace = TRUE) d3 &lt;- sample(die,size=10,replace = TRUE) d_mean &lt;- matrix(c(mean(d1),mean(d2),mean(d3)),nrow = 1) colnames(d_mean) &lt;- c(&quot;d1の平均&quot;, &quot;d2の平均&quot;, &quot;d3の平均&quot;) knitr::kable(d_mean, caption = &quot;サイコロの標本平均比較&quot;, align = &quot;ccc&quot;) Table 2: サイコロの標本平均比較 d1の平均 d2の平均 d3の平均 3.2 3 2.4 上記の通り、d1, d2, d3 いずれの標本平均も互いに異なるものであり、また3.5とも異なる。このことからも、確率変数（サイコロの出目）を用いて計算された推定値（標本平均）もまた確率変数であり、推定値と未知パラメータとの間にはズレ（誤差）が生じうることがわかる。なお、なかには3.5と等しい標本平均を偶然得た読者もいると考えられるが、それもあくまで一つの実現値である。 では、標本平均の推定値がサンプルサイズによってどれだけ真の期待値に近づくのかについて、サイコロの試行回数を10回、100回、1,000回と増やして確認する。以下の結果を見ると、サンプルサイズ（試行回数）が増えるごとに真の期待値に近づいていることが伺える。ただし、これらの結果もあくまで確率的な試行結果の実現値である。そのため、読者によっては異なる傾向を示すような結果を得る可能性があることに注意が必要である。 set.seed(541) d10 &lt;- sample(die,size=10,replace = TRUE) d100 &lt;- sample(die,size=100,replace = TRUE) d1000 &lt;- sample(die,size=1000,replace = TRUE) d_lln &lt;- matrix(c(mean(d10),mean(d100),mean(d1000)),nrow = 1) colnames(d_lln) &lt;- c(&quot;10回試行の平均&quot;, &quot;100回試行の平均&quot;, &quot;1,000回試行の平均&quot;) knitr::kable(d_lln, caption = &quot;サイコロの標本平均比較２&quot;, align = &quot;ccc&quot;) Table 3: サイコロの標本平均比較２ 10回試行の平均 100回試行の平均 1,000回試行の平均 3 3.36 3.516 "],["distribution.html", "補足（いくつかの確率分布の関係性）", " 補足（いくつかの確率分布の関係性） 統計的な分析の際によく用いられる確率分布として、正規分布、カイ二乗分布、t分布、F分布間の関係性について簡単に紹介する。なお、各分布の確率密度関数などは記載しないため、関心のある読者は参考文献を参照してほしい。 正規分布 正規分布は、様々な分布の基準として用いられる重要な分布である。期待値を中心に左右対称であり「ベルカーブ」と言われる形状を持つ。また、平均0、分散1の正規分布は特に標準正規分布と言われ、正規母集団からの無作為標本の標本平均等の分布を特定する際などに用いられる。 カイ二乗分布 標準正規分布からの無作為標本の二乗和はカイ二乗分布に従う。カイ二乗分布は、正規母集団からの無作為標本の不偏標本分散の分布を特定する際などに用いられる。 t分布 標準正規分布とカイ二乗分布の比はt分布に従う。t分布は、正規母集団からの無作為標本による標本平均と不偏標本分散の比の分布を特定する際などに用いられる。 F分布 カイ二乗分布の比はF分布に従う。F分布は、異なる正規母集団からの無作為標本の不偏標本分散の比の分布を特定する際などに用いられる。 "],["intinf.html", "区間推定", " 区間推定 点推定の節で示した通り、推定値と未知パラメータの間には、ずれ（誤差）がある。標本平均の様に好ましい性質（不偏性や一致性）を持つ推定量であっても、計算の結果示された一つの推定値がどの程度信頼できるものなのかはわからない。そこで区間推定という方法を用いて、統計的な誤差を加味した母数への検討を試みる。代表的な区間推定の方法として、「信頼水準zz%で、xx以上、yy以下という区間は真の母数を含む」という区間[xx, yy]を調べるものがある。このように求められた区間は信頼区間（confidence interval）と呼ばれ、多くの統計分析において用いられている。 では、ここで示された区間がどのように計算され、どのようなことを意味するのだろうか。以下では、もう少し一般的な形で信頼区間の導出や解釈を説明する。 標準正規分布に基づく信頼区間の求め方* はじめに、標準正規分布に基づくある区間の確率の求め方を説明する。 \\(Z_1,Z_2,...,Z_n\\) は、N(0, 1) （標準正規分布）に従う母集団からの無作為標本とする。このとき、標準正規分布がある区間 [\\(-\\infty,~z_\\alpha\\)] をとる確率は、以下の式および図のように示すことができる6。なお、\\(\\small z_\\alpha\\) は、確率\\(\\small \\alpha\\) に対応する標準正規分布上の上側確率 \\(\\small \\alpha\\) 点とする。このとき、この分布における \\(\\small z_\\alpha\\) 以上の範囲を取る確率は\\(\\small \\alpha\\) である。そのため確率の定義より、 \\(\\small z_\\alpha\\) 以下以上の範囲を取る確率は 1 \\(\\small -\\alpha\\)である。 \\[ P(Z\\leq z_\\alpha) = \\int^{z_\\alpha}_{-\\infty}~f(z)~dz =\\int^{z_\\alpha}_{-\\infty}~\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right)~ dz = 1-\\alpha \\] 標準正規分布と確率計算 同様に、以下のような関係も捉えることができる。この場合、斜線部で示されている範囲の確率は両側合わせて \\(\\small \\alpha\\) であり、その内側の確率は 1 \\(\\small - \\alpha\\) である。 \\[ P(-z_{\\alpha/2} \\leq Z\\leq z_{\\alpha/2}) = \\int^{z_{\\alpha/2}}_{-z_{\\alpha/2}}~f(z)~dz = 1-\\alpha \\] 標準正規分布と両側確率 上述の関係を、区間推定に応用するために、あるデータの標本平均に関する議論を捉える。標準正規分布に従う確率変数は、正規分布に従う確率変数を標準化することで得ることができる。ここで、\\(\\small X_1,...,X_n\\) を、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) の正規分布に従う母集団からの無作為標本とする。これまで学んだ標準化および標本平均の特性から、以下の通り、標本平均を標準化したものは標準正規分布に従うことがわかる。 \\[ \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] 先述の標準正規分布における確率計算の関係を応用し、以下を得る。 \\[ P\\left(-z_{\\alpha/2}\\leq \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\leq z_{\\alpha/2}\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\right)=1-\\alpha \\] したがって、区間 [\\(\\small \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n}\\)] は、確率 \\(\\small 1-\\alpha\\) で未知の母平均 \\(\\small \\mu\\) を含むと解釈できる。また、上記の関係から任意の確率 \\(\\alpha\\) を指定することで、区間の上限と下限（\\(\\pm z_{\\alpha/2}\\)）の具体的な値を（統計学のテキスト巻末などに記載されている）標準正規分布表などから求めることができる。 そして、このような区間を「信頼区間」といい、信頼区間の計算において仮定された確率 \\(\\small 1-\\alpha\\) を「信頼水準」もしくは「信頼係数」という。信頼係数は、信頼区間の計算のために研究者によって事前に選択される。慣習としては、90%, 95%や99% (\\(\\small \\alpha =\\) 0.10, 0.05, 0.01)を用いる事が多い。なお、信頼係数を大きくすると、信頼区間も広くなる。 t分布を使った区間推定* 上記の区間推定は母分散 \\(\\sigma^2\\) が既知である場合に計算可能であるが、多くの場合母分散は未知である。そのような場合には、自由度 n-1 のt分布を用いて、両端の確率 \\(\\small \\alpha\\) 点を \\(\\small t_{\\alpha/2}(n-1)\\) とする信頼区間を求める。\\(N(\\mu,\\sigma^2)\\) に従う母集団からの無作為標本を考えるが、今回は母分散が未知である場合を仮定する。このような場合は、母分散 \\(\\small \\sigma^2\\) のかわりに母分散の不偏推定量である不偏標本分散 \\(\\small s^2\\) を用いた以下の統計量 t をもとに信頼区間を計算する。このとき、統計量 t は自由度 n-1 の t 分布に従うことが知られている（t分布に関する詳細および証明は省略）。 \\[ t=\\frac{\\bar{X}-\\mu}{\\sqrt{s^2/n}}\\sim t(n-1) \\] ここで、先述の標準正規分布に基づくある区間の確率計算と同様の計算を、自由度 \\(\\small n-1\\) のt分布に基づき実行すると、以下のような確率と区間の関係に書き換えることができる。 \\[ P\\left(-t_{\\alpha/2}(n-1)\\leq \\frac{\\bar{X}-\\mu}{s^2/n}\\leq t_{\\alpha/2}(n-1)\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\right)=1-\\alpha \\] 信頼区間を求めるための手順は標準正規分布の場合もt分布の場合も同様だが、標準正規分布のかわりに t 分布を用いた場合、特定の確率に対応する閾値が変わることが知られている（\\(\\small z_{\\alpha/2}\\neq t_{\\alpha/2}(n-1)\\)）。 t分布は、左右対称であり標準正規分布よりもテールが厚いという特徴を持つが、自由度が大きくなると正規分布に近づくことが知られている。標準正規分布と、自由度の異なる t 分布との関係は以下のように図示化できる。自由度（df）3の t 分布よりも自由度20の t 分布のほうが標準正規分布に近い形状であることが伺える。 標準正規分布と t 分布によってある確率に対応する閾値が異なるということは、ある信頼水準に対応する信頼区間も仮定する分布によって異なるということである。 Rによる区間推定 Rで信頼区間を求めること自体は難しくない。最も手間のかからない方法としては、t.test() の分析結果を用いて、conf.int()によって信頼区間が計算できる。信頼区間の計算を実行するために、以下の例を考える。ある製品（電球）の寿命は平均1800（時間）である。企業は性能を改良するために新型の電球が開発したが、新型化に伴い製品寿命も変化したのかについては不明である。ただし、この製品の寿命は新型も旧型のものも正規分布に従い、その標準偏差は \\(\\sigma=\\) 180（時間）であると仮定する。 工場で生産された新型製品を16個無作為に選びその寿命を計測した所、以下の結果を得た。 1939.6 1680.3 1982.1 2215.6 2092.5 1928.9 2003.8 1955.5 1800.1 1659.5 2066.2 2107.2 2085.5 1878.6 2007.6 1816.1 このデータは、平均が \\(\\small \\mu\\)、分散が \\(\\small 180^2\\) である正規分布（\\(\\small N(\\mu, \\sigma^2=180^2)\\) と表記する）からの無作為標本 \\(X_1,..., X_{16}\\)の実現値とみなすことができる。なお、このデータの標本平均は1,951（時間）、不偏標本標準偏差は 154.46 である。このデータに基づく、新型電球寿命の期待値に関する95% 信頼区間（95%の確率で真の母数を含む区間）はt.test() 関数を用いると以下の様に求まる（ただし後述するが、この方法はこの例に対しては適切ではない）。 bulb &lt;- c( 1939.6, 1680.3, 1982.1, 2215.6, 2092.5, 1928.9, 2003.8, 1955.5, 1800.1, 1659.5, 2066.2, 2107.2, 2085.5, 1878.6, 2007.6, 1816.1) bulb_ci &lt;- t.test(bulb) #t検定の実施と格納 bulb_ci$conf.int #信頼区間の出力（デフォルトで95%信頼水準） ## [1] 1868.890 2033.498 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 出力されている 1868.890 2033.498 が信頼区間、attr(,\"conf.level\") 0.95 が今回計算に用いられた信頼水準（confidence level）（もしくは信頼係数（confidence coefficient）ともいう）である。 分析の結果、95%の確率で真の新製品寿命期待値がおよそ 1868.9 から 2033.5 の間に含まれることがわかった。したがって、どうやら新製品寿命は平均的に旧型製品（1,800）よりも長そうである。 しかしながら、新型電球の例では分散が既知の正規分布からの標本を仮定しているため、t分布を仮定するよりも標準正規分布により信頼区間を求めるほうが好ましい。 先述の通り、標準正規分布に基づく信頼区間は以下のように示すことができる。 \\[ \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n} \\] このとき、仮定より \\(\\small \\bar{X}=1951.194\\), \\(\\sigma=180\\) であることがわかっている。また、慣習より95%信頼水準を仮定すると、 \\(\\small \\alpha = 0.05\\)となる。そのため、区間推定の計算で必要な要素のうち現時点で不明なのは、 \\(z_{\\alpha/2}=z_{0.025}\\)の値である。この値は、任意の確率に対応する区間の閾値を表している。今回の場合、分布が左右対称の分布であり正負どちらか一方の値さえ分かればよいため、閾値（\\(z_{0.025}\\)）以上の区間を取る確率が2.5%になるような閾値に着目する。このような閾値は、Rによって以下のように求める（なお先述の通り、統計学教科書に掲載されている分布表を使っても同様の数値を求めることができる）。qnorm は、標準正規分布に従う確率変数について、引数で定義した確率に対応する境界値（分位点: quantile）を求める 関数である。 qnorm(0.025, lower.tail=FALSE) #標準正規分布における上側2.5%点の分位点を求める。 ## [1] 1.959964 これにより、計算に必要な情報が揃ったため、以下の要領で信頼区間を出力できる。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 #信頼区間の計算 upper &lt;- xbar+z*(sigma/sqrt(n)) lower &lt;- xbar-z*(sigma/sqrt(n)) #結果のまとめと出力 ci.bulb &lt;- matrix(c(lower,upper),nrow=1) colnames(ci.bulb) &lt;- c(&quot;ci.lower&quot;, &quot;ci.upper&quot;) knitr::kable(ci.bulb, caption = &quot;Bulb data CI（95%）&quot;, align = &quot;cc&quot;) (#tab:ci_bulb)Bulb data CI（95%） ci.lower ci.upper 1862.995 2039.392 分析の結果、新型製品の平均寿命に関する95%信頼区間は、標準正規分布と t 分布どちらの分布を仮定しても旧型の平均1800（時間）を含まず、それよりも大きい値を取るものであった。したがって、新型製品は製品寿命の面においても95%の確率で旧型製品よりも優れていると考えられる。 では、この95%信頼区間は、そもそもどのように解釈すべきだろうか。結論から述べると、95%信頼区間の直感的解釈については以下のように説明できる： 「母集団からサンプルを取り平均値の95%信頼区間を構築する」という手順を100回繰り返すと考える。95%という信頼水準（確率）が示していることは、計算された区間が100回に95回は母数を含むということである。言い換えると、今回得た標本平均に基づき計算された信頼区間がはずれ（真の母数を含まない区間）である可能性が5%存在するということである。 95%信頼区間の解釈として、「分析対象としている母数の値がこの区間の値をとる確率が95%である」という旨の説明を行う人がいるが、これは\\(\\color{red}{\\text{誤り}}\\)である。母数は未知だが、何かしらの定まった値なので確率的な議論を母数に用いるのは適切ではない。確率的に変動するのはあくまで区間の両端である点を理解しなければならない。なぜならば、\\(\\small \\bar{X}\\) が確率変数であるため、そこから計算される区間の両端もまた確率変数となるためである（岩田，1996）。ここで示されている信頼水準は、計算された区間が真の母数を含んでいる確率である。つまり、信頼水準は、サンプルを収集し、信頼区間を求めるという「手順そのものに対する信頼度」を表す指標であると解釈できる。 ただし、標準正規分布の確率密度関数 \\(\\small f(z)\\) は、\\(\\small 1/\\sqrt{2\\pi}\\exp\\left(-z^2/2\\right)\\) だと知られている。↩︎ "],["testtheory.html", "統計的仮説検定", " 統計的仮説検定 検定の手順と仮説 データを用いた研究では、統計的分析によって提示した仮説が支持できるか否かを判断したい場合もある。その時に用いられる方法が統計的仮説検定である。本節ではまず検定の手順について説明したあと、分析結果の意味や解釈について説明する。統計的仮説検定は、基本的に以下の手順によって実施される。 仮説（帰無仮説・対立仮説）を設ける。 仮説を検定するための統計量を選ぶ。 有意確率を定め、統計量の値について有意確率に基づく臨界値を設定する。 帰無仮説が正しいと仮定した上で統計量を計算し、その値が棄却域と採択域のどちらの領域に入るかを分析する。 統計的仮説検定でも「仮説」という言葉が重要な役割を占める。ただし、ここで重要になるのは帰無仮説と対立仮説である。用語による誤解を避け、理解を容易にするために、??章で議論した研究上着目する変数間の関係に関する予測（仮説）を「作業仮説」と呼ぶことにする。作業仮説は、リサーチクエスチョンに答えるための論理的予測である。例えば、「女性に比べ男性の方が新製品購買意図が高い」のようなものが作業仮説である。このような仮説を検証する場合、男女（グループ）間で購買意図の平均値を比較することが現実的な分析方法として考えられる。 帰無仮説と対立仮説は、統計的仮説検定の基準になる「母集団の統計的特徴に関する仮説」であり、検定という手続き上ではこれらの仮説に着目する。特に、帰無仮説は、統計的仮説検定の考察、分析の基準となる仮説であり、この仮説を棄却（否定）できるか否かを調べることが基本的な統計的仮説検定の枠組みだと言える。帰無仮説は棄却しうる仮説であり、\\(\\small H_0\\) という記号で表される事が多い。また、多くの場合において「差がない」「効果がない（0である）」や「特定の値と等しい」といった仮説が設計される。一方で対立仮説は、帰無仮説とは排反な仮説で、帰無仮説が棄却された際に採用される推測であり、\\(\\small H_1\\)や\\(\\small H_a\\)という記号で表される。データ分析を用いた研究においては対立仮説と作業仮説は論理的に整合的ないしは等しいことが好ましい。つまり、作業仮説という研究上重要な論理的推測を統計的に検証するためには、その作業仮説とは排反な帰無仮説を設計し統計的仮説検定を実施する必要がある。それによってもしその帰無仮説が棄却されたならば、対立仮説ひいては作業仮説がデータ分析によって支持されたと解釈することが可能になる。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_m\\)、女性における購買意図の期待値を \\(\\small \\mu_f\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\(H_0:~\\mu_m=\\mu_f\\) \\(H_1:~\\mu_m\\neq\\mu_f\\) \\(\\small H_0\\) は、男性における購買意図の期待値と女性における期待値が等しいというものであり、 \\(\\small H_1\\)はそれらが等しくないということを示している。そのため上記の二つの仮説は、どちらも未知パラメータについての関係を捉えており、\\(\\small H_0\\) と \\(\\small H_1\\) は互いに排反であることがわかる。その上で、もし帰無仮説が棄却され、男性の平均値のほうが女性よりも高い場合には、作業仮説が支持されたと解釈することができる。つまり統計的な検定においては、作業仮説として提示している推測を直接検証するのではなく、作業仮説と排反な帰無仮説を設計し、それが棄却されるならば暫定的に作業仮説の主張を支持しようという立場で検証を行う。なお対立仮説として\\(\\small H_1:~\\mu_m&gt;\\mu_f\\) を設定することも可能である。このような仮説に基づく検定方法は片側検定と呼ばれ、その詳細については後述する。 ここで改めて、より一般的な形でマーケティングリサーチで用いられる仮説と検定で用いられる仮説との関係を整理する。 分析上の基準である帰無仮説は何かをきちんと理解し定義する。 それが棄却された際にはどのような結論（対立仮説）が採用されるのかを理解する。 そしてその結論が自身の立てた作業仮説と帰無仮説・対立仮説の関係が整合的かを考える。 言い換えると、自身の立てた作業仮説を帰無仮説・対立仮説の対比という分析手続きで証明できるような調査・分析法を採用する必要がある。ただし、マーケティング領域におけるレポートや論文では、帰無仮説・対立仮説を記載せず、作業仮説のみを記載することが多い。 母平均の検定（両側検定）に関する理屈 ここで改めて、標準正規分布を用いた母平均に着目し、統計的仮説検定に関する一般的な説明を提示する。\\(\\small X_1,..,X_n\\) を正規母集団 \\(N(\\mu, \\sigma^2)\\) からのサンプルサイズ n の無作為標本とする。このとき、帰無仮説のもとでのパラメータの値を\\(\\mu_0\\)として、以下の帰無仮説と対立仮説を設計する。 \\[ H_0:~\\mu=\\mu_0,~~~H_1:~\\mu\\neq\\mu_0 \\] このとき、??節の信頼区間の説明でも述べた通り、正規分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\) の標本平均は以下の分布に従うことがわかっている7。 \\[ \\bar{X}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right) \\] また、これまでの議論の通り、\\(\\small \\bar{X}\\)を標準化した統計量Zは以下の分布に従うことが知られている。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] 統計的仮説検定においては、この標準化された統計量を検定統計量（検定に用いる統計量）として用いて計算を行うのだが、我々の関心の中心でもある \\(\\small \\mu\\) は未知であり、通常この統計量を計算することはできない。すなわち、未知であるパラメーターを何かしらの値で代替しなければ、上記の検定統計量は計算できない。そこで、統計的な仮説検定では、「帰無仮説が正しいと一旦仮定」した上で統計量を計算するというプロセスを経る。言い換えると、未知のパラメーターについて帰無仮説で示されている値を代入することで、検定統計量を計算可能にする。したがって、検定統計量 Z を以下のように定義する。 \\[ Z=\\frac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n}} \\] そして、もし「帰無仮説が正しければ」Zは標準正規分布に従うはずであり、言い換えると Z の計算結果は0に近い値を取る可能性が高いはずである。そこで、この Z を計算し、\\(\\small |Z|\\) がある閾値 c よりも大きい（十分に0から離れている）場合には帰無仮説を棄却する。なお、ここで用いる閾値 c のことを一般的に臨界値と呼ぶ。つまり、検定統計量 Z の計算結果に対して、以下の方針で仮説検定を行う。 \\[ \\begin{cases} |Z|&gt;c &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq c &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 臨界値 c の求め方は区間推定と同様、分析に対応する確率分布（今回であれば標準正規分布）に基づくある区間の確率計算で求まる。研究者はまず、任意の確率 \\(\\small \\alpha\\) を決める。この確率は「有意水準（significance level）」と呼ばれ、この有意水準と標準正規分布に基づく確率計算によって臨界値（下図内では \\(\\small \\pm z_{\\alpha/2}\\)）を求める。その上で、統計量の計算結果が臨界値より外側（下図における斜線部）にある場合には帰無仮説を棄却する。そのため、斜線部のような領域を棄却域、確率 \\(\\small 1-\\alpha\\) に対応する範囲を採択域と一般に呼ぶ。 Figure 1: 臨界値と確率計算 今回の場合、有意水準 \\(\\small \\alpha\\) に基づく両側臨界値 \\(\\pm z_{\\alpha/2}\\) を設定し、以下の方式で検定する。 \\[ \\begin{cases} |Z|&gt;z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 片側検定の紹介 ここまでの議論では対立仮説を \\(H_1:~\\mu\\neq\\mu_0\\) とし、左右対称の分布の両端に棄却域を設定した。このような検定方法を一般的に両側検定と呼ぶ。しかし、現実的ないしは理論的な根拠をもとに、ある値よりも高い（もしくは低い）ことが事前に予測できる場合がある。その場合には、例えば \\(\\small \\mu&gt;\\mu_0\\) や \\(\\small \\mu&lt;\\mu_0\\)といった対立仮説を設定することも可能である。このような対立仮説を利用した検定方法を一般的に片側検定と呼ぶ。ここでは、仮に\\(\\small \\mu&gt;\\mu_0\\)という対立仮説を立てた場合を考えるが、\\(\\small \\mu&lt;\\mu_0\\)のような対立仮説を設計しても正負を入れ替えることで同様の議論ができる。なお、片側検定において帰無仮説が棄却された場合、直ちに帰無仮説の値よりも大きい値を取ると判断する。しかしながら、たとえ異なる対立仮説を提示しても、採用する検定統計量や帰無仮説に基づく分布の仮定などは同じである。 片側検定を利用した場合の特殊性はその棄却域に現れる。片側検定の場合の棄却域は以下の図のように片側のみとなる。なお、その場合分布の両端に棄却域を設ける必要がないため、正の方向に \\(\\small \\alpha\\) 分の棄却域を設定する。 片側検定（正の場合） 有意水準と検定における誤り ここまでの内容をまとめると、帰無仮説を仮定して検定統計量を計算する場合、帰無仮説が正しければ、棄却域内の値を取る確率は \\(\\small 100\\times \\alpha\\)%であると言える。そして、検定統計量の計算結果が棄却域に含まれる場合、帰無仮説を棄却するという判断を下す。そのため、統計的に帰無仮説を棄却したからと言って、その結果が必ず正しいとは言い切れない。統計的検定には、根本的に第一種の誤り（Type 1 error）と第二種の誤り（Type 2 error）という二種類の誤りの可能性が内包されている。 第一種の誤りとは、帰無仮説が真であるにもかかわらず、帰無仮説を棄却してしまう誤りである。一方で第二種の誤りは、帰無仮説が真ではないにもかかわらず、帰無仮説を採択してしまう誤りである。例えば、ある薬に期待される効果があるかどうかを検証する場合を考える。この時、帰無仮説は「投薬による効果がない」、対立仮説は「投薬による効果がある」と設計する。この場合における第一種の誤りとは、「本当は効き目のない薬を効くと判断してしまう誤り」であり、第二種の誤りとは、「本当は効き目のある薬を効かないと判断したしまう誤り」である。 Table 4: 検定の誤り H0が真 H0が偽 H0を棄却 Type 1 error ✓ H0を採択 ✓ Type 2 error どちらの誤りも見過ごすことのできないものではあるが、第一種の誤りによる損失と、第二種の誤りによる損失を比較し、一般的な統計的検定においては、第一種の誤りの確率を下げることに注視する。なお、研究によっては下記にある検定力という指標に着目し、第二種の誤りに対応した議論を提示することもあるが、本書では割愛する。仮説検定では特に、第一種の誤りの確率を有意水準 \\(\\small \\alpha\\) と設定し分析する。また有意水準は、先述の通り棄却域の特定に用いられる。つまり、統計的仮説検定とは、帰無仮説が正しいと仮定した上で有意水準 \\(\\small \\alpha\\) の分だけ第一種の誤りの確率を許容したうえで仮説が正しいか否かを確認する作業である。 上記の統計的検定に関わる誤りは、\\(\\small T_0\\) は統計量 T の観測値、Rは\\(\\small H_0\\) の棄却域、Aは\\(\\small H_0\\)の採択域とし、以下のように示される。 有意水準: \\(\\alpha\\) \\[ P(T_0\\in R|H_0~\\text{が真})=\\alpha \\] 第二種の誤りの確率: \\(\\beta\\) \\[ P(T_0\\in A|H_0~\\text{が偽})=\\beta \\] 検定力: \\(1 - \\beta\\) \\[ P(T_0\\in R|H_0~\\text{が偽})=1-\\beta \\] ここで、仮説検定に関わる有意水準を \\(\\small \\alpha=0.05\\) と設定する。帰無仮説が正しいという条件のもとで、帰無仮説を棄却する確率であるため、有意水準は以下のように示すことができる。 \\[ \\alpha=P(|Z|&gt;c|\\mu=\\mu_0) =0.05 \\] しかしながら、このままだと確率計算が複雑になるため、上式を以下のように書き換える。 \\[ 1-\\alpha=P(|Z|\\leq c|\\mu=\\mu_0) = \\int^c_{-c}f(Z)dZ~_{|\\mu=\\mu_0}=0.95 \\] なお、上式の関係を表した図が図 1になる。 検定結果に関する解釈 ?? の検定について、検定統計量 Z は帰無仮説が正しければ標準正規分布に従うはずである。したがって、臨界値 \\(\\pm c\\) は、\\(\\pm z_{0.025}\\) として分布表などより導出が可能である。Rにおいては前節と同様、qnorm() により、\\(z_{0.025}=1.96\\) だとわかる。すなわち、検定統計量の計算結果が 1.96（-1.96）を上回る（下回る）場合には、帰無仮説を棄却するが、そこには第一種の誤りを犯す確率が5%残されていると解釈できる。 一方で、統計的に有意でない（帰無仮説を棄却できない）結果を得たときには、その解釈について注意が必要である。具体的には、統計的に有意でないからと言って、帰無仮説が正しい（つまり \\(\\small \\mu =\\mu_0\\) である）と結論づけることはできない。ここまでの説明の通り、有意水準とは第一種の誤りを犯す確率であり、有意水準に基づく統計的検定では主にこの確率に対応した分析を行っている。そのため、第二種の誤りである、本当は \\(\\small \\mu \\neq \\mu_0\\) であるにも関わらず、\\(\\small \\mu =\\mu_0\\) と判断している可能性については未対応である。これらの点から「有意でない」ということを理由に、帰無仮説が正しいと結論づけることは適切ではない。 なお、ここまでの議論のように母集団の分散が既知の場合、検定統計量は標準正規分布に従うと仮定できる。しかし母集団の分散が未知の場合は、信頼区間での議論と同様、標準偏差の不偏推定量を用いて、自由度 n-1 の t 分布を仮定した分析を行う。そして、t分布に基づく母平均に関する検定を一般的に「t検定（t-test）」と呼ぶ。 より実践的な視座から統計的な検定において解釈に注意が必要な点として、p-value（p値）がある。 t.test() を用いた分析例でも紹介するが、R (他のソフトウェアでも)で統計的検定を実行すると “p-value”（p値）という値を得る。p値は、帰無仮説が正しいと仮定したときに，手元のデータから計算した検定統計量以上に極端な値をとる確率だと定義できる（豊田, 2017）。そしてp値が有意水準よりも小さい場合には、帰無仮説が間違っていたという判断を下すというのが、p値に基づく帰無仮説棄却の判断である。p値については、「値が小さければ仮説が真であることを示す指標」や「小さいほど結果の重要性を示す指標」といった解釈を行うこともあるが、このような解釈は不適切である（Baker, 2016）。 棄却域と有意水準の関係に基づき述べると、以下のように説明することができる（cf. 西山など, 2019）。有意水準（帰無仮説が正しいが帰無仮説を棄却する確率）を小さく取ると、棄却域は狭くなる。例えば、ある仮説検定において、5%有意水準では帰無仮説を棄却できるが、1%ではできない場合がある。計算された検定統計量の実現値に基づき、有意水準を変えながら検定を行っていくと、これ以上有意水準を小さくすると帰無仮説が棄却されなくなるという有意水準の限界を見つけることができる。この限界をp値と呼ぶ。そのため、p値によって示されている確率は有意水準と同様のものを捉えているのだが、その計算過程が異なるという点において注意が必要である。 電球データと検定 先程の新型電球の例を再度使い母平均の検定を実施する。新型電球について我々が関心を持っていたのは、新型電球の製品寿命が旧型の寿命（1800時間）より長いか否かである。そのため、新型電球の製品寿命の期待値を \\(\\small \\mu\\) とすると、帰無仮説と対立仮説は以下のように設計できる。 \\(H_0:~\\mu=1800\\) \\(H_1:~\\mu\\neq1800\\) 改めて以下の通り、新型電球に関する16個の無作為標本から得た製品寿命の平均値を計算すると、\\(\\small \\bar{X}=1951.194\\) であった。では、この1951は 1800 から十分に離れていると言えるのだろうか。もし、十分に離れていると判断されれば帰無仮説を棄却するが、この差が十分でなければ帰無仮説を棄却しない。 mean(bulb) ## [1] 1951.194 Rを用いて統計的検定を実行すること自体は難しくない。まず、t検定に基づく母平均の検定は t.test() で実施することが可能である。母平均が特定の値を取るか否かについての検定では、mu= という引数を使って帰無仮説に対応する値を指定する。今回の分析に関するコマンドおよびその結果は以下のとおりである。 t.test(bulb, alternative = &quot;two.sided&quot;, mu = 1800) ## ## One Sample t-test ## ## data: bulb ## t = 3.9155, df = 15, p-value = 0.001377 ## alternative hypothesis: true mean is not equal to 1800 ## 95 percent confidence interval: ## 1868.890 2033.498 ## sample estimates: ## mean of x ## 1951.194 分析結果の t= と df = はそれぞれt値（検定統計量の推定値）と自由度を表している。p-valueはp値と呼ばれるある確率を表しており、先述の通り、注意の必要な指標である。また、t.test() は、信頼区間や標本平均も出力してくれるため、これらの結果に基づき解釈を行うことも可能である。帰無仮説の棄却に至るp値の基準は慣習的に、0.10（10%）、0.05（5%）、0.01（1%）が用いられる。今回の結果では、p値がp-value = 0.001377であり、5%水準で帰無仮説を棄却することができるため、新型電球の寿命は旧型（1800時間）よりも有意に高いと結論づけることができる。 上述の t.test() は母分散が未知である際に用いられる検定方法である。この点は、信頼区間において説明した内容と同様である。なお実際のデータ分析作業においては多くの場合母分散は未知であるため、t.test() を用いることが多い。しかしながら、電球の例では母集団の分散は \\(\\small 180^2\\) であることを仮定した。そのため、ここからは母分散が既知（\\(\\small \\sigma^2=180^2\\)）であることを仮定した標準正規分布に基づく母平均の検定を軸に説明していく。 電球の例においては、\\(H_0:~\\mu=1800\\)と設計していたため、検定統計量 Z は以下の通りに書き換えることができる。ただし、今回の例においては、 \\(\\small \\bar{X}=1951\\)、\\(\\small \\sigma=180\\) であることがわかっている。 \\[ Z=\\frac{1951-1800}{\\sqrt{180^2/16}} \\] ここまでの議論を踏まえ、新型電球に関する統計的検定を標準正規分布に基づき以下のように実施する。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 mu &lt;- 1800 #Test statistic Z &lt;- (xbar - mu)/(sigma/sqrt(n)) Z ## [1] 3.359861 分析の結果、検定統計量 Z の実現値が5%有意水準に基づく臨界値（1.96）よりも大きいことが示されたため、5%有意水準で帰無仮説が棄却された。つまり、5%の第一種の誤りの確率を残した上ではあるが、新型電球の製品寿命は旧型製品の寿命よりも長いと言える。このような結果は一般的に、「統計的に有意」な結果と表現される。 ここまでは、有意水準の意味を踏まえ、検定の手順及び結果の解釈について説明した。しかしながら、先述の通りもし統計的に有意でない（帰無仮説を棄却できない）結果を得たときには、その解釈について注意が必要である。もし今回の仮説検定で帰無仮説を棄却できていなかったとしたら、その結論は「新型電球寿命の平均は1800時間ではないとは言えない」となる。なんとも歯切れの悪い結論だが、統計的検定の特性上、このような解釈を提示しないといけない。 平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布↩︎ "],["meancomparison.html", "平均値に関するその他の検定", " 平均値に関するその他の検定 これまでは、母平均が特定の値を取るか否かに着目し、統計的仮説検定の基礎について説明した。しかしながら本章の冒頭でも例に挙げた通り、平均値をあるグループ間で比較したいと考えることも多い。本節では、期待値の比較に着目し、平均の差の検定と、分散分析について説明する。これらの検定では、用いる検定統計量は先述のものと異なるが、統計的仮説検定そのものの手順や、肝となる考え方は共通である。 前節で考えた、「女性に比べ男性の方が新製品購買意図が高い。」という作業仮説を再度考える。このとき、我々が観察可能なのは男性グループの標本平均（\\(\\small \\bar{X}\\)）と女性グループの標本平均（\\(\\small \\bar{Y}\\)）であるが、検定においてはそれぞれの期待値（\\(\\small \\mu_x\\) と \\(\\small \\mu_y\\)）に着目し、帰無仮説を作成する。なお、\\(\\small X_1,...,X_n\\) は\\(\\small N(\\mu_x,\\sigma^2_x)\\)に従う母集団からの無作為標本であり、\\(\\small Y_1,...,Y_n\\) は\\(\\small N(\\mu_y,\\sigma^2_y)\\)に従う母集団からの無作為標本であるとする。また、\\(\\small X_1,...,X_n\\) と \\(\\small Y_1,...,Y_n\\) は互いに独立であり、母分散は未知であるとする。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_x\\)、女性における購買意図の期待値を \\(\\small \\mu_y\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\[H_0:~\\mu_x=\\mu_y,~~H_1:~\\mu_x\\neq\\mu_y\\] 統計的検定の手順と直感的な検定統計量の作り方は前節の内容と同じである。そのため、検定における推定量と帰無仮説条件下での未知パラメータの値を特定し検定統計量を定義したい。この検定ではグループ間の平均の差を捉えているため、標本上での情報として \\(\\small \\bar{X}-\\bar{Y}\\) という関係を捉える。したがって、上記の帰無仮説と対立仮説は以下のように書き直すことができる。 \\[H_0:~\\mu_x-\\mu_y=0,~~H_1:~\\mu_x-\\mu_y\\neq0\\] また、母分散が未知である場合にはt検定を行うということも前節と同様である。このことから、以下の検定統計量を用いる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})-(\\mu_x-\\mu_y)}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] ただし、\\(s^2\\) はプールされた標本分散8と呼ばれる母集団の分散の推定量である。なお、\\(s^2\\)は母分散を捉えた推定量であるが、母分散が両群で等しい（等分散: \\(\\small \\sigma^2_x=\\sigma^2_y=\\sigma^2\\)）である場合には上記の検定統計量を自由度（\\(\\small m+n-1\\)）のt分布として分析可能である。一方で等分散ではない場合には、Welchのt検定と呼ばれる、自由度の計算を修正した分析方法を用いる。なお、Welchのt検定で用いられる自由度の詳細はここでは省略する。 このとき、帰無仮説が正しいという仮定のもとでは、\\(\\small \\mu_x-\\mu_y=0\\)である。そのため、上記の検定統計量は以下のように観察可能な情報のみで構成される形で書き換えることができる。また、帰無仮説が正しければ、この検定統計量は自由度（\\(\\small m+n-1\\)）のt分布に従うと考えられる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] そのため、データに基づき計算された検定統計量tの実現値を用いて、以下の方式で検定を行う。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(m+n-1) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(m+n-1)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] Rにおいて平均の差の検定を行うことはさほど難しくない。先程の等分散性についても、var.equal=TRUEまたはvar.equal=FALSEという引数で設定できる。var.equal= 引数についてはTRUEが等分散性を仮定するが、デフォルトでは、FALSEとなっている。 平均の差の検定では、 t.test(outcome ~ category) のように、はじめに着目する成果変数を、その後 ~（チルダ）のあとに着目するカテゴリ変数を指示することで、どの変数（outcome）の平均の差をどのカテゴリ変数（category）で検定するのかが特定化できる。ここでは、以前の章で利用した `firm2018’ データを利用して、広告集中度の高い企業と低い企業とで売上高の平均値に差があるか否かを以下のように分析する。なお以下では、等分散性について異なる仮定を置いた分析を続けて行っている。 library(tidyverse) firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) firm2018 &lt;- firmdata %&gt;% filter(fyear == 2018) %&gt;% mutate(ad_dummy = ifelse(adint &gt; median(adint),1, 0)) t.test(sales ~ ad_dummy, data = firm2018) ## ## Welch Two Sample t-test ## ## data: sales by ad_dummy ## t = -3.3989, df = 85.686, p-value = 0.001029 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -1674283.1 -438496.1 ## sample estimates: ## mean in group 0 mean in group 1 ## 725009.7 1781399.3 t.test(sales ~ ad_dummy, data = firm2018,var.equal=T) ## ## Two Sample t-test ## ## data: sales by ad_dummy ## t = -3.4555, df = 145, p-value = 0.0007207 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -1660616.4 -452162.8 ## sample estimates: ## mean in group 0 mean in group 1 ## 725009.7 1781399.3 出力結果の見方は、先述のt検定の場合と同様である。分析の結果、等分散性を仮定するか否かで、計算結果は微妙に異なるが、どちらの検定結果においても1%有意水準で帰無仮説を棄却できた。そのため、グループ間で売上高には差があり、広告集中度の高いグループのほうが売上高が高い（もしくは、売上高の高い企業ほど広告集中度が高いグループに属している）といえる。 なおRにおいては、等分散性に関する検定もvar.test(outcome ~ category)で以下のように実行可能である。 var.test(sales ~ ad_dummy, data = firm2018, ratio = 1) ## ## F test to compare two variances ## ## data: sales by ad_dummy ## F = 0.10863, num df = 74, denom df = 71, p-value &lt; 2.2e-16 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.06821636 0.17258882 ## sample estimates: ## ratio of variances ## 0.1086276 等分散性の検定では、ひとつのグループの分散ともう一方のグループの分散の比が1（等分散）であるという帰無仮説を設計する。詳細は割愛するが、帰無仮説が正しい場合には両グループの不偏標本分散の比が自由度（\\(m-1\\), \\(n-1\\)）のF分布に従う。これまでと異なる出力結果として、検定統計量の実現値F = 0.10863、分子の自由度num df = 74、分母の自由度 denom df = 71が提示される。なお、分析の結果、帰無仮説は棄却されたため、等分散とは言えないと結論づけることができる。そのため、平均の差の検定においては、Welchのt検定を利用した分析結果を採用して議論することが好ましい。 \\(s^2=\\frac{1}{m+n-1}\\{(m-1)s_x^2+(n-1)s_y^2\\}\\)↩︎ "],["ANOVA.html", "分散分析", " 分散分析 ここまでの内容では、２グループ間の平均の差に関する分析を捉えた。しかしながら、三つ以上のグループ間の平均の差に関心があることもある。例えば、異なる地域における売上高の差を比較したい場合が挙げられる。このような目的を持つ場合によく用いられるのが分散分析（Analysis of Variance; ANOVA）である。 ANOVAの構造については、要因と水準という二つの要素から説明が行われる。要因とは、観測値に影響を与えていると考えうるカテゴリ変数のことを指し、水準とは、要因を構成するいくつかの条件やグループを指す。例えばある小売企業における各店舗の一定期間内の売上高が出店エリア特性によって差があるのかという問いに関心があるとする。その際各店舗を、都市エリア、郊外エリア、農村エリアという三つのグループに分類し、それぞれのグループにおける標本平均を求めれば、エリアごとの差を分析できるだろう。この場合、「地域」が売上高に影響を与えうる要因であり、「都市、郊外、農村」という三つのグループが水準だと言える。 分析において取り上げる要因が一つである分散分析を一元配置分散分析と呼ぶ。二元配置分散分析については以下の節でモデル表現と共に紹介している。 分散分析についてのモデル表現*{#ANOAtheory} 本節では、分散分析の構造をモデルによって表現することでより明確な理解を促すことを目指す 。ここでは、要因をA、グループ（水準）の数をJとする。第\\(j\\)グループで\\(n_j\\)個の観測値が得られる。第\\(j\\)水準における\\(i\\)番目の個体の観測値を\\(y_{ij}\\)と表し、以下が成り立つとする。 \\[ y_{ij}=\\mu_j+e_{ij},~~~~e_{ij} \\sim N(0,\\sigma^2), \\] ただし、\\(i=1,2,...,n_j\\); \\(j=1,2,...,J\\)であるとする。 すべての\\(y_{ij}\\)は独立であり、 \\[ y_{ij}\\sim N(\\mu_j, \\sigma^2) \\] が成り立つ。したがって、\\(E(y_{ij})=\\mu_j\\)と、\\(V(y_{ij}=\\sigma^2)\\)も成り立つ。 上記モデルの関心は各グループで平均に差が存在するか否かである。そこで、以下の\\(\\mu\\)を全体平均とする。 \\[ \\mu=\\frac{n_1}{n}\\mu_1+\\frac{n_2}{n}\\mu_2+...+\\frac{n_J}{n}\\mu_J=\\frac{1}{n}\\sum^J_{j=1}n_j\\mu_j. \\] また、 \\(\\alpha_j=\\mu_j-\\mu\\)を第 \\(j\\)グループの効果と呼び、以下のように表現できる。 \\[ y_{ij}=\\mu+\\alpha_j+e_{ij} , \\] このとき定義より、 \\[ \\frac{1}{n}\\sum^J_{j=1}n_j\\alpha_j=0, \\]が成り立つ。もし、\\(\\alpha_1=\\alpha_2=...=\\alpha_J=0\\)ならば、以下のような水準間の差が存在しないモデルとなる \\[ y_{ij}=\\mu+e_{ij}, \\] 分散分析の第一の目的は、要因が観測値に対して影響を与えているかどうか、すなわち、\\(\\mu_1=\\mu_2=...=\\mu_J\\)が成立しているかどうかを調べることである。これは全てのグループで主効果\\(\\alpha_j\\)がゼロであるかどうかを調べることに等しい。そのため、以下を検定することと等しい。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(n-2) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(n-2)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] \\[ \\begin{cases} H_0:~ &amp;\\alpha_1=\\alpha_2=...=\\alpha_J=0\\\\ H_1:~ &amp;\\text{少なくとも１つの}\\alpha_j \\text{は0ではない} \\end{cases} \\] ここで、総変動: \\(S_T=\\sum^J_{j=1}\\sum^{n_j}_{i=1}(y_{ij}-\\bar{y})^2\\)を、群間変動: \\(S_B=\\sum^J_{j=1}n_j(\\bar{y_j}-\\bar{y})^2\\)と、郡内変動: \\(S_W=\\sum^J_{j=1}\\sum^{n_j}_{i=1}(y_{ij}-\\bar{y_j})^2\\)とに分ける(ただし、\\(S_T=S_B+S_W\\))。このとき、群間変動が大きいほど各\\(\\alpha\\)の推定値\\(\\hat{\\alpha_j}=\\bar{y_j}-\\bar{y}\\)のばらつき、すなわちグループ間での平均の違いが大きく、対立仮説\\(H_1\\)が支持されやすい。 しかしながら、この値自体は単位に依存し意味が無いため、以下の群間変動の郡内変動に対する比に基づいて検定を行う。 \\[ F=\\frac{S_B/(J-1)}{S_W/(n-J)}=\\frac{n-J}{J-1}\\times \\frac{S_B}{S_W} \\] Fは帰無仮説が正しい時に自由度（\\(J-1\\), \\(n-J\\)）のF分布に従うことが知られている。そのため、有意水準 \\(\\alpha\\) に基づく\\(H_0\\)に関する検定は、以下のように表すことができる。 \\[ \\begin{cases} F&gt;F_\\alpha(J-1, n-J)\\Rightarrow H_0\\text{を棄却}\\\\ F\\leq F_\\alpha(J-1, n-J)\\Rightarrow H_0\\text{を採択} \\end{cases} \\] 二元配置分散分析*{#twowayANOA} 一元配置分散分析観測値に影響を与える要因が2個あるモデルを考える。この場合の最も特徴的な違いが交互作用効果である。2つの要因をAとBとし、Aのグループの数をJ、Bのグループの数をKとする。（\\(A_j\\), \\(B_k\\)）につきr個の観測値\\(y_{ijk}\\)（\\(i=1,2,...,r\\)）が得られるとし、以下のモデルを考える。 \\[ y_{ijk}=\\mu_{jk}+e_{ijk}, ~e_{ijk}\\sim N(0,\\sigma^2) \\] ただし、\\(i=1,2,...,r\\); \\(j=1,2,...,J\\); \\(k=1,2,...,K\\)。 前節の内容と同様に全体平均と要因効果とに分解し、全体平均と呼ぶ。 \\[ \\mu=\\frac{1}{JK}\\sum^J_{j=1}\\sum^K_{k=1}=\\mu_{jk}, \\] そして、全体平均を以下のように分解し、前者を行平均、後者を列平均と呼ぶ。 \\[\\begin{eqnarray} \\mu_{j.}=\\frac{1}{K}\\sum^K_{k=1}=\\mu_{jk}, ~~ \\mu_{.k}=\\frac{1}{J}\\sum^J_{j=1}=\\mu_{jk}, \\end{eqnarray}\\] このとき、要因Aの第\\(j\\)グループの効果（\\(\\alpha_j\\)）、要因Bの第\\(k\\)グループの効果（\\(\\beta_k\\)）をそれぞれ、以下のように定義する。 \\[ \\alpha_j=\\mu_{j.}-\\mu, ~~\\beta_k=\\mu_{.k}-\\mu \\] この時、以下の恒等式が成り立つ。 \\[ \\mu_jk=\\mu+(\\mu_{j.}-\\mu)+(\\mu_{.k}-\\mu)+(\\mu_{jk}-\\mu_{j.}-\\mu_{.k}-\\mu)\\nonumber\\\\ =\\mu+\\alpha_j+\\beta_k+(\\alpha \\beta)_{jk}, \\] ただし、効果\\(\\alpha_j\\), \\(\\beta_k\\)を要因AとBの主効果、\\((\\alpha \\beta)_{jk}\\)をABの交互作用効果と呼ぶ。そして、定義より以下を示すことができる。 \\[ \\sum^J_{j=1}\\alpha_j=0, ~~\\sum^K_{k=1}\\beta_k=0, ~~\\sum^J_{j=1}(\\alpha \\beta)_{jk}=0, ~~ \\sum^K_{k=1}(\\alpha \\beta)_{jk}=0 \\] なお、交互作用効果が無いモデルは、以下のように示すことができる。 \\[\\begin{eqnarray} \\mu_{jk}=\\mu+\\alpha_j+\\beta_k \\end{eqnarray}\\] 二元配置モデルの効果検定*{#twowaytest} 二元配置モデルにおいても、以下のような要因Aの主効果に関する仮説に関心がある。 \\[ \\begin{cases} H_0:~ \\alpha_1=\\alpha_2=...=\\alpha_J=0\\\\ H_1:~ \\text{少なくとも１つの}\\alpha_jは0\\text{ではない} \\end{cases} \\] また、要因Bの主効果に関する仮説は以下のように定義できる。 \\[ \\begin{cases} H_0:~ \\beta_1=\\beta_2=...=\\beta_K=0\\\\ H_1:~ \\text{少なくとも１つの}\\beta_kは0\\text{ではない} \\end{cases} \\] これらに加え、交互作用効果に関する仮説は以下のように示すことができる。 \\[ \\begin{cases} H_0:~ (\\alpha \\beta)_{11}=(\\alpha \\beta)_{12}=...=(\\alpha \\beta)_{JK}=0\\\\ H_1:~ \\text{少なくとも１つの}(\\alpha \\beta)_{jk}\\text{ではない} \\end{cases} \\] これらの仮説の検定ではまず、以下のように「総変動」、「A間変動」、「B間変動」、「交互作用変動」、「グループ内変動」を定義する。ただし以下では、\\(S_T=S_A+S_B+S_{AB}+S_W\\) とする。 \\[ \\text{総変動:} S_T=\\sum^r_{i=1}\\sum^J_{j=1}\\sum^K_{k=1}=(y_{ijk}-\\bar{y})^2 \\] \\[ \\text{A間変動:} ~S_A=Kr\\sum^J_{j=1}(\\bar{y_{j.}}-\\bar{y})^2 \\] \\[ \\text{B間変動:}~S_B=Jr\\sum^K_{k=1}(\\bar{y_{.k}}-\\bar{y})^2 \\] \\[ \\text{交互作用変動:}~S_{AB}=r\\sum^K_{k=1}\\sum^J_{j=1}(\\bar{y_{jk}}-\\bar{y_{j.}}-\\bar{y_{.k}}-\\bar{y})^2 \\] \\[ \\text{グループ内変動:}~S_W=\\sum^r_{i=1}\\sum^K_{k=1}\\sum^J_{j=1}(y_{ijk}-\\bar{y_{jk}})^2 \\] これらを用いて、以下のような \\(F_A\\)、\\(F_B\\)、\\(F_{AB}\\) を検定統計量として検定を行う。 \\[ F_A=\\frac{S_A/(J-1)}{S_W/JK(r-1)} \\] \\[ F_B=\\frac{S_B/(K-1)}{S_W/JK(r-1)} \\] \\[ F_{AB}=\\frac{S_{AB}/(J-1)(K-1)}{S_W/JK(r-1)} \\] 多重比較とそれに伴う統計的問題 ANOVAにおける帰無仮説の棄却は、少なくとも1つの群は全体と異なる平均値を持っているという結論につながる。しかし、どれが異なる値を持つのかがわからないため、それを特定化するために多重比較と呼ばれる分析を行う。この多重比較には統計的問題が伴うと言われている。本節では多重比較に伴う統計的問題について考える。 多重比較に伴う問題は、第一種の誤り（帰無仮説が正しいときに帰無仮説を棄却する誤り）を起こす確率（有意水準）に着目し議論される。ANOVAにおける帰無仮説は以下のとおりである。 \\[ H_0:~ \\mu_1=\\mu_2=...=\\mu_J \\] この帰無仮説を有意水準5%を設定し、 (8) 式の統計量を用いて検定した場合の\\(H_0\\)に関する有意水準はもちろん5%になる。しかしながら、各グループごとの対となる検定を5%水準で行った場合、\\(H_0:~ \\mu_1=\\mu_2=...=\\mu_J\\)に対する有意水準は5%よりも大きくなってしまうと言われている。 この問題では、先述の\\(H_0\\)に基づき、分散分析が捉えている全体としての第一種の誤りと多重比較で捉えている個別の第一種の誤りという構造を把握する必要がある。言い換えると、多重比較により複数の検定を繰り返し行う場合、その複数の検定を「検定のセット」と考え、セット全体での第一種の誤りについて考えるということになる。 この統計的問題についてより明確に説明するために、1要因3グループの分散分析という具体例を考える。この時、分散分析としての全体の帰無仮説は\\(H_0:~ \\mu_1=\\mu_2=\\mu_3\\)となる。ここで対となる2群環の平均の差の検定を行うと、\\({}_3C_2=3\\) 通りのt検定を行うことになる。例えば、グループ1対グループ2の検定においては \\(H_0^1:~\\mu_1=\\mu_2\\) という帰無仮説を採用して検定を行うことになる。これら各対の検定における帰無仮説が棄却されれば、分散分析全体として捉えていた帰無仮説（\\(H_0:~ \\mu_1=\\mu_2=\\mu_3\\)）も当然棄却されることになる。そのため、各対に関する帰無仮説が少なくともひとつ棄却されることで、全体としての帰無仮説が棄却されるという構造を捉える必要が出てくる。この構造が、第一種の誤りについての問題につながる。 各対に関する帰無仮説に対して第一種の誤りが起こる確率は、「ひとつ目の対の検定において誤りが起きる、またはふたつ目の対の検定において誤りが起きる、またはみっつ目の対の検定において誤りが起きる」という事象の確率を捉えることになる。仮に各対の検定において有意水準5%を設定した場合、3回に1回以上第一種の誤りを起こす確率（全体としての有意水準）は、それぞれの検定において第一種の誤りを犯すという事象が互いに独立だと仮定すると、以下の通り0.05よりも多くなる。 \\[ 1-P(\\text{3回中1回も第一種の誤りを犯さない})=1-(0.95)^3\\approx 0.143 \\] そのため、分散分析に対応する複数の検定を繰り返し行う場合には、複数の検定を検定のセットと捉え、セット全体での第1種の誤りを犯す確率が事前に設定した有意水準（例：0.05等）以下に抑えられるような方法が求められる。この問題に対応した方法が多重比較法であり、代表的な例としてTukeyの多重比較法がある (Tukey’sのPairwise法については例えば南風原（2010)を参照してほしい)。 Rによる分散分析の実行 Rで分散分析を実行すること自体は難しくない。ここでは、?? 節で用いた reshape2 データを用いて、以下の通りチップ額が曜日によって異なるか否かを分析する。ANOVAの実行においてはaov()関数によって分析するモデルとデータを指定し、anova() によって分析結果を出力する（summary() を用いることも可能である）。なお、上記のデータサマリーより、day という要因には四水準 (levels) 含まれていることがうかがえる。 library(reshape2) s &lt;- aov(tip ~ day, data = tips) anova(s) ## Analysis of Variance Table ## ## Response: tip ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## day 3 9.53 3.1753 1.6724 0.1736 ## Residuals 240 455.69 1.8987 分析結果における Sum Sq は、水準間変動和（または、平方和）と呼ばれ、特定要因の水準間によって説明される観測値の変動を表している。 Mean Sq は平均平方と呼ばれ、Sum SqをDF(自由度)で割ったものである。F value は F値という検定統計量の実現値であり、Pr(&gt;F) は本検定の p値を表している。また、Residuals の行で示されているのは、残差平方和と呼ばれ、郡内変動、つまり同グループ（水準）内での値のばらつきの程度を表している。 本結果の場合、曜日間でのチップ額についての差についての推定値（F value）は1.6724 であった。そして、これが統計的に有意なのかについて検討するためにp値を参照する。このときp値は 0.1736 であり、帰無仮説が正しいと仮定したときに，手元のデータから計算した検定統計量以上に極端な値をとる確率は17％以上あるといえる。このことから、有意水準 5% で仮説検定を行っても帰無仮説を棄却できないと解釈でき、曜日による統計的に有意な差があるとは言えない。ANOVA実行している検定では、「すべての水準間で平均値は同じ」という帰無仮説を検定しており、仮に帰無仮説が棄却された場合、「少なくとも一つの水準では値が異なる」という対立仮説を支持する。そのため、ANOVAにおける帰無仮説の棄却は、少なくとも1つの群は全体と異なる平均値を持っているという結論につながる。今回の結果では、このような帰無仮説も棄却するに至らなかったということである。 もし仮にANOVAにおける帰無仮説が棄却されたとしても、それだけでは具体的にどの水準間に差があるのかはわからない。そこで、ANOVAを用いた研究では事後分析として、多重比較と呼ばれる分析を行うことが多い。しかし、この多重比較には統計的な問題が伴うと言われている（詳細は??節参照）。その問題に対応した手法として広く用いられているのが、Tukeyのpair-wise 比較である。Rではこの分析を、TukeyHSD() という関数で実行できる。具体的には、aov() でストアしたANOVAの分析結果を用いて以下のように実施する。また、plot()を用いて、pair-wise比較の結果を95%信頼区間とともに図示化することもできる。 tukey_result &lt;- TukeyHSD(s) tukey_result ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = tip ~ day, data = tips) ## ## $day ## diff lwr upr p adj ## Sat-Fri 0.25836661 -0.6443694 1.1611026 0.8806455 ## Sun-Fri 0.52039474 -0.3939763 1.4347658 0.4558054 ## Thur-Fri 0.03671477 -0.8980753 0.9715049 0.9996235 ## Sun-Sat 0.26202813 -0.2976929 0.8217492 0.6203822 ## Thur-Sat -0.22165184 -0.8141430 0.3708394 0.7678581 ## Thur-Sun -0.48367997 -1.0937520 0.1263921 0.1724212 plot(tukey_result) 分析結果における diff 列は平均値の差を表している。 lwer と upr は信頼区間の下限と上限を表しており、一番右の列はp値を示している。分析の結果、P-valueが10%水準よりも低い結果がないため、どのペアに関する検定でも有意な差は確認できなかった。したがって、分析結果は必ずしもチップ額が曜日によって変化するとは言えないことを示した。この結果は、アメリカにおけるチップ額が会計額に対する割合や提供されたサービス品質によって決まるという慣習から考えると妥当な結果である。しかしながら、前節で注意した通り、このような統計的に非有意な結果をもって「曜日はチップ額に影響を与えない」と結論づけるのは不適切である。 なお、今回の分析においてはANOVAもTukeyのpair-wise比較も有意な結論を得ることができなかったという点で、両者の結果に一貫性があった。しかしながら、ANOVAでは有意だが、Tukeyの分析ではどの組み合わせも有意ではないという一見整合でない結果を得ることもある。その場合には、慣習としてTukeyの多重比較結果を優先して解釈を提示することが多い。しかしながら、研究者が自身の実施した検定の「意味」を理解し、解釈や議論を提示することも重要である。例えば、ANOVAとTukeyでは用いている帰無仮説が異なるため、異なる比較対象を用いた検定を実施している。そのため、自身が実行した分析がどのような帰無仮説を採用しており、何と何の比較を行っているのかを正確に把握し、実施した検定の意味に適した解釈や議論を展開する事が重要になる。 本章では、基礎的な統計学の復習として、主に区間推定と統計的仮説検定について説明した。区間推定では、主に信頼区間の計算に着目し、信頼区間の意味についてきちんと理解、解釈することの重要性を強調した。また、統計的仮説検定では、母平均の検定を起点とし統計的検定の基礎的な構造と考え方について説明した。検定においては、母集団の統計的特徴に関する予測である帰無仮説と対立仮説を設計することが重要である。また、グループ間の差異に着目した検定を行う場合には、関心のある未知パラメータについての差や比に着目し検定統計量を作成する事が多いが、基本的な統計的検定の考え方と手順は変わらないという点についても説明した。 回帰分析と分散分析の関係 分散分析は回帰分析の特別な場合（回帰分析における独立変数をダミー変数とした場合）に対応する。ダミー変数とは以下の様な2値変数である。 \\[ x_i=\\left\\{ \\begin{array}{l} 1~~~\\text{if 被験者iは1群に属する}\\\\ 0~~~\\text{otherwise} \\end{array} \\right. \\] そのため、2標本問題は、\\(y=\\beta_0 +\\beta_1x+e,\\) において、以下を検定することと等しい。 \\[ \\begin{array}{l} H_0:~ \\beta_1=0\\\\ H_1:~ \\beta_1\\not=0 \\end{array} \\] また、ダミー変数の効果は以下のように定数項（切片）の違いとして表すことができる。 \\[ E(y|x=1)=\\beta_0+\\beta_1 \\] \\[ E(y|x=0)=\\beta_0 \\] 二元配置に関しては、以下のように、交差項 (interaction term) を含む回帰分析として分析を行えば良い。 \\[ y=\\beta_0 +\\beta_1x+\\beta_2z+\\beta_3(x\\times z)+e, \\] この場合、xがyに与える影響は以下のように表される。 \\[ \\frac{\\partial y}{\\partial x}=\\beta_1+\\beta_3z \\] 一方で、zがyに与える影響は以下のように表される。 \\[ \\frac{\\partial y}{\\partial z}=\\beta_2+\\beta_3x \\] そのため、xとzがyへ与える影響は互いがそれぞれに依存しており、\\(\\beta_3\\)によってそれぞれの影響に対する調整効果が表されている。 "],["pwr.html", "検定力分析とサンプルサイズ", " 検定力分析とサンプルサイズ 統計的検定を行う場合、サンプルサイズの多さが実務的含意の弊害になりうるという議論も存在する。例えば、平均の差の検定においてサンプルサイズが著しく多いと、実務的にはあまり意味を持たない極めて小さな差であっても、「統計的に有意な差」として判断される事がある。このような弊害を避けるために、検定力分析と呼ばれる分析枠組みからサンプルサイズを計算する方法が用いられる（Cohen, 1988）。 ここでは、以下のような帰無仮説と対立仮説を用いる平均の差の検定を例に取りこのアプローチによるサンプルサイズの検討を紹介する。 \\[ H_0:~~\\mu_1=\\mu_2\\\\ H_1:~~\\mu_1\\neq\\mu_2 \\] 検定力分析では、検定力と効果量という指標を用いる。検定力は第二種の誤りを犯す確率 \\(\\beta\\) を用いて \\(1-\\beta\\) と定義される。概念的に説明すると検定力とは、「母集団において 差があるとき，サンプルにおいて有意な結果が得られる確率」といえる（南風原, 2002, p.143）。一方で効果量は、サンプルサイズで変化することのない、標準化された効果の大きさについての指標である（水元・竹内, 2008）。先述の通り、サンプルサイズが大きいと、この効果量が小さくても統計的に有意な差があると判断されることもある。そのため、効果量を捉えることで実務的にどのような差があるのかを議論することが可能になる。なお、平均の差の検定においては、0.2、0.5、0.8をそれぞれ小さい、中くらい、大きい、に対応する効果量の目安として捉えられている（水本・竹内, 2008）。 検定力分析の枠組みでは、有意水準（\\(\\alpha\\)）、検定力（\\(1-\\beta\\)）、効果量（\\(d\\)）、サンプルサイズ（\\(n\\)）の4つの指標については、他の3つが定まると残りの1つの指標も自動的に求まるという関係を有する（水本・竹内, 2008）。検定力分析においては元々、分析における有意水準、効果量、サンプルサイズから分析の検定力を計算する方法として用いられてきた。しかしながら、上述の各指標同士の関係から、自身が想定する分析・検定の条件（\\(\\alpha\\)、\\(1-\\beta\\)、\\(d\\)）を指定することで、その条件を満たす \\(n\\) を求めることも可能である。この枠組みを採用することで、自身の求める大きさの効果量（例えば、\\(d=0.8\\)）を想定する条件（\\(\\alpha\\)、\\(1-\\beta\\)）で検定するために必要な \\(n\\)を逆算することが可能になる。 ここでは、検定力分析の詳細な計算方法について省略し、Rを用いた分析方法を紹介する。具体的な計算は、pwr::pwr.t.test() というパッケージおよび関数を用いる。なお、pwr には他にも様々な分析モデルに対応するサンプルサイズの計算が可能である。まずは、以下の通りパッケージをインストールし読み込む。 install.packages(&quot;pwr&quot;) library(pwr) ここでは、先行研究で推奨されている\\(\\alpha=0.05\\)（引数では sig.level）、\\(1-\\beta=0.8\\)（引数では power）という水準を用いて、平均の差の検定を行う場合のサンプルサイズを以下のように求める（Cohen, 1988）。 est_n &lt;- pwr.t.test(d=0.8, power=0.8, sig.level=0.05) est_n ## ## Two-sample t test power calculation ## ## n = 25.52458 ## d = 0.8 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 出力結果の通り、上記の条件に基づくサンプルサイズは各グループ26人ずつであるとわかる。そのため、2グループ間の比較を行う場合のサンプルサイズの合計は52となる。また、検定力とサンプルサイズの関係について、plot() 関数を用いて以下のように出力することも可能である。 plot(est_n) なお、pwr.t.test() 関数内の引数設定次第で、検定力を計算することも可能である。 例えば、\\(d=0.8\\), \\(n=30\\), \\(\\alpha=0.05\\) の場合の検定力は以下のように計算できる。 pwr.t.test(d=0.8, n=26, sig.level=0.05) ## ## Two-sample t test power calculation ## ## n = 26 ## d = 0.8 ## sig.level = 0.05 ## power = 0.8074866 ## alternative = two.sided ## ## NOTE: n is number in *each* group そのため、研究者が採用した検定の条件や、分析結果としての効果量が算出できれば、その検定における検定力も同じ関数から計算可能であることが伺えた。 "],["ch4reference.html", "参考文献", " 参考文献 浅野正彦・矢内勇生（2018）「Rによる計量政治学」，オーム社. 岩田暁一（1996）「経済分析のための統計的方法 第２版」，東洋経済新報社. 倉田博史・星野崇宏（2024）「入門統計解析」，新世社. 豊田秀樹（2017）「p値を使って学術論文を書くのは止めよう」,『心理学評論』,60(4), 379-390. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. 南風原(2002)「心理統計学の基礎 総合的理解のために」，有斐閣. 水本篤・竹内理(2008)「研究論文における効果量の報告のために 基礎概念と注意 点」 『英語教育研究』31, 57 66. 宮川公男（2002）「基本統計学」，有斐閣. Baker, M. Statisticians issue warning over misuse of P values. Nature 531, 151 (2016). Cohen J. (1988) Statistical Power Analysis for the Behavioral Sciences, 2nd edition, Lawrence Erlbaum Associates. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
