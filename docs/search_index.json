[["index.html", "マーケティング・データ分析補足資料 本ウェブサイトについて", " マーケティング・データ分析補足資料 田頭拓己 神戸大学大学院経営学研究科 2025-11-12 本ウェブサイトについて 本ウェブサイトは、マーケティング・リサーチにて広く用いられる基礎的な分析手法について、R (R studio) を用いた実行方法を紹介することを目的としている。また、本ウェブサイトは以下の図書の補足サイトとして、作成している。 田頭拓己（2025）「マーケティングデータ分析: 実務とリサーチをつなぐ」，有斐閣. そのうえで、ここではとくに分析用のコードと、図書内で省略した一部の分析手法の紹介を行っている。なお、上記の図書では分析コードだけでなく、紹介した分析手法の理論的な説明や、マーケティング・経営学領域での研究の進め方についても紙幅を割いているため、それらの内容に関心がある場合にはぜひ図書を手にとってみてほしい。 "],["rusage.html", "R と R studioに慣れる ", " R と R studioに慣れる "],["ch2intro.html", "本章の概要", " 本章の概要 本章では、統計分析ソフトウェアR（あーる）と R Studio の概要および使い方について説明する。本書では特に、R Studio と呼ばれるアプリケーションを活用した分析を想定する。さらに本章ではR Studio で活用できるプロジェクト機能を活用することを強く推奨する。本章の内容を参考に、R Studioを用いたデータ分析を行うための実行環境を整えてほしい。 R は統計、データ解析、統計グラフ作成のためのオープンソースソフトウェアである。なお、Rを用いる際には、多くの場合) R studio、Jupyter notebookや、Rコマンダーのようなユーザーインターフェイスが利用される。そしてRを使用する際には (Rコマンダーを使わないかぎりは) 基本的にソースコードを入力し計算、分析を行う。Rを用いる際に最もよく使われる環境（アプリケーション）のひとつがR studioであり、本書においても基本的にはR studioを用いることを前提とするが、R studioをデスクトップにインストールし利用する場合には、Rそのものもインストールしておく必要があることに注意が必要である。R studioは現在、Positとも呼ばれており、以下のサイトからダウンロードが可能である（https://posit.co/download/rstudio-desktop/）1。このリンクにアクセスし、自身の環境に適したRStudio Desktop ソフトウェアの実行ファイルやディスクイメージファイルをダウンロードしてほしい。そしてファイルをダウンロードしたら、それを開くことで表示されるインストーラーの手順に従い、RStudioをダウンロードしてほしい。 また本書では、Posit Cloudという、アカウント登録を行うことでブラウザ上でR studioを利用できる環境も紹介する。R studio Desktop版 の利用においては、データの所在地（ディレクトリ）設定などによってエラーが生じることが多々あり、個別のPC環境に合わせて対応、設定を行う必要がある。そのため、まとまった人数に対応する必要がある講義で R studio を活用する場合や、R studio のインストールに苦労する初学者においてはまずPosit cloud と呼ばれるクラウド版を利用し、本書で紹介するような分析作業になれることを勧める。しかし、Posit cloud のフリーアカウントには、利用可能な時間やデータ容量に制限があるため、自身の研究や仕事等でデータ処理や分析を行う場合にはR studio デスクトップ版（通常はR studio IDEのフリーバージョンで十分）をインストールし、利用してほしい。 なお、Posit社のウェブサイトの構成は時期によって変化する可能性がある。もし、RStudio Desktopのダウンロードサイトが見当たらないときにはPosit ウェブサイト内で “Rstudio” と検索してほしい↩︎ "],["desktop.html", "R studio デスクトップ版の利用", " R studio デスクトップ版の利用 R studioは、Rを利用するためのアプリケーションである。R単体で使うよりも便利な機能が搭載されており、R studioを使うことでプログラミング作業を容易にすることが可能になる。最も大きな特徴としては、Rでの操作、分析を実行するための「コンソール画面」と、実行したい操作、分析のコードを記述しておく「Rスクリプト」と呼ばれるテキストファイルを一つの画面内に同時に表示できることである。そのため、Rに実行してほしいコマンドをテキストデータのように記述、修正し書き溜めておける一方で、その実行もスムーズに行え、結果も同画面内で確認することができる。 R studio はR をより便利に使うためのツールであるため、利用者はまず、R と R studioの両方をインストールする必要がある。Rは以下のサイト（https://ftp.yz.yamagata-u.ac.jp/pub/cran/）等からダウンロードし、インストールすることが可能である。当該ウェブサイトにアクセスすると、Windows用、Mac用、Linux用のソフトウェアが選択できるため、ユーザー自身の環境に適したバージョンを選択してほしい。Windowsユーザーの場合は、“install R for the first time” と書かれているリンクから自身の適した条件を選択し、実行ファイルをダウンロードしてほしい。Macユーザーの場合には、自身の使っているMacのバージョンに適したパッケージファイルを選択し、ダウンロードしてほしい。インストーラーやパッケージファイルをダウンロードした後はその後のソフトウェアインストール手順指示に従い、Rをインストールすること。 R studio のインストールは、以下のリンクから 行うことができる（https://posit.co/download/rstudio-desktop/）。なお下図は、MacOSを使ったデバイスの場合のインストール画面を表している。RStudioのインストールにあたっては、このリンクにアクセスし、自身の環境に適したRStudio Desktop ソフトウェアの実行ファイルやディスクイメージファイルをダウンロードしてほしい。なお、特別な事情がない限り、無料版で十分分析が可能である。そしてファイルをダウンロードしたら、それを開くことで表示されるインストーラーの手順に従い、RStudioをダウンロードしてほしい。 Rstudio Desktop インストール画面（Mac例） なお、WindowsでのRおよびR studioのインストールには注意が必要である。特に、Rを用いる講義を受け持っていると、Windowsユーザーを中心に新たなパッケージのインストールができないなどのトラブルが頻発する。これらの問題点を調べると、(1) 文字コードによる文字化けの問題、(2) ユーザーアカウントのホームディレクトリ名に日本語（全角）が利用されていること、(3) Rのライブラリが(勝手に) One drive 上に作成されることが原因であることが多かった。これに対して、 R の version 4.20以降からは、UTF-8の文字コードに対応したり、デフォルトでのRのインストール場所の変更（One drive上でない）が行われたりと、問題の改善が図られている。また、自身のホームディレクトリの名前が全角文字であるときは、ホームディレクトリ以外のローカルディレクトリを設定したほうが良い。この点に関する対応には、三重大学の奥村先生によって以下のウェブサイトに説明が記載されているので、詳しくはそちらを参照してほしい（https://okumuralab.org/~okumura/stat/R-win.html）。 "],["project.html", "プロジェクト機能について", " プロジェクト機能について R および R studio のインストールが完了したら、アプリケーションを起動する。 アプリケーションを起動すると、デフォルトでは以下のような R Studio環境画面が表示される。 新しいRstudio 画面 Rstudioは、上記の図のような画面構成をしている。Rstudioの画面を構成する主なウィンドウはペインと呼ばれ、(1) RスクリプトでRコードの入力・編集に用いる”Source”、(2) Rの命令を直接入力し結果も表示される”Console”がなどが主な要素としてある。また、その他利用しているデータ情報、パッケージ、履歴など様々なタブが存在する。Rstudioの初回起動時にはSourceのペインは収納されているため、 Rスクリプトファイルを作成する必要がある。Rstudioは基本的に4分割画面で表示され、各ペインの配置については、Tools \\(\\rightarrow\\) Global option \\(\\rightarrow\\) Pane Layoutより変更が可能になる。Rstudioを操作する上で、基本的に重要となる情報は、(1) Source、(2) Console、(3) データやプロットに関する環境情報の3点であるので、以下のような配置がおすすめである。 左上 or 下: Source 左下 or 上: History (ただし、さほど重要ではないので畳んだ状態にしておく) 右上 or 下: Console 右下 or 上: 複数タブをまとめ 配置の目的はあくまで、必要な情報を同一画面上に表示することであるため、自身のやりやすい配置を考えてアレンジしてほしい。 R studio を利用する際には、「プロジェクト」機能を使うことを勧める。プロジェクトは、互いに関連し合ったファイルの集まりを指す。Rを通じた分析では、たくさんのファイルを扱うことになる。例えば、複数のRスクリプトやデータセット、加工したデータセットの保存、分析結果、出力された図表などがある。これらのファイルを手作業で一括管理することは困難である。むしろそのような管理作業に認知的な負担を費やしたくないというのが分析者の本音である。プロジェクト機能を使うことにより、作業ディレクトリとファイルの保存先をひとまとまりに指定できるため、ファイル管理の手間がなくなる。 新しいプロジェクトを作成するシンプルな方法が、Fileから作成する方法である。具体的には、File -&gt; New Project -&gt; New Directory -&gt; Create New Project -&gt;Directory nameの指定 -&gt; プロジェクトの設置場所（ディレクト）の指定、という手順で作成する。 R をデスクトップ上で利用する際には、基本的には自身のPC内にあるデータの所在地（ディレクトリ）を特定することでデータの操作や分析を行う。これに対してプロジェクト機能を利用することでそのプロジェクトを実行している際に参照するワーキングディレクトリを固定することが可能になる。この機能によってR studioを通じたデータ処理や分析作業が容易になり、不要なトラブルを避けることが可能になるため、デスクトップでR studioを使う場合には可能な限りプロジェクト機能を利用してほしい。 "],["basics.html", "Rの基本操作", " Rの基本操作 ここでは、Rを使用する上での基本的な操作方法を紹介する。Rはコマンド（命令）をconsoleを通じて実行することで動かすことができる。例えば四則演算であれば、以下のように命令し、計算が実行できる。 1 + 2 ## [1] 3 5 - 10 ## [1] -5 3 * 8 ## [1] 24 1/2 ## [1] 0.5 基本的に一つのコマンドは1行に書き、数字、演算記号、スペースは半角で入力する。以下は、べき乗、平方根、自然対数を計算するためのコマンドで計算できる。 2^3 ## [1] 8 sqrt(2) ## [1] 1.414214 log(2) ## [1] 0.6931472 Rは、ベクトルや行列の計算も可能である。c() という関数を用いると、ベクトルを作成できる。例えば、c(1, 3, 5) というコマンドによって(1, 3, 5)というベクトルが作成できる。作成したベクトルを使って以下のような計算も可能である。 c(1, 3, 5) + 1 ## [1] 2 4 6 ベクトルは、連続した数字の列を生成するための演算子である : を用いても作成することができる。例えば、1から100の整数を要素とするベクトルは以下のように作成することが可能である。 1:100 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 また、ベクトルの要素は文字列でも構わない。 cities &lt;- c(&quot;Tokyo&quot;,&quot;Osaka&quot;,&quot;Kobe&quot;) cities ## [1] &quot;Tokyo&quot; &quot;Osaka&quot; &quot;Kobe&quot; 上記の計算方法に加え、Rが持つ重要な特徴に、オブジェクトの定義がある。Rでは、任意の行列、ベクトル、数値などに名前をつけ定義したうえで、それを用いた計算を行うことができる。なお、Console上で以下のように定義（実行）したオブジェクトはenvironmentタブ内に表示されるため、各自確認をしてほしい。なお、定義したオブジェクトの確認・出力も簡単に行えるが、大文字と小文字は区別されるため、注意が必要である。 a &lt;- 1 b &lt;- 2 a ## [1] 1 A ## Error: object &#39;A&#39; not found また、定義したオブジェクトを用いた計算も実行できるため、各自以下の計算を実行し、結果を確認してほしい。 a + b a / b a ^ b なお、先程のベクトル操作と組み合わせ、ベクトル名 [i] とすることで、ベクトルの i 番目の要素にアクセスすることができる。例えば、以下のaとbというベクトルから特定の要素を取り出すことを考える。なおこの場合、同時に複数の要素を取り出すこともできる。 a &lt;- seq(10, 100, length = 10) b &lt;- 10:1 aの2番目の要素 a[2] ## [1] 20 bの2番目の要素 b[2] ## [1] 9 aの3-5番目の要素 a[3:5] ## [1] 30 40 50 aの1,3,5番目の要素 a[c(1,3,5)] ## [1] 10 30 50 分析で繰り返し必要になる機能がRで使えないときは、function()関数を使って、新たな関数を作成できる。例えば、最大値と最小値を並べて表示したい場合を考える。そのために、ここでは ‘mm’ という新たな(オブジェクトxの最小値と最大値で構成されるベクトルを返す)関数を作ってみる。 mm &lt;- function(x){ c(min(x), max(x)) } そして上記の関数を利用して、以下のオブジェクト a, b の最小値と最大値を出力する。 a &lt;- c(1, 5, 100, 2, -8, 7) b &lt;- c(1, 6, 8, 0, 120) mm(a) ## [1] -8 100 mm(b) ## [1] 0 120 しかし、すべての関数を自作するのは難しい。Rでは様々な計算を実行するための関数が用意されており、多くのマーケティング研究においては既存の関数を用いることで対応が可能である。実は上記のmm関数の中で使っている “min”や”max”も、それぞれ最小値と最大値を返す関数である。他にも例えば、meanや median があり、これらはそれぞれ平均値と中央値を計算するための関数である。 関数の利用においては例えば、f()のように関数名 “f” のあとにカッコをつけて表記する。()の中には、引数（arguments）を用い、計算に必要な情報を指定することが必要となる。例えば、seq() という関数を用いて、2以上20以下の偶数の数列(sequence)を作ることが可能である。seq(from = x, to = y, by = z)は等差数列を作るための関数であり、第一の引数で最初の数、第二の引数で最後の数、第三の引数で間隔を指定することで数列を作成できる。以下で提示される二通りの表記ではどちらも同じ結果を返す。 seq(from = 2, to = 20, by = 2) ## [1] 2 4 6 8 10 12 14 16 18 20 seq(2, 20, 2) ## [1] 2 4 6 8 10 12 14 16 18 20 特定の関数に対する引数を確認したい場合は ‘?関数名’とconsoleに命令することで確認が可能になる。例えば、seq関数について知りたければ、’?seq’ で確認できる。 Rに元から含まれている関数以外にも他者が開発してくれた関数も存在する。そしていくつかの関数をまとめたpackagesが多数存在する。これまで世界中の開発者たちが作成したパッケージを公開してくれている。パッケージは何らかの目的や課題を達成することを目的に構成されたコードライブラリであり、それらをインストールし、各セッションごとに起動することで利用できる。 CRAN（The Comprehensive R Archive Network、Rに関するコードとドキュメントを配布するサーバーネットワーク）で公開されているパッケージは、install.packages()でインストール可能である。また、Rstudio の場合、ペインからpackagesタブ\\(\\rightarrow\\)Install\\(\\rightarrow\\)パッケージ名の入力という手順でもインストールが可能である。そしてインストール済パッケージは、library() によって起動することで、活用可能にある。ここで注意しておきたいのは、library()によるパッケージの起動はセッションごとに実行しないといけないという点である。まずは以下の通り、本講義で用いる “tidyverse” パッケージをインストールし起動してみる。“tidyverse”は、複数のパッケージから構成されているパッケージであり、データの整形・分析を行うために役立つ複数のパッケージをまとめてインストール・起動できる。 install.packages(&quot;tidyverse&quot;) library(&quot;tidyverse&quot;) "],["rscript.html", "R スクリプトのすゝめ", " R スクリプトのすゝめ Rstudio環境で作業を行う際には、Consoleに直接コマンドを入力するのではなく、‘.R’ という拡張子のファイルを使って、「Rスクリプト」を作成することを勧める。RスクリプトはRでの分析に対応しRコマンドの集まりとして記されるファイルであり、SourceエディタからRスクリプトに記載されたコマンドを、Consoleを通じて実行する。 Rスクリプトを用いることの重要性は、コマンドの修正可能性と、分析の再現性という二点から理解できる。まず修正可能性として、そもそもコマンドを書くうえでは大小様々な誤りが付き物である。このような間違いに対応し適宜修正を加えていくためには、実行するコマンドを一つ一つ console に直接記載するのではなく、Rスクリプトとして分析過程を記録し、その内容に基づき記載、修正を加えることが好ましい。 第二に再現性においては、Rスクリプトとしてデータ整形・分析のプロセスを文書ファイルとして保存しておくことで研究者自身もしくは第三者が分析を再現することが可能になる。これは研究プロセスの客観性を高め、分析結果の信頼性を高めるために非常に重要な要素である。 これらに加え、Rスクリプトの利用は研究者個人の研究遂行上の利点もある。自身が行った研究であっても分析の細部に関しては時間経過とともに忘れてしまうものである。その際に、Rスクリプトによる分析プロセスの追跡可能性が役に立つ。また、同様の分析を再度別データで実施する場合も、既存のRスクリプトを応用することで効率的に分析が可能になる。これらに加え、共同研究において他の研究者と分析プロセスを共有する場合にもRスクリプトが役に立つ。 Rスクリプトを書く 先述のRスクリプトの利点を活かすために、Rスクリプトの作成においては、いつ作成されたなんのためのファイルなのか、そしてファイル内に記載されているコマンドがどのような意図によるものなのかがわかるように書くべきである。そのための重要になるのがコメント機能である。一つの行の中で#記号よりも後ろの部分はコメントとして処理（コメントアウト）される。コメントアウト機能とは、プログラムコードにおいて、分析や実行から除外するための指示である。この機能を使い、コメントで自然言語による説明を加えることで、コマンドの説明や意図等、自分や他人がスクリプトを見返して内容を理解できるようにする。例えば、Xという変数の平均値を求める場合、以下のようにRスクリプトを書くようにする。 #Xの平均値を求める。 mean(X) Rスクリプトは、Rstudioの左上にある+ボタンから新規作成可能である。ここでは試しに、新規Rスクリプトを作成し、“mktg01.R” という名前で保存してほしい。保存したRスクリプトはファイルから開くことができる。“mktg01.R”ファイルを作成したら、試しに以下のコマンドを書き込み、実行してほしい。Rスクリプトからコマンドを実行する際には、コマンド記入後、Rスクリプト上で実行したい行にカーソルを合わせた状態でcommand (control) + Return (Enter)を入力する。もう一度同じキーを押すと、2行目のコマンドが実行される。これらを実行することで、Rコンソール上に、下記と同じ結果が出ていることを確認してほしい。なお、コマンドを記入の際には、こまめに command (control) + s により保存することを心がけるようにしてほしい。 a &lt;- 9 sqrt(a) ## [1] 3 また、Rスクリプトを作成する際には、ファイルの冒頭に以下の説明を書き込む習慣をつけると後々見返すときに便利である。 ファイル名 目的 作成者 作成日 最新更新日 例えば、上記の内容とコマンドを含めた “mktg01.R” ファイルは、以下のようになる。 Rスクリプト例 "],["ch2summary.html", "本章のまとめ", " 本章のまとめ Rはデータの管理、分析、図表の作成を行うことができる統計分析プログラミング言語である。 Rを動かすには、コマンドと呼ばれる命令をコンソールを通じて実行する。 コマンドは基本的にRスクリプトに書き込んでからcommand (control) + Return (Enter)で実行する。 Rスクリプトにはコマンドだけでなくコメントを使った説明も追加する。 分析には既存の関数やパッケージを使うことが多い。 "],["positcloud.html", "コラム：Posit cloudを使ってみる", " コラム：Posit cloudを使ってみる R studioをより手軽に利用できるサービスとしてPosit Cloudがある。Posit Cloudはブラウザを通じてR studio環境を利用できるサービスであり、アカウント登録をするだけでよく、コンピュータへのRおよびR studioのダウンロードとインストールが不要である。 Posit Cloudの利用方法はとても簡単である。大まかな利用までの流れは以下のとおりである。 以下のリンク（https://posit.co/）からサイトへアクセスし、ProductsタブからPosit Cloudを選択する。 その後、進んだ画面で “Get Started” \\(\\rightarrow\\) （特別な理由がなければ）Free planを選択し “Sign up” \\(\\rightarrow\\) 好きな方法でアカウントを作成する。 登録が完了すると、自身のアカウントのホーム画面へ移動する。新しいR studio セッションを開始するためには、画面右上の New projectボタンを押し、“New Rstudio Project” を選択する。 New projectのセットアップが完了すると、Studio環境画面が表示される。 新しいRstudio 画面 このような手順でデスクトップ版と同様のR studio 画面にブラウザからアクセスすることができる。これにより、（容量の制限はあるものの）デスクトップ版と同様の分析を実行することができるため、大人数講義や、RやR studioのインストールに不安のある方はこちらのサービスを利用してほしい。 "],["ch2reference.html", "参考文献", " 参考文献 浅野雅彦・矢内勇生 (2018) 「Rによる計量政治学」, オーム社. ランダージャレド（2015）「みんなのR 第2版」，高柳新市・牧山幸史・蓑田高志訳，マイナビ. "],["handling.html", "データ整理と要約・可視化 ", " データ整理と要約・可視化 "],["Ch3intro.html", "本章の概要", " 本章の概要 本章では、Rを用いたデータの処理と記述的な分析について紹介する。マーケティング領域では、様々なタイプのデータを扱うが、どのようなデータであってもデータを取り込み、分析可能な形に処理した後、データの特徴について確認することが必要になる。最終的に高度な統計分析を行うことを想定していたとしても、自身の獲得したデータの特徴を確認することは非常に重要である。そのため、本章ではデータの読み込みやデータ処理といった、分析の前に必要な技術的過程を紹介する。 データセットの構築が完了したあとは、分析を行うのだが、本章ではRを通じて実行可能な基本的なデータ分析手法を紹介する。Rには、様々な計算を実行するための関数が用意されており（例、mean, median, sqrt 等）、これらを使えば、分析者はシンプルなコマンドで分析が可能になる。関数のは f(argument) のように関数名 f のあとにカッコをつけて表記することで利用する事ができる。なお、argument は日本では引数とよばれ、計算に必要な情報の指定である。関数の利用において作業者は具体的な関数名とそれに対応する引数を指定する必要がある。例えば、データ（列ベクトル）x の平均値を計算したい場合には、以下のようなコマンドで実行できる。 mean(x) ただし、 na.rm = TRUE はデータに欠損値がある場合に、それを無視して（欠損値でない観測値のみで）計算を行うための引数である。 Rの作業として本章では主に、1. データの読み込み（csv, excel, etc.）、2. dplyrの利用とデータ整形、3. パイプ演算子を用いた複数処理の実行、について学ぶ。なお、これらの作業は、統計的な分析を実行する前のデータ前処理としても広く使われるものなので、データ分析をしたいと考える人達にとってはとても重要なスキルになる。 分析可能な形にデータを処理した後は、データの特徴を確認することが必要になる。具体的には、記述統計や図示化を用いて、特定の変数の分布や変数間の関係について確認を行うことが重要である。この過程により、調査の背景にある実情を把握できるとともに、入手したデータ（のコーディングなど）にエラーがないかを確認することにもつながる。本章では、関数を用いた基本的な記述統計の計算はもちろん、先述のパイプ演算子を用いて、ある特徴を持つ観測における記述統計の計算などを簡単に行う方法も紹介する。 また、二変数間の関係を捉えるための基本的な指標である相関係数についても説明する。そこでは、相関係数の意味や係数の解釈における注意点についても紹介した後、データを可視化することの重要性も合わせて説明する。データの可視化においては主に、ggplot2 というパッケージを用いた方法を紹介する。本章では主にRパッケージ内に含まれているデータ例を用いて、可視化の方法を紹介するため、読者においてはぜひ自身の関心のあるデータを用いて実行してみてほしい。 "],["readData.html", "データの読み込み", " データの読み込み 本節で用いるパッケージをまだインストールしていない読者は、以下のコマンドを用いてインストールしてほしい。また、インストールを完了したら、library()関数によって各パッケージを起動し、作業に備えてほしい。 install.packages(c(&quot;tidyverse&quot;,&quot;readr&quot;,&quot;readxl&quot;)) library(tidyverse) library(readr) library(readxl) ここからは、データセットを用いた情報の取り込みとデータ処理作業を行っていく。多くの場合、R外部で作成されたデータを取り込み利用するのだが、あるソフトウェアで作成・保存されたデータセットが他の環境で利用できるとは限らないという点に注意が必要である。そのため、ソフト特性に依存しない汎用的な形式を使うことが好まれることも多い。汎用性の高いファイル形式の代表的な例がCSV (comma separated values) である。以下は、mktData.csvという架空のファイルをdfというオブジェクト名で取り込むための、見本コードである。ここで用いる関数は、readrというパッケージのread_csv() という関数である。なお、以下のコードは、実在しない ’mktData.csv’というデータセットを引数に利用した見本コードであるため、このコードをそのまま実行してもエラーを返すだけであることに注意をしてほしい。実際には、自身が利用するファイル名を指定してファイルを読み込むことになる。なお、以下のコードの2行目は、データの1行目に変数名（列名）が含まれていない場合の引数の指定方法である。また、下記コードで利用している :: は用いるパッケージを指示するための演算子である。これにより、library() を用いなくても、指定したパッケージ内の関数を利用することができる。 df1 &lt;- readr::read_csv(&quot;mktData.csv&quot;) df2 &lt;- readr::read_csv(&quot;mktData.csv&quot;, col_name = FALSE) なお、R studioデスクトップ版を利用している場合には、ファイルが格納されているディレクトリ名も指定する必要がある。Rにおいては様々なファイルを入力・出力することになるため、利用するディレクトリが一貫していないとそれだけで作業が煩雑になる。そのため、??章 で紹介している「プロジェクト機能」必ずを活用するようにほしい。 ここで用いるデータは、有斐閣ウェブサイトを通じて配布している。そのため、読者においては各自のコンピュータにダウンロードし、分析用に活用してほしい。ここではまず、分析に利用するデータを格納するディレクトリを作成するコードを紹介する。以下のコードは、プロジェクトを作成しそのためのディレクトリを指定していることを前提にしている。 具体的は、以下の通りdir.create() を使ってproject内に新たに data というディレクトリ（フォルダ）を作成する。 dir.create(&quot;data&quot;) 新たなディレクトリを作成したら、そこに、ウェブサイトよりダウンロードしたデータを入れてほしい。ここではまず “2022idpos.csv”というデータを用いる。データが無事 data ディレクトリに含まれたら、以下のコマンドによってそのデータファイルをR の作業スペースに読み込み、それに “idpos” というオブジェクト名を定義する。なお、ここで分析者はディレクトリを指定することも必要になる。また、コード内の na は、欠損値がどのように保存されているかを指定するための引数であり、もし欠損値が空欄であればnaによる指定は必要ない。 idpos &lt;- readr::read_csv(&quot;data/2022idpos.csv&quot;, na = &quot;.&quot;) 問題なくデータを読み込むことができたら、そのデータの冒頭数行を head() 関数によって表示する。head() 関数の結果によって、このデータセットのうち、4つの変数（列）について6つの観測（行）が表示されるはずである。なお、R studio 画面内の Environment タブからこのidposデータが3000行、5列のデータセットであることを確認できる。 head(idpos) ## # A tibble: 6 × 4 ## id date spent coupon ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019/9/25 14326 1 ## 2 32 2019/9/10 10232 1 ## 3 30 2019/9/9 6881 1 ## 4 29 2019/9/4 6365 0 ## 5 46 2019/9/10 7595 1 ## 6 44 2019/9/14 7858 0 また、読み込んだデータ特徴の確認は他の関数でも実行できる。例えば、name() 関数を使えば、データ内の変数名 (列名) を確認できるし、tidyverseに含まれる glimpse() 関数によってもデータの冒頭数行を含むいくつかの情報を返してくれる。 names(idpos) ## [1] &quot;id&quot; &quot;date&quot; &quot;spent&quot; &quot;coupon&quot; glimpse(idpos) ## Rows: 3,000 ## Columns: 4 ## $ id &lt;dbl&gt; 12, 32, 30, 29, 46, 44, 44, 32, 3, 34, 36, 3, 42, 18, 38, 4, 19… ## $ date &lt;chr&gt; &quot;2019/9/25&quot;, &quot;2019/9/10&quot;, &quot;2019/9/9&quot;, &quot;2019/9/4&quot;, &quot;2019/9/10&quot;, … ## $ spent &lt;dbl&gt; 14326, 10232, 6881, 6365, 7595, 7858, 9405, 1821, 8375, 1828, 6… ## $ coupon &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, … なお、このidposデータは、POS (Point of sales) という小売店レジでの取引データとロイヤルティプログラムなどの会員IDを含むID-POSと呼ばれるデータを想定して簡略化し作成した、演習用人工データである。データには、小売店舗での取引日（date）、金額（spent）、クーポン利用の有無 (coupon)、性別 (gender) が含まれている。本来のPOSデータは、より詳細な日時や具体的な製品単品レベルの取引品目など、より詳細な情報が含まれているはずだが、ここでは簡単化のためにこのようなデータにしている。また、もしスプレッドシート形式で表示したい場合には View() 関数をconsoleに直接入力することでそれが可能になる。例えば、idposデータを用いて以下のようなコードを入力することで、Sourceウィンドウに新しいタブができ、そこにデータセットが表示される。 View(idpos) "],["normalization.html", "データの整形", " データの整形 tidyverse の活用 データの整形には、tidyverseパッケージ群に含まれるdplyrというパッケージを用いる。作業では、tidyverseをインストール・起動しておけばdplyrも利用できるため、特に心配する必要はない。dplyr には、いくつもの便利な関数がふくまれているが、本節では主に以下の関数および機能を紹介する。 summarize(): データセットを関数内で定義した統計量等を用いて要約された新しいデータセットに変換するための関数 mutate(): データセットに新しい変数を追加するための関数 filter(): データセットから、関数内で指示する特定のレコード（行）を残す（フィルタリング）するための関数 select(): データセットから、関数内で指示する特定の列を抽出するための関数 arrange(): データセットから、関数内で指示する順番で並べ替えるための関数 パイプ演算子 %&gt;%: 前（左）の関数の出力を次（右）の関数の入力として渡すため演算子 summarize は、ある変数の平均値や標準偏差などの記述統計量を計算することができる関数である。例えば、dataというデータセットに含まれる var_name という変数の平均値を計算し、それを M という変数名として定義する場合、以下のコマンドを用いる（以下のコマンドは見本コードである）。 summarize(data, M = mean(var_name)) mutate は、データセットに引数内で指定した定義の変数（列）を追加する関数である。例えば、以下の見本コードにような指示によって、data というデータセットに対し、definition で定義した変数をnew_varとして追加することができる。実際の分析でdefinitionを定義する場合には、様々な関数や論理式を利用する事が多い。例えば、“new_var = var1/100” という定義を用いれば、var1を1/100倍した値をnew_varとして定義することになる。また、“new_var = var1 – mean(var1)”という定義を用いれば、var1の観測値からvar1の平均値を引いた値をnew_varとしている。なお、このような操作化を一般的に「中心化」と呼ぶ。 mutate(data, new_var = definition) また、mutate関数の利用においては、条件分岐を用いた変数の作成を行うこともある。そのように、研究者がある変数の値に応じて異なる値を変数を作成するときには、mutate内で、ifelse()関数を用いる事が多い。ifelse() 内の第一引数は条件、第二引数は条件が満たされたときの処理、第三引数は条件が満たされないときの処理をそれぞれ表す。なお、特定の条件の指定には “==” （同値）, “&gt;=”（以上）, “&lt;=”（以下） を使う。具体的には、var1 が2ならば1をとり、それ以外であれば０をとるという条件でnew_varを作成するという指示は、以下のようになる（以下は見本コードである）。 mutate(data, new_var = ifelse(var1 == 2, 1, 0)) filter関数は、データから特定の条件に合致する行だけ取り出す場合に用いる関数である。例えば、男性（gender == “male”）のサンプル情報のみ抽出したい場合には以下のような指示になる。 filter(data, gender == &quot;male&quot;) なお、特定の条件以外のものを指定したいときは、 という論理式 “!=” (not equal) を使う。男性以外の行を選ぶための指示は、以下の通りになる。 filter(data, gender != &quot;male&quot;) select関数は、特定の変数（列）を選んで新たなデータフレームを作成することができる関数である。例えば、dataというデータセットから、var1、var2、var3 という変数（列）を抽出して、data2というdataframeとして定義するには、以下のような指示になる。 data2&lt;- select(data, var1, var2, var3) 反対に、取り除きたい変数を指定するときには、以下のように “-” を使う。 data2&lt;- select(data, -var1) 列の指定方法には、いくつかのやり方が存在する。並んでいる列をまとめて指定するときは:（コロン）を使う。例えば、var1からvar5までの列をまとめて抽出し、それをdata2として定義するのは以下のようにできる。 data2&lt;- select(data, var1:var5) また、tidyverseのstarts_with()（ends_with()）を使うことで、変数名の冒頭（末尾）が特定の文字列から始まる変数を指定するようなことも可能である。例えば、“v” という文字から始まる変数を取り出すための指示は、いかのようになる。 data3&lt;- select(data, starts_with(&quot;v&quot;)) arrangeは、データの並べかえを可能にする関数である。例えば、以下ではvar1の値が小さい順（昇順）に並べ替えるような指示を示す。一方で、降順にする場合は、desc(var1)と引数を指定する必要がある2。 data2 &lt;- arrange(data, var1) data2 &lt;- arrange(data, desc(var1)) また、tidyverse環境において、変数名を変更することも、rename() 関数で可能になる。以下の指示によって、var_name という変数を new_name に変更する事ができる。 data3 &lt;- rename(data2, new_name = var_name) dplyr を活用すると、パイプ演算子（%&gt;%）が使える（ショートカット: command (control) + Shift + m）。パイプ演算子は、左側の処理結果を演算子右側の関数の第一引数として利用するための指示である。たとえば、以下のコマンドではまず \\(\\small 10-6\\) が計算され、その結果である “4” が sqrt() の引数として利用される（sqrt(4) は 2）。 (10-6) %&gt;% sqrt() ## [1] 2 パイプ演算子は、複数のデータ操作処理を連続して行う際に便利である。例えば、顧客の情報を含むデータセット(data)から、男性に該当する情報のみを抽出し、var1(例、購買額)についてのランキングを作成したうえでいくつかの変数を含んだデータセット（new_data）を作成する場合を考える。その際に実行すべき作業とそれらに対応する関数は以下のように示すことができる。 男性の情報だけ抜き出す(filter) Var1の値について降順に並べ替える(arrange) 第一位から最下位までの順位を割り当てた ranking 変数を作る(mutate) var1 , var2, var3, var4, rankingだけ残し(select) new_dataとして定義する 上記の作業を一気に行うためのコードをパイプ演算子を使わずに書くと以下の様になる（以下は見本コード）。 new_data &lt;- select(  mutate(   arrange(    filter(data, gender == &quot;male&quot;),    desc(var1)),    ranking = 1:n()),   var1, var2, var3, var4, ranking) パイプ演算子を使わない場合、先に実行する処理が内側に来ており、一見して何を行っているのか理解するのが難しい。一方でパイプ演算子を使い、左側の処理結果を演算子右側の関数の第一引数として利用すると、以下のように書き換えることができる。 new_data &lt;- data %&gt;% filter(gender == &quot;male&quot;)%&gt;% arrange(desc(var1)) %&gt;% mutate(ranking = 1:n()) %&gt;% select(var1, var2, var3, var4, ranking) パイプ演算子の利用により、各関数の処理を一つの行で示せる。また、処理の順番通りに関数を記載することが可能なので、コードの記述容易性と可読性の両方が高まる。また、パイプ演算子による操作は次の関数の第一引数以外に反映されることも可能である。第一引数以外の引数に左側の処理結果を反映させる際には、該当する箇所に “.” （ドット）を使う。たとえば、\\(\\small 10-2\\)の計算結果を用いて2から8までの偶数で構成されるベクトルを返すためのコードは以下のように書くことができる。 (10-2) %&gt;% seq(from = 2, to = ., by = 2) ## [1] 2 4 6 8 データの整形・処理作業が終わったら、そのデータを自身のコンピュータ内のストレージに保存したいと考えるかもしれない。Rでは、外部への書き出しという形でデータを保存することが可能である。例えば、df という名前のデータフレームをnew_dataというファイル名で、dataというディレクトリにcsv形式を用いて保存するためには、以下のようなコードを用いる（以下は見本コード）。また、csv以外にもファイル形式は選択可能であり、例えばRのデータ形式(.Rds)で保存する場合には、“#Rds” 以降のコードを用いる。 readr::write_csv(df, path = &quot;data/new_data.csv&quot;) #Rds readr::write_rds(df, path = &quot;data/new_data.Rds&quot;) 企業データの処理 これまでに学んだデータ処理の手法を実行するために、本節では、 MktRes_firmdata.xlsxデータを用いる。このデータをwebサイトより data ディレクトリにダウンロードし、以下の要領で読み込んでほしい。 firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) このデータは、小売・サービス分野の企業約160社（企業数は年によって異なる）に関する2010年から2019年までの財務データである（計1440件）。このデータは、日本生産性本部における顧客満足度調査の対象になっている企業リストを作成し、その企業の中から金融領域の企業や、データを入手できなかった一部の企業を教育的意図から排除したものである。したがって、日本の小売・サービス分野において全国的に知名度のある代表的な企業の財務データ（の一部）だと考えられる。 なお、本データには以下の変数が含まれており、データ内の単位は従業員数（人）を除き百万円である。 fyear: 決算年 legalname: 企業名 ind_en: 日経業種名（英文） parent:親会社名（もしあれば） fiscal_month: 決算月 current_liability: 流動負債 ltloans: 長期借入金 total_liability: 負債合計 current_assets: 流動資産 ppent: 有形固定資産 total_assets: 資産合計 net_assets_per_capital: 純資産合計／資本合計 sales: 売上高 sga: 販売費及び一般管理費 operating_profit: 営業利益 net_profit: 当期純利益 pnet_profit: 親会社株主に帰属する当期純利益（連結）／当期利益（単独） re: 利益剰余金 adv: 広告・宣伝費 labor_cost: 人件費 rd: 研究開発費 other_sg: その他販売費及び一般管理費 emp: 期末従業員数 temp: 平均臨時従業員数 tempratio: temp/(emp+temp) indgrowth: 産業成長率 adint: 広告集中率（adv/sales） rdint: 研究集中率（rd/sales） mkexp: (sga - rd) / sales op: operating_profit / sales roa: pnet_profit / total_assets 本データセットは、複数年にわたる複数サンプルからのデータであり、一般的にこのような構造のデータをパネルデータという。パネルデータの分析の概要は ?? 節で紹介している。 ここではこのデータを用いて、以下の作業を行う。 2018年度のデータのみを抽出する。 企業名、年、売上高、人件費、期末従業員数、平均臨時従業員数のみの変数を含むデータセットにする。 労働単価（人件費/（期末従業員数+平均臨時従業員数））変数を作成する。 労働単価の高い順に並び替えてトップ10企業を出力する。 firm2018_check &lt;- firmdata %&gt;% filter(fyear == 2018) %&gt;% select(legalname, fyear, sales, labor_cost, emp, temp) %&gt;% mutate(wage = labor_cost/(temp+emp), na.rm=TRUE) %&gt;% arrange(desc(wage)) head(firm2018_check, n = 10) ## # A tibble: 10 × 8 ## legalname fyear sales labor_cost emp temp wage na.rm ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 株式会社リクルート 2018 2310756 388583 45856 2449 8.04 TRUE ## 2 株式会社 大丸松坂屋百貨店 2018 459840 62692 6695 3581 6.10 TRUE ## 3 株式会社 帝国ホテル 2018 58426 17307 1940 998 5.89 TRUE ## 4 株式会社 髙島屋 2018 912848 83779 7761 8849 5.04 TRUE ## 5 株式会社コメリ 2018 346862 43991 4646 4777 4.67 TRUE ## 6 株式会社オートバックスセブン 2018 213840 22139 4171 747 4.50 TRUE ## 7 株式会社ロイヤルホテル 2018 40884 13115 2049 894 4.46 TRUE ## 8 オルビス株式会社 2018 248574 28555 4181 2330 4.39 TRUE ## 9 株式会社ファンケル 2018 122496 15103 1381 2213 4.20 TRUE ## 10 近畿日本ツーリスト株式会社 2018 411821 38186 6956 2189 4.18 TRUE このように、データの中から研究課題と整合的な情報を抽出したり、変数を作成したりすることができる。ただし、研究者にとって都合の良い結果を得るために恣意的に用いるデータを制限、操作することは、研究不正となる。そのため、実際の研究では、どのようなデータ・情報を用いるかについて事前に計画しておく必要がある。 “desc”は、descending orderの略であり、降順を表す。↩︎ "],["summary.html", "データの要約と可視化", " データの要約と可視化 ここからは、記述統計や可視化によってデータを要約する方法について説明する。記述統計では、統計量と呼ばれる指標を用いてデータの特徴を数値から把握する。一方で可視化においては、図表を作成することでデータの特徴を視覚的に理解することを目的とする。実証的なマーケティング研究においては、データを用いた仮説の検証という方法が主流かもしれないが、仮説検証に用いるデータはどのようなものなのかを要約し、それを（論文やレポートの）読者へ伝えるプロセスは必要である。記述統計やデータの可視化は、このプロセスにおいて機能する方法である。 記述統計 記述統計の利用においては、データのタイプ別に利用すべき統計量が異なることに注意が必要である。データには量的変数とカテゴリ（を示す質的）変数があるが、量的変数は数値で測定できるものであり、その計算結果を解釈することも可能である。一方でカテゴリ変数は、各観測個体が属している状態やグループを表す指標であり、それを計算してもそこから含意を得るのが難しい。Rのような統計ソフトは非常に素直なので、たとえカテゴリ変数であってもそこに数値が入力されていれば、記述統計に必要な計算を実行し、結果を返してくれる。しかしながら研究においてはそれらの結果を適切に解釈する必要があり、自身が用いている変数のタイプに応じた分析を実行する必要がある。 その上で本節ではまずひとつの量的変数の情報を要約するための記述統計を紹介する。一つの数値によってデータ全体を代表させるような数値を代表値と呼ぶ。代表値は主に、データの中心を示す指標と考えられる。本節ではデータの中心を表す指標として中央値 (median) と平均値 (mean) を紹介する。中央値は、データのすべての観測値において、その値より小さな観測値の数と大きな観測値の数が等しくなるような真ん中の値を表す。そのため、（1, 3, 2, 5, 4）というデータにおける中央値は3である。これは、このデータを、1, 2, 3, 4, 5 と並べ替えると、3よりより小さな観測値の数と大きな観測値の数が等しくなっていることから確認できる3。 d &lt;- c(1, 3, 2, 5, 4) median(d) ## [1] 3 d2 &lt;- c(1, 3, 2, 5, 4, 6) median(d2) ## [1] 3.5 平均値（算術平均と呼ばれる）は、最もよく使われる代表値の一つである。平均値は、n個のデータ、\\(\\small x_1,x_2,...,x_n\\) に対して以下のように定義される。 \\[\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\] 観測値と平均値の差（\\(x_i - \\bar{x}\\)）は偏差と呼ばれ、偏差の和はゼロである（\\(\\sum_ix_i - \\bar{x}=0\\)）という性質を持つ。つまり、平均値を中心として、データの正の方向へのばらつきと負の方向へのばらつきが釣り合いが取れているということが伺える。この点が、平均値がデータの中心を表す代表値として用いられるひとつの理由である。また、平均値にはいくつかの好ましい統計的性質があるのだが、それについては後述する。Rにおいては、mean() 関数を用いることで分析が可能である。例えば、9人の生徒に対して行われた数学(x)と国語(y)のテスト(10 点満点)の結果が、それぞれ以下の通りであったとしよう。 数学: (3,3,5,5,5,5,5,7,7) 国語: (2,3,3,5,5,5,7,7,8) このときの平均値は以下のように求まる。 math &lt;- c(3,3,5,5,5,5,5,7,7) jpn &lt;- c(2,3,3,5,5,5,7,7,8) mean(math) ## [1] 5 mean(jpn) ## [1] 5 計算の結果、どちらも平均値は5であった。データの中心を表す代表値の値が等しかったため、これら2科目のテスト結果は同じ分布を持つと判断して良いのだろうか。自明かもしれないが、そのような解釈は不適切である。具体的には、データの「ばらつき」についても確認する必要がある。分布のばらつきは、平均値からの離れ方(平均値からの偏差) によって判断される事が多く、これが大きなデータが多い場合は、よりデータは散らばっ て分布していると解釈される。一方でデータが平均の近くに集まって分布している場合、ばらつきが小さいと捉えられる。この分布のばらつきは主に、分散や標準偏差という指標で測られる。 分散 (Variance, \\(S^2\\)で定義する) は以下のように、平均からの偏差の二乗の和をデータ数で割ったものだと定義される。平均からの偏差の和を計算すると、正の方向へのズレとマイナス方向へのずれがあるので、互いに相殺しあって合計は 0 になる。そこで、偏差の二乗和を用いることでデータ全体がどの程度平均からばらついているかを把握する。 \\[S^2 = \\frac{1}{n}\\sum_i^n (x_i-\\bar{x})^2\\] しかしながら、分散は元の値を二乗しているのでもとのデータと単位が異なる。そのため、分散の正の平方根 (\\(\\sqrt{\\cdot}\\)) を取った値を標準偏差と呼び、この標準偏差を用いることも多い4。なお、Rでは var() と sd() によって分散と標準偏差をそれぞれ求める。ただし、Rの関数による計算では \\(s^2=\\frac{1}{n-1}\\sum_i^n (x_i-\\bar{x})^2\\) で定義される「不偏標本分散」および「不偏標準誤差」という指標を用いる。これらの指標は、不偏性（??節参照）という統計的に好ましい性質を持っているため、分析ソフトではこちらの計算方法が用いられる。そのため、Rを用いた分散の計算値が n で割った際の手計算値と異なることがあるのでその点には注意が必要である。 var(math) ## [1] 2 var(jpn) ## [1] 4.25 先程の数学と国語のテスト結果データを用いて分散を計算すると、国語の方が分散が大きいことがわかる。つまり、両テストとも平均値は同じであるものの、国語のほうがそのスコアのばらつきが大きいことがわかる。このように、代表値とともにデータのばらつきに関する情報も踏まえてデータの特徴を把握することが好ましい。 観察されたデータと標準偏差を用いて、特定の観測結果がデータ内において「相対的に」どのような位置にいるのかを捉えることも可能になる。具体的には、任意の量的変数 \\(x_1,...,x_n\\) に対して、標準化されたスコア \\(z_1,..,z_n\\) は以下のように定義できる。 \\[ z_i=\\frac{(x_i-\\bar{x})}{\\sqrt{(S^2)}} \\] ただし、 \\(S^2\\) は変数 \\(x\\) の分散である（不偏標本分散を用いることもある）。上記定義の通り、標準化スコアは観測値の平均からの偏差を標準偏差で割っており、ある観測が平均値から標準偏差何個分ズレているかを示していると解釈できる。なお、標準化スコアは、平均が0、分散が1になることも知られている。 一方でデータの観測数（ \\(n\\) ）が偶数である場合、\\(\\small n/2\\) 番目と、\\(\\small (n/2)+1\\) 番目が中央となるため、n個のデータの観測値を、\\(x_1,x_2,...,x_n\\) とすると、これらふたつの値の平均値（ \\(\\small \\frac{x_{\\frac{n}{2}}+x_{ \\frac{n}{2}+1}}{2}\\) ）が中央値となる。Rにおいてはmedian() 関数によって以下のように計算することができる。↩︎ 偏差の二乗和のかわりに偏差の絶対値を用いた平均偏差という指標も存在する。しかしながら、分散や標準偏差のほうが好ましい統計的性質を持つことから、二乗和が用いられることが多い。↩︎ "],["categoricalVar.html", "カテゴリ変数の要約", " カテゴリ変数の要約 一方でカテゴリ変数は、代表値や分散によって含意を得るのではなく、頻度のカウント（集計）や、クロス集計を用いることが多い。これにより、各カテゴリにどれぐらいの観測数があるのかを確認することが可能になる。カテゴリ変数の内容（出現頻度）の確認には、table() 関数を用いる。また、with()関数を用いて同様の結果を得ることも可能である。ここでは、先ほど用いた firmdata から2018年度の情報を抽出し、日経業種に基づく産業の違いから、どのカテゴリの企業がどれだけデータ内にいるのかを確認する。なお、tidyverseを起動していない場合には、必要に応じて library(tidyverse) を事前に指示してほしい。 firm2018 &lt;- firmdata %&gt;% filter(fyear == 2018) table(firm2018$ind_en) ## ## Air Transportation Amusement Services Bakery Products ## 8 4 1 ## Communication Services Cosmetics &amp; Toilet Goods Department Stores ## 2 3 7 ## Foods, NEC Home &amp; Pre-Fabs Hotels ## 1 2 5 ## Miscellaneous Services Miscellaneous Wholesales Motor Vehicles ## 27 2 4 ## Musical Instrument Railroad (Major) Railroad (Minor) ## 1 27 2 ## Real Estate - Sales Retail Stores, NEC Supermarket Chains ## 1 35 14 ## Trucking ## 1 with(firm2018, table(ind_en)) また、table関数にて2つのカテゴリ変数を指定することで、両変数に対応するカテゴリの出現頻度を返してくれる。このような表のことをクロス集計表とよぶ。例えば、同データにおける広告集中的な企業を把握するため、広告集中度が中央値よりも高ければ1、それ以外であれば0を取るダミー変数（??節参照）を作成し、各産業カテゴリとの関係を確認する。 firm2018 &lt;- firm2018 %&gt;% mutate(ad_dummy = ifelse(adint &gt; median(adint),1, 0)) with(firm2018, table(ind_en,ad_dummy)) ## ad_dummy ## ind_en 0 1 ## Air Transportation 4 4 ## Amusement Services 4 0 ## Bakery Products 0 1 ## Communication Services 1 1 ## Cosmetics &amp; Toilet Goods 0 3 ## Department Stores 0 7 ## Foods, NEC 0 1 ## Home &amp; Pre-Fabs 0 2 ## Hotels 5 0 ## Miscellaneous Services 17 10 ## Miscellaneous Wholesales 1 1 ## Motor Vehicles 0 4 ## Musical Instrument 0 1 ## Railroad (Major) 27 0 ## Railroad (Minor) 2 0 ## Real Estate - Sales 0 1 ## Retail Stores, NEC 11 24 ## Supermarket Chains 2 12 ## Trucking 1 0 上の表では、各行に産業名が記載されており、その右隣に、広告集中度が低い（ad_dummy=0）企業数が、さらにその右隣には広告集中度が高い（ad_dummy=1）企業数がそれぞれ記載されている。これらのデータを確認すると、鉄道会社やアミューズメント、ホテル、トラック運送業において広告集中度が高い企業が少ないことがわかる。それ以外では産業内でも広告集中度の高い企業と低い企業とが比較的バラけている。 特定のカテゴリに着目して、カテゴリ変数についての集計を行うことも可能である。例えば、広告集中度が高い企業における産業のばらつきを調べたいときには、filter() 関数を用いれば良い。 firm2018 %&gt;% filter(ad_dummy == 1) %&gt;% with(table(ind_en)) ## ind_en ## Air Transportation Bakery Products Communication Services ## 4 1 1 ## Cosmetics &amp; Toilet Goods Department Stores Foods, NEC ## 3 7 1 ## Home &amp; Pre-Fabs Miscellaneous Services Miscellaneous Wholesales ## 2 10 1 ## Motor Vehicles Musical Instrument Real Estate - Sales ## 4 1 1 ## Retail Stores, NEC Supermarket Chains ## 24 12 カテゴリ変数と量的変数の関係を調べることも、グループ別に量的変数の要約を行う形で可能である。具体的には、 group_by() 関数を用いる。group_by() は関数内で指定した変数を用いてデータをグループ化し、グループごとの集計や処理を可能にする関数である。例えば、売上高と広告集中度の平均と標準偏差を産業ごとに確認することは、以下のような指示で可能になる。 firm2018 %&gt;% group_by(ind_en) %&gt;% summarize(obs = n(), sales_m = mean(sales), sales_sd = sd(sales), adint_m = mean(adint), adint_sd = sd(adint)) ## # A tibble: 19 × 6 ## ind_en obs sales_m sales_sd adint_m adint_sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Air Transportation 8 1772786. 305240. 0.00311 0.00333 ## 2 Amusement Services 4 298138. 263017. 0 0 ## 3 Bakery Products 1 1059442 NA 0.0122 NA ## 4 Communication Services 2 547088. 172736. 0.0231 0.0327 ## 5 Cosmetics &amp; Toilet Goods 3 140669. 100063. 0.108 0.0498 ## 6 Department Stores 7 843248. 348819. 0.0195 0.00535 ## 7 Foods, NEC 1 504153 NA 0.0229 NA ## 8 Home &amp; Pre-Fabs 2 4143505 0 0.00857 0 ## 9 Hotels 5 62135. 58060. 0 0 ## 10 Miscellaneous Services 27 311867. 456037. 0.0114 0.0204 ## 11 Miscellaneous Wholesales 2 176520 52778. 0.0203 0.0287 ## 12 Motor Vehicles 4 5279122. 4233188. 0.0254 0.00404 ## 13 Musical Instrument 1 434373 NA 0.0443 NA ## 14 Railroad (Major) 27 1302921. 1037834. 0 0 ## 15 Railroad (Minor) 2 260502 0 0 0 ## 16 Real Estate - Sales 1 1861195 NA 0.0114 NA ## 17 Retail Stores, NEC 35 571019. 547247. 0.0243 0.0272 ## 18 Supermarket Chains 14 4335164. 3511347. 0.0147 0.00782 ## 19 Trucking 1 1118094 NA 0 NA このように、カテゴリごとの量的変数の要約も実行可能である。なお、標準偏差が NA となっている箇所は、観測数が 1 であり、標準偏差を計算できない状況を表している。 "],["visualization.html", "データの可視化", " データの可視化 本サイトでのデータの可視化では、主にtidyverse内に含まれる ggplot2 というパッケージを用いる。データの可視化では、円グラフ、折れ線グラフ、帯グラフなどの様々なグラフを用いて視覚化されることも多いだろう。しかしなが本節では、主にヒストグラム、箱ひげ図、バイオリンプロットをRでの実行例とともに紹介する。これらの図は、量的変数の分布を視覚的に示すことについて優れた可視化の方法だと言える。ここでは、ggplot2に内包されている diamonds データを用いて可視化を学ぶ（tidyverseを起動することで自動的に ggplot2も起動されるため、このタイミングでtidyverseを起動していない場合には、必要に応じて library(tidyverse) によってパッケージを起動してほしい）。diamonds データについては以下のように確認できる。 head(diamonds) ## # A tibble: 6 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 なお、Macのデスクトップ版でggplot2等を使うと日本語が文字化けするので、Macユーザーは別途以下のコマンドを実行する必要がある。 #For mac users theme_set(theme_gray(base_size = 10, base_family = &quot;HiraMinProN-W3&quot;)) ここではまず、ggplot2の ggplot() 関数を用いて図示化のためのオブジェクトを作成する。この関数では、以下の引数を指定する。 data: 可視化に用いるデータフレームの指定 mapping: データから抽出する変数と画面に表示される図との関係の指定 mapping内で、aes() 関数（aesthetics）で視覚化に用いる変数とプロット要素間の接続を図ることも多い。 aes() 関数は、データ内の変数がをどのように視覚的情報（美的特性）にマッピングするか（例えば、x軸とy軸の変数はなにか）を特定するための関数である。こらの引数により、ggplot関数で作成された図示化オブジェクトには、着目するデータと変数が特定される。 続いて、ggplot()で作られたオブジェクトに対して、geom (geometry) に関する情報を 追加し、グラフィックの層(layer)を加えることで図を作成する。このプロセスでは、geom_point() による散布図や、geom_histogram() によるヒストグラムなど、具体的な図表のタイプに対応する関数を利用することで、図を作成できる。また、geomに関する関数以降に labs() というラベルに関する関数を追加することで、図に必要な情報を加筆することが可能になる。 ggplot2を用いたデータ可視化の例として、まずはヒストグラムを描画する。ヒストグラムはデータの分布を離散的に示すものであり、連続変数を階級で分けて各階級の頻度を図示化する。ヒストグラムは一つの変数を扱った図なので、mapping引数ではひとつの変数を指定する。その上で作成した図示化オブジェクトに geom_histogram() を追加することでヒストグラムを描画する。以下では、ダイアモンドの価格の観測頻度についての可視化例を紹介する。以下の図では、価格の程度を離散的に区切り、その区切られた各範囲の価格を取る観測がデータ内にどれだけ存在するかを示している。 p1 &lt;- ggplot(diamonds, mapping = aes(x = price)) p1 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム1: ダイアモンド価格&quot;) なお、縦軸を確率密度(density)に変えるときは、geom_density()を用いる。その際、fillという引数を設定すると、密度を範囲に色を塗ることができる (なお、“p1” というオブジェクトは再利用できるので、再びggplot()によって指定する必要はない)。 p1 + geom_density(fill = &quot;black&quot;, alpha = 0.5) + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム2: ダイアモンド価格（geom_density）&quot;) 次に、箱ひげ図の作り方を紹介する。箱ひげ図は、四分位数と四分位範囲等を図示化したものである。四分位数はデータを4等分する区切りの値であり、第一四分位はQ1、第二四分位はQ2、第三四分位はQ3、最大値はQ4で示される。四分位範囲はQ3-Q1の範囲で示されるものである。ここでは、Cutの質（Fair, Good, Very Good, Premium, Ideal）ごとに価格の分布を比べるため、複数の箱ひげ図を並べる例を提示する。 p2 &lt;- ggplot(diamonds, mapping = aes(x = cut, y = price)) p2 + geom_boxplot() + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;箱ひげ図1: ダイアモンド価格&quot;) 箱ひげ図を作成すると、ひげの上下に点が表示されることがある（上図では上部が太線のように見えている）。これは、外れ値の候補として全体の分布から離れて存在する観測値が示されている。ここで示される外れ値の候補は、Q1よりも四分位範囲\\(\\times 1.5\\) 以上小さい、ないしは、Q3よりも四分位範囲\\(\\times 1.5\\) 以上大きいかで特定される。外れ値がある場合、入力ミスなどのエラーではないか、異質な観測値でないか、を検討、確認することが必要になる。 次に紹介する図のタイプはバイオリンプロットである。バイオリンプロットは、箱ひげ図よりももう少し詳しくデータの分布を確認できる図である。ggplot2では、geom_violin() を用いる。例えば、先程の箱ひげ図をバイオリンプロットで示すと、以下のようになる。以下の図は、バイオリンプロット内に箱ひげ図を示すことでよりわかりやすい図を作成するように工夫している。 p2 + geom_violin() + geom_boxplot(fill = &quot;gray&quot;, width = 0.1) + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;バイオリンプロット: ダイアモンド価格&quot;) バイオリンプロットで横に広がっているところは、ヒストグラムで言う山が高いところを意味しており、そこに多くのデータが集まっていることを示している。 "],["cor.html", "二変数間の関係の要約", " 二変数間の関係の要約 ここまでの内容は（カテゴリ変数に関する一部の説明を除き）、一つの変数に関する要約と可視化を扱っていた。しかし、データ分析では二つの異なる変数間の関係を捉えたいと考えることも多い。二変数間の関係を数量的に要約するための指標の代表例が共分散や相関係数である。データ数をnとする変数xとyの共分散（\\(S_{xy}\\)）は、以下のように定義される。なお、Rで共分散を求める際には cov() 関数を用いる。 \\(S_{xy}=\\frac{1}{n}\\sum_i^n (x_i-\\bar{x})(y_i-\\bar{y})\\) また、\\(S^2_x\\)と\\(S^2_y\\)をそれぞれxとyの分散としたとき、相関係数（\\(\\rho_{xy}\\)5）は以下のように定義される。Rで相関係数を求める際には cor() 関数を用いる。 \\(\\rho_{xy}=\\frac{S_{xy}}{\\sqrt{S_x^2}\\cdot \\sqrt{S_y^2}}\\) 共分散は、二つのデータ間の共変動を示す指標であるものの、この数値を持って我々研究者が二変数の関係について（例えばその強弱などを）解釈するのは困難である。そこで、二変数間の関係を数値的に解釈する場合には、一般的に相関係数を用いる。相関係数は、-1 から 1 までの値を取り、正の値を取る場合は正の相関、負の値を取る場合は負の相関を示している。また、相関係数が正（負）の値かつ1に近いほど強い正（負）の相関であることが知られている。ただし、相関係数で表される二変数間の関係は、どれだけ線形関係に近いかである。言い換えると、相関が高いとはデータがどれだけ直線上に集まって分布しているかを示しており、グラフ等で示される線形関係の傾きについては何も回答することができないという点に注意が必要である。 例えば、以下のようなデータセットを考える。 X &lt;- tibble(x1 = c(-3, -1, 0, 2, 5), y1 = c(16, 12, 10, 6, 0), y2 = c(8, 6, 5, 3, 0)) X ## # A tibble: 5 × 3 ## x1 y1 y2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -3 16 8 ## 2 -1 12 6 ## 3 0 10 5 ## 4 2 6 3 ## 5 5 0 0 ただし、tibble はより大規模なデータ操作が容易になる特性を持った、tidyverse 上で用いられるデータフレームの形式である。tibble() 関数を用いることで、オブジェクトとなるデータフレームをtibbleとして定義することができる。 以下の計算で示すように、このデータセットにおける x1 と y1 の相関係数は -1 であり、両者の関係を図で示すと、すべてのデータが直線上（\\(y=-2x+10\\)）に並ぶことがわかる。なお、$ はデータフレーム内の変数名を指定するための記号である（データ$変数）。一方で、x1 と y2 との相関係数も -1 であるものの、両者の線形関係は \\(y=-x+5\\)である。このことからも、相関係数が線形関数の傾きや切片についての情報は何も持たないことがわかる。なお、geom_point() は散布図を描くための関数であり、geom_smooth() は引数method で指定する方法で、データに関する近似線を描画するための関数である。ここでは、method = lm として、最小二乗法で求めた直線を描画している。 cor(X$x1, X$y1) ## [1] -1 ggplot(data = X, mapping = aes(x = x1, y = y1)) + geom_point() + geom_smooth(method = lm) また我々は、二変数間の相関係数がゼロであることが、両者が無関係であることを意味しないことにも注意をしなければならない。例えば、以下のようなデータセットにおけるA と B の相関は 0 になる。 ## # A tibble: 5 × 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -2 4 ## 2 -1 1 ## 3 0 0 ## 4 1 1 ## 5 2 4 cor(AB$A, AB$B) ## [1] 0 しかしながら、両者の関係を描画すると、\\(y = x^2\\) という二次関数の関係にあることがわかる。つまり、相関係数がゼロだからといって、二つの変数間に関係がないと結論づける事はできず、相関ではなく異なる複数の分析アプローチによって関係を特定していくことが必要になる。 ggplot(data = AB, mapping = aes(x = A, y = B)) + geom_point() + geom_smooth(method = lm, formula = y ~ x + I(x^2), se = FALSE) 二変数間の関係についての可視化もggplot2にて対応できる。具体的には、geom_point()という関数を用いるのだが、mappingに対する引数として、x と y 二つの変数を指定することが必要になる。ダイアモンドの価格は、カラット数に大きく依存すると考えられる。そこで、以下のようにカラット数と価格との間の共分散と相関係数を計算する。 cov(diamonds$carat,diamonds$price) ## [1] 1742.765 cor(diamonds$carat,diamonds$price) ## [1] 0.9215913 これらの変数間の相関係数は約0.92であり、高い正の相関関係であることが確認された。続いて、これらの変数の関係を可視化する。二変数間の関係を端的に可視化する方法が散布図である。散布図は、一方の変数を横軸に、もう一方の変数を縦軸に取り、各データのそれぞれの値の組み合わせをプロットしたものである。 p3 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price)) p3 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図1: カラット：価格&quot;) 研究目的によっては、二つの変数間の関係をカテゴリごとに比較したい場合もあるだろう。例えば、我々はカラットと価格の関係は、カットの質によって変わるのか、という問いに関心があるとしよう。その場合には、(1) 同一図内にてカテゴリごとに色分けする方法と、(2) カテゴリごとに分割して図示化する方法がある。それぞれのggplot2での実行方法は、以下のとおりである。 Mapping = aes() 内に、 color = categ_varと指定することで、categ_var変数のカテゴリに基づき色分けする。 facet_grid() や facet_wrap() を用いる。 まず、(1) の図内での色分け方法は、以下のようなコマンドで実行できる。 p4 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price, color = cut)) p4 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, color = &quot;カット&quot;, title = &quot;散布図 2: カット別、カラット：価格&quot;) このように、mapping = aes() 内にて色付けに関する引数を設定することで散布図内の観測値を色分けできる。ただし、ここで重要なのは、color =という引数では、カテゴリ変数を指定すべきであり、色そのもの（例えば、redやblue）を指定するものではないということである。しかしながら、散布図 2のように多くのカテゴリが含まれる場合にはこの可視化の方法適さないかもしれない。そこで、以下の方法を紹介する。facet_wrap() を用いた図の作成では、散布図 2のように color 引数を指定する必要はなく、p3 を再利用できる。facet_wrap()は、関数内で指定した変数（~cut）に基づいて図を分けて描画するように指示ができる。geom_point() で散布図作成の指示を与えたあとに、facet_wrap() のレイヤーを重ねる指示を与えれば、散布図 3が作成される。 p3 + geom_point() + facet_wrap(~cut) + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図 3: カット別、カラット：価格&quot;) 散布図 3をみると、基本的にはカラット数と価格には正の相関があるものの、カットの質が低い（例、Fair）場合にはばらつきが大きいことがうかがえる。 これまでに学んだdplyrによるデータ処理方法をパイプ演算子でつなげることで、特定の群のみを対象にした図示化も容易になる。ここでは例として、1.00カラット以上と未満とで分けて、それぞれのヒストグラムを作成してみる。 p5 &lt;- diamonds %&gt;% filter(carat &gt;= 1.0) %&gt;% ggplot(mapping = aes(x = price)) p5 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット以上&quot;) p6 &lt;- diamonds %&gt;% filter(carat &lt; 1.0) %&gt;% ggplot(mapping = aes(x = price)) p6 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット未満&quot;) Rで図を作成したら保存（出力）したいと考えることも多いだろう。日本語を使っていない図はggsaveを使い簡単に保存できる。具体的には、まず、作成した図そのもの（図示化のためのggplot() オブジェクトではない）をオブジェクトとして定義（例、plot1）する。ggsaveの使用例は以下の様になる (以下は見本コード)。 ggsave(filename = &quot;plot1.pdf&quot;, plot = plot1, width = 10, height = 5, units = &quot;cm&quot;) 日本語を含む頭の場合、quartz() を用いた以下の手順を経て図を保存する。 1. quartz()で作図デバイスを起動する。 2. 作図デバイスを開いたまま、Rstudio内で図を表示する。 3. dev.off()という指示で作図デバイスを閉じることで図が保存される。 また、Rstudio内のplotタブから、クリック-バイ-クリックで実行することも可能である（Export -&gt; Save as Image/ Save as PDF -&gt; Directory -&gt; File name）。 なお、\\(\\rho\\)はローと読む↩︎ "],["ch3reference.html", "参考文献", " 参考文献 倉田博史・星野崇宏（2011）「入門統計解析」、新世社. 高橋将宜・渡辺美智子 (2017). 「欠測データ処理」, 共立出版. 松村優哉・湯谷啓明・紀ノ定保礼・前田和寛（2021）「改訂2版 RユーザのためのRStudio[実践]入門〜tidyverseによるモダンな分析フローの世界」，技術評論社. Healy, Kieran (2018) Data Visualization: A Practical Introduction, Princeton University Press. "],["test.html", "基礎統計学復習 ", " 基礎統計学復習 "],["ch4Intro.html", "本章の概要", " 本章の概要 本章では、基礎的な統計学と、統計的推定・検定について説明する。前章では、データを集めて分析を行うことで、ある変数の平均値などの統計量を計算できることを学んだ。しかし、その計算された数値にどのような意味があるのかを解釈するのが難しい場合もある。例えば、ある変数の平均値を異なるグループ（例えば、性別）それぞれで計算したとする。このとき、ほとんどの場合においてグループ間で同じ値を取ることはないと考えられる。しかしながら、この違う値が誤差の範囲なのか、意味のある（誤差を考慮しても無視できないほど大きな）差なのかについて検討することは重要である。このような目的を達成するために利用されるのが、、統計的な分析（区間推定や検定）である。 Rを用いて、統計的な分析（区間推定や検定）を実行すること自体はさほど難しくない。基本的な分析に必要な関数は基本パッケージに搭載されており、コードの書き方（引数の設定など）もネット上で検索すれば容易に知ることができる。しかしながら、自身もしくは他者が実行した分析をきちんと理解するためには、基礎的な統計学の内容を理解している必要がある。本章では、基礎統計学に関する説明を提示する。なお、本書で紹介する内容はあくまで簡易的な内容であるため、統計学を未習の場合は基礎統計学の図書を用いて学習することを強く推奨する。また、章末に統計学や計量経済学の学習に役立つ参考文献を提示しているので、各自の学習に役立ててほしい。 本章で扱う内容は主に、標本から得た情報に基づき母集団の性質について推測するアプローチを想定したものである。具体的には、以下の内容を含める： 確率モデルと期待値・分散 統計的推測と点推定・区間推定 統計的仮説検定 平均値に関する検定・分散分析 検定力分析とサンプルサイズ 本章の後半では様々な検定方法を紹介するが、本書ではこれらの分析手法について記憶するよりも、マーケティング領域の学生において特に誤解の多い統計的分析に関する基本的な性質について理解してほしいと考えている。第一に、確率変数によって定義された統計量もまた確率変数であるという点である。データを収集し、分析を実行し統計量を計算すると、自身が観察した値が唯一の値であると考えてしまうかもしれない。しかし、母集団とは確率分布であり、標本はその確率分布に従う確率変数、データはその確率変数の実現値だと解釈できる（倉田・星野、2024）。そのため、自身が観察した推定値もまた確率変数の実現値であるということを理解することが大切である。 第二に、信頼区間についてである。統計的な分析では、信頼区間の計算を行うことも多い。しかしながら、信頼区間は誤解のもとで解釈される場合も多いため、注意が必要である。信頼区間は、未知パラメータ（母平均等）を一定の確率（信頼水準）で含む区間を示す。信頼区間には確率を割り当て、例えば95%信頼区間のようなものを計算するのだが、よくある誤った解釈として「母集団の期待値（未知パラメータ）は95%の確率でxx以上、yy以下という区間に含まれる」というものである。基礎的な統計学の枠組みでは、未知パラメータは確率変数ではない。そのため、未知パラメータに確率を割り当てるような表現は誤りである。確率変数であるのは、推定された区間の両端である。 第三に、統計的仮説検定についてである。統計的仮説検定ではまず、母集団の統計的特徴に関する記述である「帰無仮説」と「対立仮説」を定める必要がある。特に帰無仮説は検定における分析や考察の基準となり、帰無仮説を棄却するか否かによって仮説検定の判断を行う。通常、「棄却する」という言葉は帰無仮説に対してのみ用いるため、論文の中で利用している理論仮説や作業仮説に対しては使わない。また、統計的仮説検定では、帰無仮説が真であるにもかかわらず帰無仮説を棄却してしまう第一種の誤りと、帰無仮説が真ではないにもかかわらず、帰無仮説を採択してしまう第二種の誤りとが存在する。第一種の誤りを起こす確率は有意水準という。統計的仮説検定では、事前に設定した有意水準の分だけ（慣習として5%や1%といった基準が採用される）第一種の誤りを起こす確率を許容したうえで帰無仮説が正しいか否かを判断する。一方で、「帰無仮説を棄却できない」という結果を得た際には注意が必要である。具体的には、帰無仮説を棄却できない（統計的に有意でない）からと言って、帰無仮説が正しいと結論づけることはできない。そのため例えば、統計的に有意でない結果をもとに「〇〇は××に影響がない（もしくは、差がない）ことが明らかになった」という解釈を行うことは適切ではない。 第四にp値についての解釈である。Rや他のソフトウェアで統計的検定を実行すると “p-value”（p値）という値を得る。p値にも誤解が蔓延しており、「値が小さければ仮説が真であることを示す指標」や「小さいほど結果の重要性を示す指標」といった解釈を行うべきではない（Baker, 2016）。p値は、帰無仮説が正しいと仮定したときに，手元のデータから計算した検定統計量以上に極端な値を取る確率だと定義できる（豊田, 2017）。そしてp値が有意水準よりも小さい場合には、帰無仮説が間違っていたという判断を下すというのが、p値に基づく帰無仮説棄却の判断である。 本章では、上記の注意点について理解するための統計的な原理原則と、それに対応するRを用いた統計的分析手法を提示する。 "],["probability.html", "確率モデル、期待値と分散", " 確率モデル、期待値と分散 確率と離散確率変数 伝統的なデータ分析では、標本から得た情報に基づき母集団の性質について推測する。このとき、母集団は確率分布、標本はその確率分布に従う確率変数、データはその確率変数の実現値だと考える（倉田・星野、2024）。そのうえで確率とは、起こりうる事象の集合内において、各事象の起こりやすさの度合いを0以上1以下の実数で表したものである。より詳細な定義として、標本空間を \\(\\small \\Omega\\)、任意の事象 A に対して実数 P(A) が定まっていて、以下の三つを満たすとき、P(A)は事象 A の確率という： 確率は非負であり、以下を満たす： \\[0\\leq P(A)\\leq 1\\] 全事象を \\(\\Omega\\)、空事象を \\(\\emptyset\\) とするとこれらの確率は以下の様に示される： \\[P(\\Omega)=1,~~ P(\\emptyset)=0\\] 事象 \\(A_1,A_2,...\\) が互いに排反ならば、これらのうち少なくとも１つが起こる事象 \\(A_1\\cup A_2\\cup ...\\) の確率は以下となる： \\[ P(A_1\\cup A_2\\cup ...)=P(A_1)+P(A_2)+... \\] 確率変数とは、ある標本空間上で定義される取りうる各事象に対してそれぞれ一定の確率と対応関係のあるような変数である。例えば、細工のないサイコロを投げるとき、出た目の値を \\(x\\) とすると、\\(x\\) は1から6までの整数を取りうる変数だと言い換えることができる。この場合標本空間は、取りうる出目に対応した6個の標本点からなる。またこれらの標本点には、それぞれ対応する確率が以下のように付与されている。 Table 1: サイコロの確率分布 x 1 2 3 4 5 6 確率 1/6 1/6 1/6 1/6 1/6 1/6 このように、確率変数の取る値に対応して確率が付与されるルール（ \\(x\\) の関数としての確率 \\(P(x)\\) ）を確率分布や確率分布関数という。 確率変数は主に、離散確率変数と連続確率変数に分けることができる。離散確率変数は、サイコロのように、取りうる値が離散的な確率変数である。一方で、連続確率変数は、ある範囲の中で連続的にどんな値も取りうる確率変数である。離散確率変数では、サイコロの表で示されているように、取りうる特定の値に対応する確率を確率分布に基づき計算できる。 連続確率変数 一方で連続確率変数の場合、取りうる値の数が無限に存在する。例えば、-1 から 1 までの区間を取りうる値の範囲とする連続確率変数があったとする。この変数は例えば、0.90という値を取りうるが、同様に、0.91 や 0.900001 といった値も取りうる。このように、連続確率変数が取りうる値の数は無限に存在するため、取りうる特定の値に対応する確率は0になる。もし取りうる各値に確率が付与されていると、確率の合計が無限大になってしまうという問題に直面する。そのため、連続確率変数の場合、取りうる区間に対して確率が付与される。これを踏まえて連続確率変数を捉え直すと、「連続確率変数はその取りうる任意の区間に対して一定の確率が対応するような変数」だといえる。 また、連続確率変数における取りうる区間の起こりやすさには「確率密度」が対応することで計算可能になる。言い換えると、確率変数 \\(x\\) の値に確率密度がどのように対応するのかという関係は、\\(f(x)\\) という確率密度関数（probability density function: PDF）として示される。PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) が区間 [a, b] を取る確率 \\(P(a\\leq x \\leq b)\\) は、以下の積分計算で求められる。 \\[ P(a\\leq x \\leq b)=\\int^b_a f(x) dx \\] 以下の図はPDFの例であり、図内の曲線はPDFを、灰色に塗られている面積はある区間の確率を示している。なお、上記の式で示されている関係から、PDFを特定（仮定）することで、ある確率に対応する区間の両端 a, b を求めることも可能である。以降の節で紹介する統計的分析では、この関係を用いて分析することもあるが、詳しくは後述する。 連続確率変数例 連続確率変数を用いた具体的な確率計算例を紹介するために、ここでは一様分布（uniform distribution）を用いる。区間 [a, b] を持つ一様分布に従う確率変数 \\(x\\) のPDFは以下のように示される。 \\[ f(x) = \\begin{cases} \\frac{1}{b-a} &amp; a\\leq x \\leq b\\\\ 0 &amp; otherwise \\end{cases} \\] 具体的な計算を実行するために、ここでは、-1から3までの区間を持つ一様分布を考える。この一様分布のPDFは、\\(\\small f(x)=\\frac{1}{4}~for~-1\\leq x\\leq 3\\)（その他の区間の確率は0）となる。このとき、\\(x\\) が区間 [0, 2] を取る確率は、以下のように求められる。 \\[ P(0\\leq x \\leq 2) = \\int^2_0 \\frac{1}{4} dx=\\left[\\frac{x}{4}\\right]^2_0=\\frac{1}{2}-0=\\frac{1}{2} \\] 期待値 期待値とは確率の考え方を含む理論的な平均値（\\(\\mu\\)）といえる。確率分布 \\(P(x)\\) を持つ離散確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\sum_x x \\cdot P(x)=\\mu \\] 一方、PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\int_{-\\infty}^\\infty x \\cdot f(x) dx=\\mu \\] この定義に基づき、先程の区間 [-1, 3] を持つ一様分布の期待値を以下のように求める。 \\[ E(x)= \\int^3_{-1} \\frac{x}{4} dx=\\left[\\frac{x^2}{8}\\right]_{-1}^3=\\frac{9-1}{8}=1 \\] 期待値 \\(E(x)\\) は一般的に、\\(a\\) を定数、\\(g(x)\\) と \\(h(x)\\) を \\(x\\) の関数とするとき、以下が成り立つ： \\(E(a) = a\\) \\(E[a\\cdot g(x)]=a\\cdot E[g(x)]\\) \\(E[g(x)+h(x)]=E[g(x)]+E[h(x)]\\) これらの性質により、確率変数 \\(x\\) の分散（\\(\\sigma^2\\)）は以下のように求まる。つまり、分散は二乗の期待値から期待値の二乗を引くことで計算できる。 \\[ \\sigma^2=E\\left[\\bigl(x-E(x)\\bigr)^2\\right]=E\\left[(x-\\mu)^2\\right]=E(x^2)-E(x)^2 \\] "],["inference.html", "統計的推測", " 統計的推測 前節で述べた通り、我々は研究の対象となる集団全体ではなく、その一部から情報を取得し分析を行う。このとき、その集団全体を母集団、母集団から抽出した一部を標本と呼ぶ。統計的な分析においては、確率分布を用いて母集団をモデル化し、標本をその確率分布に従う確率変数とみなすことで母集団と標本の関係を捉える。そのため、データ分析は標本を対象とするものの、分析者の関心は、母集団の特徴である母数（parameter）についてであることが多い。母集団の平均（\\(\\mu\\)）や分散（\\(\\sigma^2\\)）は母数の代表例である。しかし、母数は通常未知であり直接知ることはできないため、標本の情報を用いて母集団の特徴について推測する。このプロセスを統計的推測と呼ぶ。統計的推測を行うためには、原則として母集団からの無作為標本抽出（random sampling）が必要になる。統計的推測では、互いに独立に同一の分布に従う（Identically Independently Distributed: IID）ような標本が好ましく、無作為標本は、IIDを満たすことが知られている。 統計的推測においては、「推定」「統計量」「推定量」「推定値」などの似たような言葉が利用されるが、これらはそれぞれ異なる意味を持つ。推定とは、標本の情報に基づき母数について把握しようとする作業そのものを示す。一方で、標本として得られるデータに基づき計算できる値（計算式で示されることも多い）を一般的に統計量というのだが、その中でも特に推定に用いる統計量を推定量という。そして推定値は、推定量についてデータから求めた実際の計算値を表す。 また、推定には「点推定」と「区間推定」がある。点推定とは、未知の母数について１つの数値に基づいて推定する方法である。例えば、標本平均は母平均（\\(\\mu\\)）を点推定するための推定量である。一方で区間推定は、未知の母数を一定の確率で含む区間を推定する方法である。これは、点推定では捉えきれない統計的誤差を考慮して区間を推定する方法であり、母平均の信頼区間の測定は区間推定の代表例である。 "],["pestimate.html", "点推定", " 点推定 点推定は特定の推定量によって母数を捉えようとするが、どのような推定量を用いるべきなのだろうか。本節では、不偏性（unbiasedness）、一致性（consistency）、効率性（efficiency）という統計的に重要な推定量の性質について説明する。なお、以下の説明では、未知パラメータ \\(\\small \\theta\\)（シータ）に対する推定量 \\(\\small \\hat{\\theta}\\)（シータハット）を考える。不偏性とは、推定量の「期待値」が未知パラメータの真の値に等しいという性質であり、以下のように示すことができる。 \\[ E(\\hat{\\theta})=\\theta \\] つまり、実際の推定量の実現値がどうかは置いておいて、期待値の下では推定量が未知パラメータを示していることを表すものであり、サンプルサイズに関係のない推定量の性質である。そして、不偏性を満たす推定量のことを不偏推定量（unbiased estimator）という。なお、上記の定義から、統計的なバイアス（B）は、以下のように定義できる。 \\[ B=E(\\hat{\\theta})-\\theta \\] 第二に一致性とは、サンプルサイズが十分に大きいとき、推定量が未知パラメータの真の値と等しくなる確率が1に近づくという性質である。この性質について詳しく論じるには、漸近理論を学ぶ必要があるため、本書では詳細を省略するが、サンプルサイズを大きくすると未知パラメータの真の値に近づくような推定量を示した性質だと解釈できる。なお、任意の\\(\\small \\epsilon &gt;0\\)（\\(\\small \\epsilon\\): イプシロン）に対して以下のような性質を持つ推定量を一致推定量という。 \\[ \\lim_{n\\rightarrow \\infty} P\\left(|\\hat{\\theta}-\\theta|\\leq \\epsilon \\right)=1 \\] 第三に効率性は、推定量の分散の小ささを示している。分散の小さい推定量の方が、期待値から離れた値を取りにくく、好ましい推定量と考えられる。複数の不偏推定量や一致推定量がある場合、効率性をもとに好ましい推定量を考える。 なお、代表的な推定量である標本平均は母集団期待値の推定量として好ましい性質（不偏性と一致性）も持っている。以下では、期待値 \\(\\mu\\)、分散 \\(\\sigma^2\\) の確率分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\)を考える（つまり、\\(\\small E(X)=\\mu\\), \\(\\small Var(X)=\\sigma^2\\)）。このとき、標本平均（\\(\\small \\bar{X}\\)）の不偏性は以下のように示すことができる。 \\[ E(\\bar{X})= \\left[\\frac{1}{n}(X_1+X_2+...+X_n)\\right] = \\frac{1}{n}~\\left[E(X_1)+E(X_2)+...+E(X_n)\\right] = \\frac{1}{n}\\cdot n\\mu=\\mu. \\] また標本平均の分散については、以下となることが知られている（計算は省略）。 \\[ Var(\\bar{X})=\\frac{\\sigma^2}{n} \\] 上記と同様の無作為標本による標本平均の一致性については、任意の \\(\\small \\epsilon&gt;0\\) に対していかが成り立つことが知られている。 \\[ \\lim_{n\\rightarrow \\infty}P(|\\bar{X}-\\mu|\\leq \\epsilon)=1 \\] 言い換えると、サンプルサイズが増えることで標本平均 \\(\\small\\bar{X}\\) は母集団の真の平均 \\(\\small \\mu\\) と等しくなる確率が1に近づく。なお、標本平均がもつこの特性は「大数の法則（Law of Large Number）」として知られている。 また、標本平均はその分布の収束に関しても重要な特性を持っている。ここで、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) を持つ確率分布に従う母集団からのn個の無作為標本 \\(\\small X_1,.., X_n\\) を考える。サンプルサイズが十分に大きい場合、 \\(\\small \\bar{X}\\sim N(\\mu,\\sigma^2/n)\\)（\\(\\small \\bar{X}\\) が平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布に従う）となることが知られている。この性質を「中心極限定理（Central Limit Theorem）」というが、詳細な証明や定義は省略するので、文末の参考文献を参照してほしい（例えば、岩田, 1996; 倉田・星野, 2024; 宮川, 2002）。また、この定理を以下のような\\(\\small \\bar{X}\\) を標準化した確率変数に応用することも可能である。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] このとき、中心極限定理よりサンプルサイズが十分に大きい場合には Z の分布関数は標準正規分布（N(0,1)）の分布関数に収束する。詳細については割愛するが、サンプルサイズが十分に大きい場合、「標本平均」や「標本平均を標準化した確率変数」の確率分布が正規分布や標準正規分布に近似できるという定理は、統計的な推定や検定において重要なものである。 また、母集団分散の推定量としては、不偏標本分散が使われる事が多い。上記と同じ無作為標本に対し、標本分散 \\(S^2\\) と、不偏標本分散 \\(s^2\\) は以下のように定義される。 \\[ S^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X}) \\] \\[ s^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X}) \\] そして、それぞれの推定量の期待値は以下のようになることが知られている（計算省略）。そのため、母集団分散の推定量として、不偏標本分散（\\(s^2\\)）が用いられる。 \\[ E(S^2)=\\frac{n-1}{n}\\sigma^2 \\] \\[ E(s^2)=\\sigma^2 \\] "],["estimates.html", "推定量もまた確率変数", " 推定量もまた確率変数 本節では、推定値と母数との関係を標本平均（\\(\\small \\bar{X}\\)）を使って説明する。ある母集団からランダムサンプルを収集し、標本平均を計算することを考える。ここで計算された数値は真の母平均を捉えた唯一の値なのだろうか。 結論としては、点推定の推定値は母数そのものではなく、ひとつのある実現値でしかないことに注意が必要である。この理由は、「確率変数から計算される推定量もまた確率変数である」という事実から理解することができる。 例えば、我々が神戸大学経営学部生の一ヶ月あたりの平均収入（仕送りは除く）に関心があるとする。このとき（実現可能性は置いておいて）、経営学部全体を母集団とする無作為標本を100件収集し、標本平均を計算した結果 \\(\\small \\bar{X}=\\) 0 だったとする。もしこのような極端な結果を得た場合、多くの人が「標本平均の実現値は必ずしも真の母平均そのものではない」という説明に納得がいくだろう。同様の調査（100件のランダムサンプリング）をもう一度行い平均収入を計算し直すと、おそらく0とは異なる推定値を得る可能性が高い。仮に、\\(\\small\\bar{X}=\\) 50,000 だった場合、その結果をどのように解釈するだろうか。仕送りを除く大学生の月当たり収入の平均が5万円だという結果はなんとも尤もらしいと感じるかもしれない。しかしながら、たとえ尤もらしい結果を得たとしても、それはひとつの分析結果であり、真の母平均を示す唯一の値ではない。その点については、平均収入が0という結果も5万円という結果も同様に注意が必要である。 確率変数から計算される推定量もまた確率変数であるという点をさらに直感的に経験するために、細工のない6面サイコロを（バーチャルに）振ってもらう。なお、コード内では、出力結果を少し見やすくするために、knitr というパッケージを利用している。同じように作業を行いたい場合には、以下のようにインストールしてほしい。 install.packages(&quot;knitr&quot;) 標本平均についての議論を行う前に、理論的な期待値を求める。6面サイコロの出目の期待値 \\(\\mu\\) は以下の通りである。 \\[ \\mu = 1\\cdot \\frac{1}{6}+2\\cdot \\frac{1}{6}+...+6\\cdot \\frac{1}{6}=3.5 \\] ここで、以下のようなコマンドを用いてR内でサイコロを振ってみてほしい（実際にサイコロを振ってもらっても構わないが、面倒くさい）。sample() 関数は指定した標本空間からランダムに size の数だけサンプルを抽出するための関数である。replace = TRUE は、復元を認めるという指示になっている。なお、ここでは、乱数の再現性確保のために、set.seed()を用いている。set.seed()のカッコの中に数値を入力することで、その数値を使う場合には常に同じ乱数を発生させることが可能になり、乱数の固定と再現性の確保ができる。そのため、読者においては、set.seed()内に異なる値を入れてもらって構わない。 set.seed(442)# 乱数の再現性確保のための指示。関数内の数字に特に意味はないため各自別の値を使っても良い。 die &lt;- 1:6 d &lt;- sample(die,size=1,replace = TRUE) d ## [1] 6 本書内では以上で示されている通り、6という出目を得た。上記のコマンドを実施した各自がそれぞれ異なる値を得ているだろう。ここで得た6という数字は、サイコロの出目という確率変数の実現値（\\(n=1\\)）である。そのため、本データの標本平均も6であり、真の期待値とは異なる。ただし、読者によっては1件の標本による標本平均という表現を直感的に理解しにくいかもしれない。そのため、以下のように サイコロを10回振る試行を3回実施し、各サンプリング結果に基づき標本平均を以下のように計算する。 set.seed(352) d1 &lt;- sample(die,size=10,replace = TRUE) d2 &lt;- sample(die,size=10,replace = TRUE) d3 &lt;- sample(die,size=10,replace = TRUE) d_mean &lt;- matrix(c(mean(d1),mean(d2),mean(d3)),nrow = 1) colnames(d_mean) &lt;- c(&quot;d1の平均&quot;, &quot;d2の平均&quot;, &quot;d3の平均&quot;) knitr::kable(d_mean, caption = &quot;サイコロの標本平均比較&quot;, align = &quot;ccc&quot;) Table 2: サイコロの標本平均比較 d1の平均 d2の平均 d3の平均 3.2 3 2.4 上記の通り、d1, d2, d3 いずれの標本平均も互いに異なるものであり、また3.5とも異なる。このことからも、確率変数（サイコロの出目）を用いて計算された推定値（標本平均）もまた確率変数であり、推定値と未知パラメータとの間にはズレ（誤差）が生じうることがわかる。なお、なかには3.5と等しい標本平均を偶然得た読者もいると考えられるが、それもあくまで一つの実現値である。 では、標本平均の推定値がサンプルサイズによってどれだけ真の期待値に近づくのかについて、サイコロの試行回数を10回、100回、1,000回と増やして確認する。以下の結果を見ると、サンプルサイズ（試行回数）が増えるごとに真の期待値に近づいていることが伺える。ただし、これらの結果もあくまで確率的な試行結果の実現値である。そのため、読者によっては異なる傾向を示すような結果を得る可能性があることに注意が必要である。 set.seed(541) d10 &lt;- sample(die,size=10,replace = TRUE) d100 &lt;- sample(die,size=100,replace = TRUE) d1000 &lt;- sample(die,size=1000,replace = TRUE) d_lln &lt;- matrix(c(mean(d10),mean(d100),mean(d1000)),nrow = 1) colnames(d_lln) &lt;- c(&quot;10回試行の平均&quot;, &quot;100回試行の平均&quot;, &quot;1,000回試行の平均&quot;) knitr::kable(d_lln, caption = &quot;サイコロの標本平均比較２&quot;, align = &quot;ccc&quot;) Table 3: サイコロの標本平均比較２ 10回試行の平均 100回試行の平均 1,000回試行の平均 3 3.36 3.516 "],["distribution.html", "補足（いくつかの確率分布の関係性）", " 補足（いくつかの確率分布の関係性） 統計的な分析の際によく用いられる確率分布として、正規分布、カイ二乗分布、t分布、F分布間の関係性について簡単に紹介する。なお、各分布の確率密度関数などは記載しないため、関心のある読者は参考文献を参照してほしい。 正規分布 正規分布は、様々な分布の基準として用いられる重要な分布である。期待値を中心に左右対称であり「ベルカーブ」と言われる形状を持つ。また、平均0、分散1の正規分布は特に標準正規分布と言われ、正規母集団からの無作為標本の標本平均等の分布を特定する際などに用いられる。 カイ二乗分布 標準正規分布からの無作為標本の二乗和はカイ二乗分布に従う。カイ二乗分布は、正規母集団からの無作為標本の不偏標本分散の分布を特定する際などに用いられる。 t分布 標準正規分布とカイ二乗分布の比はt分布に従う。t分布は、正規母集団からの無作為標本による標本平均と不偏標本分散の比の分布を特定する際などに用いられる。 F分布 カイ二乗分布の比はF分布に従う。F分布は、異なる正規母集団からの無作為標本の不偏標本分散の比の分布を特定する際などに用いられる。 "],["intinf.html", "区間推定", " 区間推定 点推定の節で示した通り、推定値と未知パラメータの間には、ずれ（誤差）がある。標本平均の様に好ましい性質（不偏性や一致性）を持つ推定量であっても、計算の結果示された一つの推定値がどの程度信頼できるものなのかはわからない。そこで区間推定という方法を用いて、統計的な誤差を加味した母数への検討を試みる。代表的な区間推定の方法として、「信頼水準zz%で、xx以上、yy以下という区間は真の母数を含む」という区間[xx, yy]を調べるものがある。このように求められた区間は信頼区間（confidence interval）と呼ばれ、多くの統計分析において用いられている。 では、ここで示された区間がどのように計算され、どのようなことを意味するのだろうか。以下では、もう少し一般的な形で信頼区間の導出や解釈を説明する。 標準正規分布に基づく信頼区間の求め方* はじめに、標準正規分布に基づくある区間の確率の求め方を説明する。 \\(Z_1,Z_2,...,Z_n\\) は、N(0, 1) （標準正規分布）に従う母集団からの無作為標本とする。このとき、標準正規分布がある区間 [\\(-\\infty,~z_\\alpha\\)] をとる確率は、以下の式および図のように示すことができる6。なお、\\(\\small z_\\alpha\\) は、確率\\(\\small \\alpha\\) に対応する標準正規分布上の上側確率 \\(\\small \\alpha\\) 点とする。このとき、この分布における \\(\\small z_\\alpha\\) 以上の範囲を取る確率は\\(\\small \\alpha\\) である。そのため確率の定義より、 \\(\\small z_\\alpha\\) 以下以上の範囲を取る確率は 1 \\(\\small -\\alpha\\)である。 \\[ P(Z\\leq z_\\alpha) = \\int^{z_\\alpha}_{-\\infty}~f(z)~dz =\\int^{z_\\alpha}_{-\\infty}~\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right)~ dz = 1-\\alpha \\] 標準正規分布と確率計算 同様に、以下のような関係も捉えることができる。この場合、斜線部で示されている範囲の確率は両側合わせて \\(\\small \\alpha\\) であり、その内側の確率は 1 \\(\\small - \\alpha\\) である。 \\[ P(-z_{\\alpha/2} \\leq Z\\leq z_{\\alpha/2}) = \\int^{z_{\\alpha/2}}_{-z_{\\alpha/2}}~f(z)~dz = 1-\\alpha \\] 標準正規分布と両側確率 上述の関係を、区間推定に応用するために、あるデータの標本平均に関する議論を捉える。標準正規分布に従う確率変数は、正規分布に従う確率変数を標準化することで得ることができる。ここで、\\(\\small X_1,...,X_n\\) を、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) の正規分布に従う母集団からの無作為標本とする。これまで学んだ標準化および標本平均の特性から、以下の通り、標本平均を標準化したものは標準正規分布に従うことがわかる。 \\[ \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] 先述の標準正規分布における確率計算の関係を応用し、以下を得る。 \\[ P\\left(-z_{\\alpha/2}\\leq \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\leq z_{\\alpha/2}\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\right)=1-\\alpha \\] したがって、区間 [\\(\\small \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n}\\)] は、確率 \\(\\small 1-\\alpha\\) で未知の母平均 \\(\\small \\mu\\) を含むと解釈できる。また、上記の関係から任意の確率 \\(\\alpha\\) を指定することで、区間の上限と下限（\\(\\pm z_{\\alpha/2}\\)）の具体的な値を（統計学のテキスト巻末などに記載されている）標準正規分布表などから求めることができる。 そして、このような区間を「信頼区間」といい、信頼区間の計算において仮定された確率 \\(\\small 1-\\alpha\\) を「信頼水準」もしくは「信頼係数」という。信頼係数は、信頼区間の計算のために研究者によって事前に選択される。慣習としては、90%, 95%や99% (\\(\\small \\alpha =\\) 0.10, 0.05, 0.01)を用いる事が多い。なお、信頼係数を大きくすると、信頼区間も広くなる。 t分布を使った区間推定* 上記の区間推定は母分散 \\(\\sigma^2\\) が既知である場合に計算可能であるが、多くの場合母分散は未知である。そのような場合には、自由度 n-1 のt分布を用いて、両端の確率 \\(\\small \\alpha\\) 点を \\(\\small t_{\\alpha/2}(n-1)\\) とする信頼区間を求める。\\(N(\\mu,\\sigma^2)\\) に従う母集団からの無作為標本を考えるが、今回は母分散が未知である場合を仮定する。このような場合は、母分散 \\(\\small \\sigma^2\\) のかわりに母分散の不偏推定量である不偏標本分散 \\(\\small s^2\\) を用いた以下の統計量 t をもとに信頼区間を計算する。このとき、統計量 t は自由度 n-1 の t 分布に従うことが知られている（t分布に関する詳細および証明は省略）。 \\[ t=\\frac{\\bar{X}-\\mu}{\\sqrt{s^2/n}}\\sim t(n-1) \\] ここで、先述の標準正規分布に基づくある区間の確率計算と同様の計算を、自由度 \\(\\small n-1\\) のt分布に基づき実行すると、以下のような確率と区間の関係に書き換えることができる。 \\[ P\\left(-t_{\\alpha/2}(n-1)\\leq \\frac{\\bar{X}-\\mu}{s^2/n}\\leq t_{\\alpha/2}(n-1)\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\right)=1-\\alpha \\] 信頼区間を求めるための手順は標準正規分布の場合もt分布の場合も同様だが、標準正規分布のかわりに t 分布を用いた場合、特定の確率に対応する閾値が変わることが知られている（\\(\\small z_{\\alpha/2}\\neq t_{\\alpha/2}(n-1)\\)）。 t分布は、左右対称であり標準正規分布よりもテールが厚いという特徴を持つが、自由度が大きくなると正規分布に近づくことが知られている。標準正規分布と、自由度の異なる t 分布との関係は以下のように図示化できる。自由度（df）3の t 分布よりも自由度20の t 分布のほうが標準正規分布に近い形状であることが伺える。 標準正規分布と t 分布によってある確率に対応する閾値が異なるということは、ある信頼水準に対応する信頼区間も仮定する分布によって異なるということである。 Rによる区間推定 Rで信頼区間を求めること自体は難しくない。最も手間のかからない方法としては、t.test() の分析結果を用いて、conf.int()によって信頼区間が計算できる。信頼区間の計算を実行するために、以下の例を考える。ある製品（電球）の寿命は平均1800（時間）である。企業は性能を改良するために新型の電球が開発したが、新型化に伴い製品寿命も変化したのかについては不明である。ただし、この製品の寿命は新型も旧型のものも正規分布に従い、その標準偏差は \\(\\sigma=\\) 180（時間）であると仮定する。 工場で生産された新型製品を16個無作為に選びその寿命を計測した所、以下の結果を得た。 1939.6 1680.3 1982.1 2215.6 2092.5 1928.9 2003.8 1955.5 1800.1 1659.5 2066.2 2107.2 2085.5 1878.6 2007.6 1816.1 このデータは、平均が \\(\\small \\mu\\)、分散が \\(\\small 180^2\\) である正規分布（\\(\\small N(\\mu, \\sigma^2=180^2)\\) と表記する）からの無作為標本 \\(X_1,..., X_{16}\\)の実現値とみなすことができる。なお、このデータの標本平均は1,951（時間）、不偏標本標準偏差は 154.46 である。このデータに基づく、新型電球寿命の期待値に関する95% 信頼区間（95%の確率で真の母数を含む区間）はt.test() 関数を用いると以下の様に求まる（ただし後述するが、この方法はこの例に対しては適切ではない）。 bulb &lt;- c( 1939.6, 1680.3, 1982.1, 2215.6, 2092.5, 1928.9, 2003.8, 1955.5, 1800.1, 1659.5, 2066.2, 2107.2, 2085.5, 1878.6, 2007.6, 1816.1) bulb_ci &lt;- t.test(bulb) #t検定の実施と格納 bulb_ci$conf.int #信頼区間の出力（デフォルトで95%信頼水準） ## [1] 1868.890 2033.498 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 出力されている 1868.890 2033.498 が信頼区間、attr(,\"conf.level\") 0.95 が今回計算に用いられた信頼水準（confidence level）（もしくは信頼係数（confidence coefficient）ともいう）である。 分析の結果、95%の確率で真の新製品寿命期待値がおよそ 1868.9 から 2033.5 の間に含まれることがわかった。したがって、どうやら新製品寿命は平均的に旧型製品（1,800）よりも長そうである。 しかしながら、新型電球の例では分散が既知の正規分布からの標本を仮定しているため、t分布を仮定するよりも標準正規分布により信頼区間を求めるほうが好ましい。 先述の通り、標準正規分布に基づく信頼区間は以下のように示すことができる。 \\[ \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n} \\] このとき、仮定より \\(\\small \\bar{X}=1951.194\\), \\(\\sigma=180\\) であることがわかっている。また、慣習より95%信頼水準を仮定すると、 \\(\\small \\alpha = 0.05\\)となる。そのため、区間推定の計算で必要な要素のうち現時点で不明なのは、 \\(z_{\\alpha/2}=z_{0.025}\\)の値である。この値は、任意の確率に対応する区間の閾値を表している。今回の場合、分布が左右対称の分布であり正負どちらか一方の値さえ分かればよいため、閾値（\\(z_{0.025}\\)）以上の区間を取る確率が2.5%になるような閾値に着目する。このような閾値は、Rによって以下のように求める（なお先述の通り、統計学教科書に掲載されている分布表を使っても同様の数値を求めることができる）。qnorm は、標準正規分布に従う確率変数について、引数で定義した確率に対応する境界値（分位点: quantile）を求める 関数である。 qnorm(0.025, lower.tail=FALSE) #標準正規分布における上側2.5%点の分位点を求める。 ## [1] 1.959964 これにより、計算に必要な情報が揃ったため、以下の要領で信頼区間を出力できる。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 #信頼区間の計算 upper &lt;- xbar+z*(sigma/sqrt(n)) lower &lt;- xbar-z*(sigma/sqrt(n)) #結果のまとめと出力 ci.bulb &lt;- matrix(c(lower,upper),nrow=1) colnames(ci.bulb) &lt;- c(&quot;ci.lower&quot;, &quot;ci.upper&quot;) knitr::kable(ci.bulb, caption = &quot;Bulb data CI（95%）&quot;, align = &quot;cc&quot;) (#tab:ci_bulb)Bulb data CI（95%） ci.lower ci.upper 1862.995 2039.392 分析の結果、新型製品の平均寿命に関する95%信頼区間は、標準正規分布と t 分布どちらの分布を仮定しても旧型の平均1800（時間）を含まず、それよりも大きい値を取るものであった。したがって、新型製品は製品寿命の面においても95%の確率で旧型製品よりも優れていると考えられる。 では、この95%信頼区間は、そもそもどのように解釈すべきだろうか。結論から述べると、95%信頼区間の直感的解釈については以下のように説明できる： 「母集団からサンプルを取り平均値の95%信頼区間を構築する」という手順を100回繰り返すと考える。95%という信頼水準（確率）が示していることは、計算された区間が100回に95回は母数を含むということである。言い換えると、今回得た標本平均に基づき計算された信頼区間がはずれ（真の母数を含まない区間）である可能性が5%存在するということである。 95%信頼区間の解釈として、「分析対象としている母数の値がこの区間の値をとる確率が95%である」という旨の説明を行う人がいるが、これは\\(\\color{red}{\\text{誤り}}\\)である。母数は未知だが、何かしらの定まった値なので確率的な議論を母数に用いるのは適切ではない。確率的に変動するのはあくまで区間の両端である点を理解しなければならない。なぜならば、\\(\\small \\bar{X}\\) が確率変数であるため、そこから計算される区間の両端もまた確率変数となるためである（岩田，1996）。ここで示されている信頼水準は、計算された区間が真の母数を含んでいる確率である。つまり、信頼水準は、サンプルを収集し、信頼区間を求めるという「手順そのものに対する信頼度」を表す指標であると解釈できる。 ただし、標準正規分布の確率密度関数 \\(\\small f(z)\\) は、\\(\\small 1/\\sqrt{2\\pi}\\exp\\left(-z^2/2\\right)\\) だと知られている。↩︎ "],["testtheory.html", "統計的仮説検定", " 統計的仮説検定 検定の手順と仮説 データを用いた研究では、統計的分析によって提示した仮説が支持できるか否かを判断したい場合もある。その時に用いられる方法が統計的仮説検定である。本節ではまず検定の手順について説明したあと、分析結果の意味や解釈について説明する。統計的仮説検定は、基本的に以下の手順によって実施される。 仮説（帰無仮説・対立仮説）を設ける。 仮説を検定するための統計量を選ぶ。 有意確率を定め、統計量の値について有意確率に基づく臨界値を設定する。 帰無仮説が正しいと仮定した上で統計量を計算し、その値が棄却域と採択域のどちらの領域に入るかを分析する。 統計的仮説検定でも「仮説」という言葉が重要な役割を占める。ただし、ここで重要になるのは帰無仮説と対立仮説である。用語による誤解を避け、理解を容易にするために、??章で議論した研究上着目する変数間の関係に関する予測（仮説）を「作業仮説」と呼ぶことにする。作業仮説は、リサーチクエスチョンに答えるための論理的予測である。例えば、「女性に比べ男性の方が新製品購買意図が高い」のようなものが作業仮説である。このような仮説を検証する場合、男女（グループ）間で購買意図の平均値を比較することが現実的な分析方法として考えられる。 帰無仮説と対立仮説は、統計的仮説検定の基準になる「母集団の統計的特徴に関する仮説」であり、検定という手続き上ではこれらの仮説に着目する。特に、帰無仮説は、統計的仮説検定の考察、分析の基準となる仮説であり、この仮説を棄却（否定）できるか否かを調べることが基本的な統計的仮説検定の枠組みだと言える。帰無仮説は棄却しうる仮説であり、\\(\\small H_0\\) という記号で表される事が多い。また、多くの場合において「差がない」「効果がない（0である）」や「特定の値と等しい」といった仮説が設計される。一方で対立仮説は、帰無仮説とは排反な仮説で、帰無仮説が棄却された際に採用される推測であり、\\(\\small H_1\\)や\\(\\small H_a\\)という記号で表される。データ分析を用いた研究においては対立仮説と作業仮説は論理的に整合的ないしは等しいことが好ましい。つまり、作業仮説という研究上重要な論理的推測を統計的に検証するためには、その作業仮説とは排反な帰無仮説を設計し統計的仮説検定を実施する必要がある。それによってもしその帰無仮説が棄却されたならば、対立仮説ひいては作業仮説がデータ分析によって支持されたと解釈することが可能になる。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_m\\)、女性における購買意図の期待値を \\(\\small \\mu_f\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\(H_0:~\\mu_m=\\mu_f\\) \\(H_1:~\\mu_m\\neq\\mu_f\\) \\(\\small H_0\\) は、男性における購買意図の期待値と女性における期待値が等しいというものであり、 \\(\\small H_1\\)はそれらが等しくないということを示している。そのため上記の二つの仮説は、どちらも未知パラメータについての関係を捉えており、\\(\\small H_0\\) と \\(\\small H_1\\) は互いに排反であることがわかる。その上で、もし帰無仮説が棄却され、男性の平均値のほうが女性よりも高い場合には、作業仮説が支持されたと解釈することができる。つまり統計的な検定においては、作業仮説として提示している推測を直接検証するのではなく、作業仮説と排反な帰無仮説を設計し、それが棄却されるならば暫定的に作業仮説の主張を支持しようという立場で検証を行う。なお対立仮説として\\(\\small H_1:~\\mu_m&gt;\\mu_f\\) を設定することも可能である。このような仮説に基づく検定方法は片側検定と呼ばれ、その詳細については後述する。 ここで改めて、より一般的な形でマーケティングリサーチで用いられる仮説と検定で用いられる仮説との関係を整理する。 分析上の基準である帰無仮説は何かをきちんと理解し定義する。 それが棄却された際にはどのような結論（対立仮説）が採用されるのかを理解する。 そしてその結論が自身の立てた作業仮説と帰無仮説・対立仮説の関係が整合的かを考える。 言い換えると、自身の立てた作業仮説を帰無仮説・対立仮説の対比という分析手続きで証明できるような調査・分析法を採用する必要がある。ただし、マーケティング領域におけるレポートや論文では、帰無仮説・対立仮説を記載せず、作業仮説のみを記載することが多い。 母平均の検定（両側検定）に関する理屈 ここで改めて、標準正規分布を用いた母平均に着目し、統計的仮説検定に関する一般的な説明を提示する。\\(\\small X_1,..,X_n\\) を正規母集団 \\(N(\\mu, \\sigma^2)\\) からのサンプルサイズ n の無作為標本とする。このとき、帰無仮説のもとでのパラメータの値を\\(\\mu_0\\)として、以下の帰無仮説と対立仮説を設計する。 \\[ H_0:~\\mu=\\mu_0,~~~H_1:~\\mu\\neq\\mu_0 \\] このとき、??節の信頼区間の説明でも述べた通り、正規分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\) の標本平均は以下の分布に従うことがわかっている7。 \\[ \\bar{X}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right) \\] また、これまでの議論の通り、\\(\\small \\bar{X}\\)を標準化した統計量Zは以下の分布に従うことが知られている。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] 統計的仮説検定においては、この標準化された統計量を検定統計量（検定に用いる統計量）として用いて計算を行うのだが、我々の関心の中心でもある \\(\\small \\mu\\) は未知であり、通常この統計量を計算することはできない。すなわち、未知であるパラメーターを何かしらの値で代替しなければ、上記の検定統計量は計算できない。そこで、統計的な仮説検定では、「帰無仮説が正しいと一旦仮定」した上で統計量を計算するというプロセスを経る。言い換えると、未知のパラメーターについて帰無仮説で示されている値を代入することで、検定統計量を計算可能にする。したがって、検定統計量 Z を以下のように定義する。 \\[ Z=\\frac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n}} \\] そして、もし「帰無仮説が正しければ」Zは標準正規分布に従うはずであり、言い換えると Z の計算結果は0に近い値を取る可能性が高いはずである。そこで、この Z を計算し、\\(\\small |Z|\\) がある閾値 c よりも大きい（十分に0から離れている）場合には帰無仮説を棄却する。なお、ここで用いる閾値 c のことを一般的に臨界値と呼ぶ。つまり、検定統計量 Z の計算結果に対して、以下の方針で仮説検定を行う。 \\[ \\begin{cases} |Z|&gt;c &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq c &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 臨界値 c の求め方は区間推定と同様、分析に対応する確率分布（今回であれば標準正規分布）に基づくある区間の確率計算で求まる。研究者はまず、任意の確率 \\(\\small \\alpha\\) を決める。この確率は「有意水準（significance level）」と呼ばれ、この有意水準と標準正規分布に基づく確率計算によって臨界値（下図内では \\(\\small \\pm z_{\\alpha/2}\\)）を求める。その上で、統計量の計算結果が臨界値より外側（下図における斜線部）にある場合には帰無仮説を棄却する。そのため、斜線部のような領域を棄却域、確率 \\(\\small 1-\\alpha\\) に対応する範囲を採択域と一般に呼ぶ。 Figure 1: 臨界値と確率計算 今回の場合、有意水準 \\(\\small \\alpha\\) に基づく両側臨界値 \\(\\pm z_{\\alpha/2}\\) を設定し、以下の方式で検定する。 \\[ \\begin{cases} |Z|&gt;z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 片側検定の紹介 ここまでの議論では対立仮説を \\(H_1:~\\mu\\neq\\mu_0\\) とし、左右対称の分布の両端に棄却域を設定した。このような検定方法を一般的に両側検定と呼ぶ。しかし、現実的ないしは理論的な根拠をもとに、ある値よりも高い（もしくは低い）ことが事前に予測できる場合がある。その場合には、例えば \\(\\small \\mu&gt;\\mu_0\\) や \\(\\small \\mu&lt;\\mu_0\\)といった対立仮説を設定することも可能である。このような対立仮説を利用した検定方法を一般的に片側検定と呼ぶ。ここでは、仮に\\(\\small \\mu&gt;\\mu_0\\)という対立仮説を立てた場合を考えるが、\\(\\small \\mu&lt;\\mu_0\\)のような対立仮説を設計しても正負を入れ替えることで同様の議論ができる。なお、片側検定において帰無仮説が棄却された場合、直ちに帰無仮説の値よりも大きい値を取ると判断する。しかしながら、たとえ異なる対立仮説を提示しても、採用する検定統計量や帰無仮説に基づく分布の仮定などは同じである。 片側検定を利用した場合の特殊性はその棄却域に現れる。片側検定の場合の棄却域は以下の図のように片側のみとなる。なお、その場合分布の両端に棄却域を設ける必要がないため、正の方向に \\(\\small \\alpha\\) 分の棄却域を設定する。 片側検定（正の場合） 有意水準と検定における誤り ここまでの内容をまとめると、帰無仮説を仮定して検定統計量を計算する場合、帰無仮説が正しければ、棄却域内の値を取る確率は \\(\\small 100\\times \\alpha\\)%であると言える。そして、検定統計量の計算結果が棄却域に含まれる場合、帰無仮説を棄却するという判断を下す。そのため、統計的に帰無仮説を棄却したからと言って、その結果が必ず正しいとは言い切れない。統計的検定には、根本的に第一種の誤り（Type 1 error）と第二種の誤り（Type 2 error）という二種類の誤りの可能性が内包されている。 第一種の誤りとは、帰無仮説が真であるにもかかわらず、帰無仮説を棄却してしまう誤りである。一方で第二種の誤りは、帰無仮説が真ではないにもかかわらず、帰無仮説を採択してしまう誤りである。例えば、ある薬に期待される効果があるかどうかを検証する場合を考える。この時、帰無仮説は「投薬による効果がない」、対立仮説は「投薬による効果がある」と設計する。この場合における第一種の誤りとは、「本当は効き目のない薬を効くと判断してしまう誤り」であり、第二種の誤りとは、「本当は効き目のある薬を効かないと判断したしまう誤り」である。 Table 4: 検定の誤り H0が真 H0が偽 H0を棄却 Type 1 error ✓ H0を採択 ✓ Type 2 error どちらの誤りも見過ごすことのできないものではあるが、第一種の誤りによる損失と、第二種の誤りによる損失を比較し、一般的な統計的検定においては、第一種の誤りの確率を下げることに注視する。なお、研究によっては下記にある検定力という指標に着目し、第二種の誤りに対応した議論を提示することもあるが、本書では割愛する。仮説検定では特に、第一種の誤りの確率を有意水準 \\(\\small \\alpha\\) と設定し分析する。また有意水準は、先述の通り棄却域の特定に用いられる。つまり、統計的仮説検定とは、帰無仮説が正しいと仮定した上で有意水準 \\(\\small \\alpha\\) の分だけ第一種の誤りの確率を許容したうえで仮説が正しいか否かを確認する作業である。 上記の統計的検定に関わる誤りは、\\(\\small T_0\\) は統計量 T の観測値、Rは\\(\\small H_0\\) の棄却域、Aは\\(\\small H_0\\)の採択域とし、以下のように示される。 有意水準: \\(\\alpha\\) \\[ P(T_0\\in R|H_0~\\text{が真})=\\alpha \\] 第二種の誤りの確率: \\(\\beta\\) \\[ P(T_0\\in A|H_0~\\text{が偽})=\\beta \\] 検定力: \\(1 - \\beta\\) \\[ P(T_0\\in R|H_0~\\text{が偽})=1-\\beta \\] ここで、仮説検定に関わる有意水準を \\(\\small \\alpha=0.05\\) と設定する。帰無仮説が正しいという条件のもとで、帰無仮説を棄却する確率であるため、有意水準は以下のように示すことができる。 \\[ \\alpha=P(|Z|&gt;c|\\mu=\\mu_0) =0.05 \\] しかしながら、このままだと確率計算が複雑になるため、上式を以下のように書き換える。 \\[ 1-\\alpha=P(|Z|\\leq c|\\mu=\\mu_0) = \\int^c_{-c}f(Z)dZ~_{|\\mu=\\mu_0}=0.95 \\] なお、上式の関係を表した図が図 1になる。 検定結果に関する解釈 ?? の検定について、検定統計量 Z は帰無仮説が正しければ標準正規分布に従うはずである。したがって、臨界値 \\(\\pm c\\) は、\\(\\pm z_{0.025}\\) として分布表などより導出が可能である。Rにおいては前節と同様、qnorm() により、\\(z_{0.025}=1.96\\) だとわかる。すなわち、検定統計量の計算結果が 1.96（-1.96）を上回る（下回る）場合には、帰無仮説を棄却するが、そこには第一種の誤りを犯す確率が5%残されていると解釈できる。 一方で、統計的に有意でない（帰無仮説を棄却できない）結果を得たときには、その解釈について注意が必要である。具体的には、統計的に有意でないからと言って、帰無仮説が正しい（つまり \\(\\small \\mu =\\mu_0\\) である）と結論づけることはできない。ここまでの説明の通り、有意水準とは第一種の誤りを犯す確率であり、有意水準に基づく統計的検定では主にこの確率に対応した分析を行っている。そのため、第二種の誤りである、本当は \\(\\small \\mu \\neq \\mu_0\\) であるにも関わらず、\\(\\small \\mu =\\mu_0\\) と判断している可能性については未対応である。これらの点から「有意でない」ということを理由に、帰無仮説が正しいと結論づけることは適切ではない。 なお、ここまでの議論のように母集団の分散が既知の場合、検定統計量は標準正規分布に従うと仮定できる。しかし母集団の分散が未知の場合は、信頼区間での議論と同様、標準偏差の不偏推定量を用いて、自由度 n-1 の t 分布を仮定した分析を行う。そして、t分布に基づく母平均に関する検定を一般的に「t検定（t-test）」と呼ぶ。 より実践的な視座から統計的な検定において解釈に注意が必要な点として、p-value（p値）がある。 t.test() を用いた分析例でも紹介するが、R (他のソフトウェアでも)で統計的検定を実行すると “p-value”（p値）という値を得る。p値は、帰無仮説が正しいと仮定したときに，手元のデータから計算した検定統計量以上に極端な値をとる確率だと定義できる（豊田, 2017）。そしてp値が有意水準よりも小さい場合には、帰無仮説が間違っていたという判断を下すというのが、p値に基づく帰無仮説棄却の判断である。p値については、「値が小さければ仮説が真であることを示す指標」や「小さいほど結果の重要性を示す指標」といった解釈を行うこともあるが、このような解釈は不適切である（Baker, 2016）。 棄却域と有意水準の関係に基づき述べると、以下のように説明することができる（cf. 西山など, 2019）。有意水準（帰無仮説が正しいが帰無仮説を棄却する確率）を小さく取ると、棄却域は狭くなる。例えば、ある仮説検定において、5%有意水準では帰無仮説を棄却できるが、1%ではできない場合がある。計算された検定統計量の実現値に基づき、有意水準を変えながら検定を行っていくと、これ以上有意水準を小さくすると帰無仮説が棄却されなくなるという有意水準の限界を見つけることができる。この限界をp値と呼ぶ。そのため、p値によって示されている確率は有意水準と同様のものを捉えているのだが、その計算過程が異なるという点において注意が必要である。 電球データと検定 先程の新型電球の例を再度使い母平均の検定を実施する。新型電球について我々が関心を持っていたのは、新型電球の製品寿命が旧型の寿命（1800時間）より長いか否かである。そのため、新型電球の製品寿命の期待値を \\(\\small \\mu\\) とすると、帰無仮説と対立仮説は以下のように設計できる。 \\(H_0:~\\mu=1800\\) \\(H_1:~\\mu\\neq1800\\) 改めて以下の通り、新型電球に関する16個の無作為標本から得た製品寿命の平均値を計算すると、\\(\\small \\bar{X}=1951.194\\) であった。では、この1951は 1800 から十分に離れていると言えるのだろうか。もし、十分に離れていると判断されれば帰無仮説を棄却するが、この差が十分でなければ帰無仮説を棄却しない。 mean(bulb) ## [1] 1951.194 Rを用いて統計的検定を実行すること自体は難しくない。まず、t検定に基づく母平均の検定は t.test() で実施することが可能である。母平均が特定の値を取るか否かについての検定では、mu= という引数を使って帰無仮説に対応する値を指定する。今回の分析に関するコマンドおよびその結果は以下のとおりである。 t.test(bulb, alternative = &quot;two.sided&quot;, mu = 1800) ## ## One Sample t-test ## ## data: bulb ## t = 3.9155, df = 15, p-value = 0.001377 ## alternative hypothesis: true mean is not equal to 1800 ## 95 percent confidence interval: ## 1868.890 2033.498 ## sample estimates: ## mean of x ## 1951.194 分析結果の t= と df = はそれぞれt値（検定統計量の推定値）と自由度を表している。p-valueはp値と呼ばれるある確率を表しており、先述の通り、注意の必要な指標である。また、t.test() は、信頼区間や標本平均も出力してくれるため、これらの結果に基づき解釈を行うことも可能である。帰無仮説の棄却に至るp値の基準は慣習的に、0.10（10%）、0.05（5%）、0.01（1%）が用いられる。今回の結果では、p値がp-value = 0.001377であり、5%水準で帰無仮説を棄却することができるため、新型電球の寿命は旧型（1800時間）よりも有意に高いと結論づけることができる。 上述の t.test() は母分散が未知である際に用いられる検定方法である。この点は、信頼区間において説明した内容と同様である。なお実際のデータ分析作業においては多くの場合母分散は未知であるため、t.test() を用いることが多い。しかしながら、電球の例では母集団の分散は \\(\\small 180^2\\) であることを仮定した。そのため、ここからは母分散が既知（\\(\\small \\sigma^2=180^2\\)）であることを仮定した標準正規分布に基づく母平均の検定を軸に説明していく。 電球の例においては、\\(H_0:~\\mu=1800\\)と設計していたため、検定統計量 Z は以下の通りに書き換えることができる。ただし、今回の例においては、 \\(\\small \\bar{X}=1951\\)、\\(\\small \\sigma=180\\) であることがわかっている。 \\[ Z=\\frac{1951-1800}{\\sqrt{180^2/16}} \\] ここまでの議論を踏まえ、新型電球に関する統計的検定を標準正規分布に基づき以下のように実施する。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 mu &lt;- 1800 #Test statistic Z &lt;- (xbar - mu)/(sigma/sqrt(n)) Z ## [1] 3.359861 分析の結果、検定統計量 Z の実現値が5%有意水準に基づく臨界値（1.96）よりも大きいことが示されたため、5%有意水準で帰無仮説が棄却された。つまり、5%の第一種の誤りの確率を残した上ではあるが、新型電球の製品寿命は旧型製品の寿命よりも長いと言える。このような結果は一般的に、「統計的に有意」な結果と表現される。 ここまでは、有意水準の意味を踏まえ、検定の手順及び結果の解釈について説明した。しかしながら、先述の通りもし統計的に有意でない（帰無仮説を棄却できない）結果を得たときには、その解釈について注意が必要である。もし今回の仮説検定で帰無仮説を棄却できていなかったとしたら、その結論は「新型電球寿命の平均は1800時間ではないとは言えない」となる。なんとも歯切れの悪い結論だが、統計的検定の特性上、このような解釈を提示しないといけない。 平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布↩︎ "],["meancomparison.html", "平均値に関するその他の検定", " 平均値に関するその他の検定 これまでは、母平均が特定の値を取るか否かに着目し、統計的仮説検定の基礎について説明した。しかしながら本章の冒頭でも例に挙げた通り、平均値をあるグループ間で比較したいと考えることも多い。本節では、期待値の比較に着目し、平均の差の検定と、分散分析について説明する。これらの検定では、用いる検定統計量は先述のものと異なるが、統計的仮説検定そのものの手順や、肝となる考え方は共通である。 前節で考えた、「女性に比べ男性の方が新製品購買意図が高い。」という作業仮説を再度考える。このとき、我々が観察可能なのは男性グループの標本平均（\\(\\small \\bar{X}\\)）と女性グループの標本平均（\\(\\small \\bar{Y}\\)）であるが、検定においてはそれぞれの期待値（\\(\\small \\mu_x\\) と \\(\\small \\mu_y\\)）に着目し、帰無仮説を作成する。なお、\\(\\small X_1,...,X_n\\) は\\(\\small N(\\mu_x,\\sigma^2_x)\\)に従う母集団からの無作為標本であり、\\(\\small Y_1,...,Y_n\\) は\\(\\small N(\\mu_y,\\sigma^2_y)\\)に従う母集団からの無作為標本であるとする。また、\\(\\small X_1,...,X_n\\) と \\(\\small Y_1,...,Y_n\\) は互いに独立であり、母分散は未知であるとする。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_x\\)、女性における購買意図の期待値を \\(\\small \\mu_y\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\[H_0:~\\mu_x=\\mu_y,~~H_1:~\\mu_x\\neq\\mu_y\\] 統計的検定の手順と直感的な検定統計量の作り方は前節の内容と同じである。そのため、検定における推定量と帰無仮説条件下での未知パラメータの値を特定し検定統計量を定義したい。この検定ではグループ間の平均の差を捉えているため、標本上での情報として \\(\\small \\bar{X}-\\bar{Y}\\) という関係を捉える。したがって、上記の帰無仮説と対立仮説は以下のように書き直すことができる。 \\[H_0:~\\mu_x-\\mu_y=0,~~H_1:~\\mu_x-\\mu_y\\neq0\\] また、母分散が未知である場合にはt検定を行うということも前節と同様である。このことから、以下の検定統計量を用いる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})-(\\mu_x-\\mu_y)}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] ただし、\\(s^2\\) はプールされた標本分散8と呼ばれる母集団の分散の推定量である。なお、\\(s^2\\)は母分散を捉えた推定量であるが、母分散が両群で等しい（等分散: \\(\\small \\sigma^2_x=\\sigma^2_y=\\sigma^2\\)）である場合には上記の検定統計量を自由度（\\(\\small m+n-1\\)）のt分布として分析可能である。一方で等分散ではない場合には、Welchのt検定と呼ばれる、自由度の計算を修正した分析方法を用いる。なお、Welchのt検定で用いられる自由度の詳細はここでは省略する。 このとき、帰無仮説が正しいという仮定のもとでは、\\(\\small \\mu_x-\\mu_y=0\\)である。そのため、上記の検定統計量は以下のように観察可能な情報のみで構成される形で書き換えることができる。また、帰無仮説が正しければ、この検定統計量は自由度（\\(\\small m+n-1\\)）のt分布に従うと考えられる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] そのため、データに基づき計算された検定統計量tの実現値を用いて、以下の方式で検定を行う。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(m+n-1) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(m+n-1)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] Rにおいて平均の差の検定を行うことはさほど難しくない。先程の等分散性についても、var.equal=TRUEまたはvar.equal=FALSEという引数で設定できる。var.equal= 引数についてはTRUEが等分散性を仮定するが、デフォルトでは、FALSEとなっている。 平均の差の検定では、 t.test(outcome ~ category) のように、はじめに着目する成果変数を、その後 ~（チルダ）のあとに着目するカテゴリ変数を指示することで、どの変数（outcome）の平均の差をどのカテゴリ変数（category）で検定するのかが特定化できる。ここでは、以前の章で利用した `firm2018’ データを利用して、広告集中度の高い企業と低い企業とで売上高の平均値に差があるか否かを以下のように分析する。なお以下では、等分散性について異なる仮定を置いた分析を続けて行っている。 library(tidyverse) firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) firm2018 &lt;- firmdata %&gt;% filter(fyear == 2018) %&gt;% mutate(ad_dummy = ifelse(adint &gt; median(adint),1, 0)) t.test(sales ~ ad_dummy, data = firm2018) ## ## Welch Two Sample t-test ## ## data: sales by ad_dummy ## t = -3.3989, df = 85.686, p-value = 0.001029 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -1674283.1 -438496.1 ## sample estimates: ## mean in group 0 mean in group 1 ## 725009.7 1781399.3 t.test(sales ~ ad_dummy, data = firm2018,var.equal=T) ## ## Two Sample t-test ## ## data: sales by ad_dummy ## t = -3.4555, df = 145, p-value = 0.0007207 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -1660616.4 -452162.8 ## sample estimates: ## mean in group 0 mean in group 1 ## 725009.7 1781399.3 出力結果の見方は、先述のt検定の場合と同様である。分析の結果、等分散性を仮定するか否かで、計算結果は微妙に異なるが、どちらの検定結果においても1%有意水準で帰無仮説を棄却できた。そのため、グループ間で売上高には差があり、広告集中度の高いグループのほうが売上高が高い（もしくは、売上高の高い企業ほど広告集中度が高いグループに属している）といえる。 なおRにおいては、等分散性に関する検定もvar.test(outcome ~ category)で以下のように実行可能である。 var.test(sales ~ ad_dummy, data = firm2018, ratio = 1) ## ## F test to compare two variances ## ## data: sales by ad_dummy ## F = 0.10863, num df = 74, denom df = 71, p-value &lt; 2.2e-16 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.06821636 0.17258882 ## sample estimates: ## ratio of variances ## 0.1086276 等分散性の検定では、ひとつのグループの分散ともう一方のグループの分散の比が1（等分散）であるという帰無仮説を設計する。詳細は割愛するが、帰無仮説が正しい場合には両グループの不偏標本分散の比が自由度（\\(m-1\\), \\(n-1\\)）のF分布に従う。これまでと異なる出力結果として、検定統計量の実現値F = 0.10863、分子の自由度num df = 74、分母の自由度 denom df = 71が提示される。なお、分析の結果、帰無仮説は棄却されたため、等分散とは言えないと結論づけることができる。そのため、平均の差の検定においては、Welchのt検定を利用した分析結果を採用して議論することが好ましい。 \\(s^2=\\frac{1}{m+n-1}\\{(m-1)s_x^2+(n-1)s_y^2\\}\\)↩︎ "],["ANOVA.html", "分散分析", " 分散分析 ここまでの内容では、２グループ間の平均の差に関する分析を捉えた。しかしながら、三つ以上のグループ間の平均の差に関心があることもある。例えば、異なる地域における売上高の差を比較したい場合が挙げられる。このような目的を持つ場合によく用いられるのが分散分析（Analysis of Variance; ANOVA）である。 ANOVAの構造については、要因と水準という二つの要素から説明が行われる。要因とは、観測値に影響を与えていると考えうるカテゴリ変数のことを指し、水準とは、要因を構成するいくつかの条件やグループを指す。例えばある小売企業における各店舗の一定期間内の売上高が出店エリア特性によって差があるのかという問いに関心があるとする。その際各店舗を、都市エリア、郊外エリア、農村エリアという三つのグループに分類し、それぞれのグループにおける標本平均を求めれば、エリアごとの差を分析できるだろう。この場合、「地域」が売上高に影響を与えうる要因であり、「都市、郊外、農村」という三つのグループが水準だと言える。 分析において取り上げる要因が一つである分散分析を一元配置分散分析と呼ぶ。二元配置分散分析については以下の節でモデル表現と共に紹介している。 分散分析についてのモデル表現*{#ANOAtheory} 本節では、分散分析の構造をモデルによって表現することでより明確な理解を促すことを目指す 。ここでは、要因をA、グループ（水準）の数をJとする。第\\(j\\)グループで\\(n_j\\)個の観測値が得られる。第\\(j\\)水準における\\(i\\)番目の個体の観測値を\\(y_{ij}\\)と表し、以下が成り立つとする。 \\[ y_{ij}=\\mu_j+e_{ij},~~~~e_{ij} \\sim N(0,\\sigma^2), \\] ただし、\\(i=1,2,...,n_j\\); \\(j=1,2,...,J\\)であるとする。 すべての\\(y_{ij}\\)は独立であり、 \\[ y_{ij}\\sim N(\\mu_j, \\sigma^2) \\] が成り立つ。したがって、\\(E(y_{ij})=\\mu_j\\)と、\\(V(y_{ij}=\\sigma^2)\\)も成り立つ。 上記モデルの関心は各グループで平均に差が存在するか否かである。そこで、以下の\\(\\mu\\)を全体平均とする。 \\[ \\mu=\\frac{n_1}{n}\\mu_1+\\frac{n_2}{n}\\mu_2+...+\\frac{n_J}{n}\\mu_J=\\frac{1}{n}\\sum^J_{j=1}n_j\\mu_j. \\] また、 \\(\\alpha_j=\\mu_j-\\mu\\)を第 \\(j\\)グループの効果と呼び、以下のように表現できる。 \\[ y_{ij}=\\mu+\\alpha_j+e_{ij} , \\] このとき定義より、 \\[ \\frac{1}{n}\\sum^J_{j=1}n_j\\alpha_j=0, \\]が成り立つ。もし、\\(\\alpha_1=\\alpha_2=...=\\alpha_J=0\\)ならば、以下のような水準間の差が存在しないモデルとなる \\[ y_{ij}=\\mu+e_{ij}, \\] 分散分析の第一の目的は、要因が観測値に対して影響を与えているかどうか、すなわち、\\(\\mu_1=\\mu_2=...=\\mu_J\\)が成立しているかどうかを調べることである。これは全てのグループで主効果\\(\\alpha_j\\)がゼロであるかどうかを調べることに等しい。そのため、以下を検定することと等しい。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(n-2) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(n-2)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] \\[ \\begin{cases} H_0:~ &amp;\\alpha_1=\\alpha_2=...=\\alpha_J=0\\\\ H_1:~ &amp;\\text{少なくとも１つの}\\alpha_j \\text{は0ではない} \\end{cases} \\] ここで、総変動: \\(S_T=\\sum^J_{j=1}\\sum^{n_j}_{i=1}(y_{ij}-\\bar{y})^2\\)を、群間変動: \\(S_B=\\sum^J_{j=1}n_j(\\bar{y_j}-\\bar{y})^2\\)と、郡内変動: \\(S_W=\\sum^J_{j=1}\\sum^{n_j}_{i=1}(y_{ij}-\\bar{y_j})^2\\)とに分ける(ただし、\\(S_T=S_B+S_W\\))。このとき、群間変動が大きいほど各\\(\\alpha\\)の推定値\\(\\hat{\\alpha_j}=\\bar{y_j}-\\bar{y}\\)のばらつき、すなわちグループ間での平均の違いが大きく、対立仮説\\(H_1\\)が支持されやすい。 しかしながら、この値自体は単位に依存し意味が無いため、以下の群間変動の郡内変動に対する比に基づいて検定を行う。 \\[ F=\\frac{S_B/(J-1)}{S_W/(n-J)}=\\frac{n-J}{J-1}\\times \\frac{S_B}{S_W} \\] Fは帰無仮説が正しい時に自由度（\\(J-1\\), \\(n-J\\)）のF分布に従うことが知られている。そのため、有意水準 \\(\\alpha\\) に基づく\\(H_0\\)に関する検定は、以下のように表すことができる。 \\[ \\begin{cases} F&gt;F_\\alpha(J-1, n-J)\\Rightarrow H_0\\text{を棄却}\\\\ F\\leq F_\\alpha(J-1, n-J)\\Rightarrow H_0\\text{を採択} \\end{cases} \\] 二元配置分散分析*{#twowayANOA} 一元配置分散分析観測値に影響を与える要因が2個あるモデルを考える。この場合の最も特徴的な違いが交互作用効果である。2つの要因をAとBとし、Aのグループの数をJ、Bのグループの数をKとする。（\\(A_j\\), \\(B_k\\)）につきr個の観測値\\(y_{ijk}\\)（\\(i=1,2,...,r\\)）が得られるとし、以下のモデルを考える。 \\[ y_{ijk}=\\mu_{jk}+e_{ijk}, ~e_{ijk}\\sim N(0,\\sigma^2) \\] ただし、\\(i=1,2,...,r\\); \\(j=1,2,...,J\\); \\(k=1,2,...,K\\)。 前節の内容と同様に全体平均と要因効果とに分解し、全体平均と呼ぶ。 \\[ \\mu=\\frac{1}{JK}\\sum^J_{j=1}\\sum^K_{k=1}=\\mu_{jk}, \\] そして、全体平均を以下のように分解し、前者を行平均、後者を列平均と呼ぶ。 \\[\\begin{eqnarray} \\mu_{j.}=\\frac{1}{K}\\sum^K_{k=1}=\\mu_{jk}, ~~ \\mu_{.k}=\\frac{1}{J}\\sum^J_{j=1}=\\mu_{jk}, \\end{eqnarray}\\] このとき、要因Aの第\\(j\\)グループの効果（\\(\\alpha_j\\)）、要因Bの第\\(k\\)グループの効果（\\(\\beta_k\\)）をそれぞれ、以下のように定義する。 \\[ \\alpha_j=\\mu_{j.}-\\mu, ~~\\beta_k=\\mu_{.k}-\\mu \\] この時、以下の恒等式が成り立つ。 \\[ \\mu_jk=\\mu+(\\mu_{j.}-\\mu)+(\\mu_{.k}-\\mu)+(\\mu_{jk}-\\mu_{j.}-\\mu_{.k}-\\mu)\\nonumber\\\\ =\\mu+\\alpha_j+\\beta_k+(\\alpha \\beta)_{jk}, \\] ただし、効果\\(\\alpha_j\\), \\(\\beta_k\\)を要因AとBの主効果、\\((\\alpha \\beta)_{jk}\\)をABの交互作用効果と呼ぶ。そして、定義より以下を示すことができる。 \\[ \\sum^J_{j=1}\\alpha_j=0, ~~\\sum^K_{k=1}\\beta_k=0, ~~\\sum^J_{j=1}(\\alpha \\beta)_{jk}=0, ~~ \\sum^K_{k=1}(\\alpha \\beta)_{jk}=0 \\] なお、交互作用効果が無いモデルは、以下のように示すことができる。 \\[\\begin{eqnarray} \\mu_{jk}=\\mu+\\alpha_j+\\beta_k \\end{eqnarray}\\] 二元配置モデルの効果検定*{#twowaytest} 二元配置モデルにおいても、以下のような要因Aの主効果に関する仮説に関心がある。 \\[ \\begin{cases} H_0:~ \\alpha_1=\\alpha_2=...=\\alpha_J=0\\\\ H_1:~ \\text{少なくとも１つの}\\alpha_jは0\\text{ではない} \\end{cases} \\] また、要因Bの主効果に関する仮説は以下のように定義できる。 \\[ \\begin{cases} H_0:~ \\beta_1=\\beta_2=...=\\beta_K=0\\\\ H_1:~ \\text{少なくとも１つの}\\beta_kは0\\text{ではない} \\end{cases} \\] これらに加え、交互作用効果に関する仮説は以下のように示すことができる。 \\[ \\begin{cases} H_0:~ (\\alpha \\beta)_{11}=(\\alpha \\beta)_{12}=...=(\\alpha \\beta)_{JK}=0\\\\ H_1:~ \\text{少なくとも１つの}(\\alpha \\beta)_{jk}\\text{ではない} \\end{cases} \\] これらの仮説の検定ではまず、以下のように「総変動」、「A間変動」、「B間変動」、「交互作用変動」、「グループ内変動」を定義する。ただし以下では、\\(S_T=S_A+S_B+S_{AB}+S_W\\) とする。 \\[ \\text{総変動:} S_T=\\sum^r_{i=1}\\sum^J_{j=1}\\sum^K_{k=1}=(y_{ijk}-\\bar{y})^2 \\] \\[ \\text{A間変動:} ~S_A=Kr\\sum^J_{j=1}(\\bar{y_{j.}}-\\bar{y})^2 \\] \\[ \\text{B間変動:}~S_B=Jr\\sum^K_{k=1}(\\bar{y_{.k}}-\\bar{y})^2 \\] \\[ \\text{交互作用変動:}~S_{AB}=r\\sum^K_{k=1}\\sum^J_{j=1}(\\bar{y_{jk}}-\\bar{y_{j.}}-\\bar{y_{.k}}-\\bar{y})^2 \\] \\[ \\text{グループ内変動:}~S_W=\\sum^r_{i=1}\\sum^K_{k=1}\\sum^J_{j=1}(y_{ijk}-\\bar{y_{jk}})^2 \\] これらを用いて、以下のような \\(F_A\\)、\\(F_B\\)、\\(F_{AB}\\) を検定統計量として検定を行う。 \\[ F_A=\\frac{S_A/(J-1)}{S_W/JK(r-1)} \\] \\[ F_B=\\frac{S_B/(K-1)}{S_W/JK(r-1)} \\] \\[ F_{AB}=\\frac{S_{AB}/(J-1)(K-1)}{S_W/JK(r-1)} \\] 多重比較とそれに伴う統計的問題 ANOVAにおける帰無仮説の棄却は、少なくとも1つの群は全体と異なる平均値を持っているという結論につながる。しかし、どれが異なる値を持つのかがわからないため、それを特定化するために多重比較と呼ばれる分析を行う。この多重比較には統計的問題が伴うと言われている。本節では多重比較に伴う統計的問題について考える。 多重比較に伴う問題は、第一種の誤り（帰無仮説が正しいときに帰無仮説を棄却する誤り）を起こす確率（有意水準）に着目し議論される。ANOVAにおける帰無仮説は以下のとおりである。 \\[ H_0:~ \\mu_1=\\mu_2=...=\\mu_J \\] この帰無仮説を有意水準5%を設定し、 (8) 式の統計量を用いて検定した場合の\\(H_0\\)に関する有意水準はもちろん5%になる。しかしながら、各グループごとの対となる検定を5%水準で行った場合、\\(H_0:~ \\mu_1=\\mu_2=...=\\mu_J\\)に対する有意水準は5%よりも大きくなってしまうと言われている。 この問題では、先述の\\(H_0\\)に基づき、分散分析が捉えている全体としての第一種の誤りと多重比較で捉えている個別の第一種の誤りという構造を把握する必要がある。言い換えると、多重比較により複数の検定を繰り返し行う場合、その複数の検定を「検定のセット」と考え、セット全体での第一種の誤りについて考えるということになる。 この統計的問題についてより明確に説明するために、1要因3グループの分散分析という具体例を考える。この時、分散分析としての全体の帰無仮説は\\(H_0:~ \\mu_1=\\mu_2=\\mu_3\\)となる。ここで対となる2群環の平均の差の検定を行うと、\\({}_3C_2=3\\) 通りのt検定を行うことになる。例えば、グループ1対グループ2の検定においては \\(H_0^1:~\\mu_1=\\mu_2\\) という帰無仮説を採用して検定を行うことになる。これら各対の検定における帰無仮説が棄却されれば、分散分析全体として捉えていた帰無仮説（\\(H_0:~ \\mu_1=\\mu_2=\\mu_3\\)）も当然棄却されることになる。そのため、各対に関する帰無仮説が少なくともひとつ棄却されることで、全体としての帰無仮説が棄却されるという構造を捉える必要が出てくる。この構造が、第一種の誤りについての問題につながる。 各対に関する帰無仮説に対して第一種の誤りが起こる確率は、「ひとつ目の対の検定において誤りが起きる、またはふたつ目の対の検定において誤りが起きる、またはみっつ目の対の検定において誤りが起きる」という事象の確率を捉えることになる。仮に各対の検定において有意水準5%を設定した場合、3回に1回以上第一種の誤りを起こす確率（全体としての有意水準）は、それぞれの検定において第一種の誤りを犯すという事象が互いに独立だと仮定すると、以下の通り0.05よりも多くなる。 \\[ 1-P(\\text{3回中1回も第一種の誤りを犯さない})=1-(0.95)^3\\approx 0.143 \\] そのため、分散分析に対応する複数の検定を繰り返し行う場合には、複数の検定を検定のセットと捉え、セット全体での第1種の誤りを犯す確率が事前に設定した有意水準（例：0.05等）以下に抑えられるような方法が求められる。この問題に対応した方法が多重比較法であり、代表的な例としてTukeyの多重比較法がある (Tukey’sのPairwise法については例えば南風原（2010)を参照してほしい)。 Rによる分散分析の実行 Rで分散分析を実行すること自体は難しくない。ここでは、?? 節で用いた reshape2 データを用いて、以下の通りチップ額が曜日によって異なるか否かを分析する。ANOVAの実行においてはaov()関数によって分析するモデルとデータを指定し、anova() によって分析結果を出力する（summary() を用いることも可能である）。なお、上記のデータサマリーより、day という要因には四水準 (levels) 含まれていることがうかがえる。 library(reshape2) s &lt;- aov(tip ~ day, data = tips) anova(s) ## Analysis of Variance Table ## ## Response: tip ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## day 3 9.53 3.1753 1.6724 0.1736 ## Residuals 240 455.69 1.8987 分析結果における Sum Sq は、水準間変動和（または、平方和）と呼ばれ、特定要因の水準間によって説明される観測値の変動を表している。 Mean Sq は平均平方と呼ばれ、Sum SqをDF(自由度)で割ったものである。F value は F値という検定統計量の実現値であり、Pr(&gt;F) は本検定の p値を表している。また、Residuals の行で示されているのは、残差平方和と呼ばれ、郡内変動、つまり同グループ（水準）内での値のばらつきの程度を表している。 本結果の場合、曜日間でのチップ額についての差についての推定値（F value）は1.6724 であった。そして、これが統計的に有意なのかについて検討するためにp値を参照する。このときp値は 0.1736 であり、帰無仮説が正しいと仮定したときに，手元のデータから計算した検定統計量以上に極端な値をとる確率は17％以上あるといえる。このことから、有意水準 5% で仮説検定を行っても帰無仮説を棄却できないと解釈でき、曜日による統計的に有意な差があるとは言えない。ANOVA実行している検定では、「すべての水準間で平均値は同じ」という帰無仮説を検定しており、仮に帰無仮説が棄却された場合、「少なくとも一つの水準では値が異なる」という対立仮説を支持する。そのため、ANOVAにおける帰無仮説の棄却は、少なくとも1つの群は全体と異なる平均値を持っているという結論につながる。今回の結果では、このような帰無仮説も棄却するに至らなかったということである。 もし仮にANOVAにおける帰無仮説が棄却されたとしても、それだけでは具体的にどの水準間に差があるのかはわからない。そこで、ANOVAを用いた研究では事後分析として、多重比較と呼ばれる分析を行うことが多い。しかし、この多重比較には統計的な問題が伴うと言われている（詳細は??節参照）。その問題に対応した手法として広く用いられているのが、Tukeyのpair-wise 比較である。Rではこの分析を、TukeyHSD() という関数で実行できる。具体的には、aov() でストアしたANOVAの分析結果を用いて以下のように実施する。また、plot()を用いて、pair-wise比較の結果を95%信頼区間とともに図示化することもできる。 tukey_result &lt;- TukeyHSD(s) tukey_result ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = tip ~ day, data = tips) ## ## $day ## diff lwr upr p adj ## Sat-Fri 0.25836661 -0.6443694 1.1611026 0.8806455 ## Sun-Fri 0.52039474 -0.3939763 1.4347658 0.4558054 ## Thur-Fri 0.03671477 -0.8980753 0.9715049 0.9996235 ## Sun-Sat 0.26202813 -0.2976929 0.8217492 0.6203822 ## Thur-Sat -0.22165184 -0.8141430 0.3708394 0.7678581 ## Thur-Sun -0.48367997 -1.0937520 0.1263921 0.1724212 plot(tukey_result) 分析結果における diff 列は平均値の差を表している。 lwer と upr は信頼区間の下限と上限を表しており、一番右の列はp値を示している。分析の結果、P-valueが10%水準よりも低い結果がないため、どのペアに関する検定でも有意な差は確認できなかった。したがって、分析結果は必ずしもチップ額が曜日によって変化するとは言えないことを示した。この結果は、アメリカにおけるチップ額が会計額に対する割合や提供されたサービス品質によって決まるという慣習から考えると妥当な結果である。しかしながら、前節で注意した通り、このような統計的に非有意な結果をもって「曜日はチップ額に影響を与えない」と結論づけるのは不適切である。 なお、今回の分析においてはANOVAもTukeyのpair-wise比較も有意な結論を得ることができなかったという点で、両者の結果に一貫性があった。しかしながら、ANOVAでは有意だが、Tukeyの分析ではどの組み合わせも有意ではないという一見整合でない結果を得ることもある。その場合には、慣習としてTukeyの多重比較結果を優先して解釈を提示することが多い。しかしながら、研究者が自身の実施した検定の「意味」を理解し、解釈や議論を提示することも重要である。例えば、ANOVAとTukeyでは用いている帰無仮説が異なるため、異なる比較対象を用いた検定を実施している。そのため、自身が実行した分析がどのような帰無仮説を採用しており、何と何の比較を行っているのかを正確に把握し、実施した検定の意味に適した解釈や議論を展開する事が重要になる。 本章では、基礎的な統計学の復習として、主に区間推定と統計的仮説検定について説明した。区間推定では、主に信頼区間の計算に着目し、信頼区間の意味についてきちんと理解、解釈することの重要性を強調した。また、統計的仮説検定では、母平均の検定を起点とし統計的検定の基礎的な構造と考え方について説明した。検定においては、母集団の統計的特徴に関する予測である帰無仮説と対立仮説を設計することが重要である。また、グループ間の差異に着目した検定を行う場合には、関心のある未知パラメータについての差や比に着目し検定統計量を作成する事が多いが、基本的な統計的検定の考え方と手順は変わらないという点についても説明した。 回帰分析と分散分析の関係 分散分析は回帰分析の特別な場合（回帰分析における独立変数をダミー変数とした場合）に対応する。ダミー変数とは以下の様な2値変数である。 \\[ x_i=\\left\\{ \\begin{array}{l} 1~~~\\text{if 被験者iは1群に属する}\\\\ 0~~~\\text{otherwise} \\end{array} \\right. \\] そのため、2標本問題は、\\(y=\\beta_0 +\\beta_1x+e,\\) において、以下を検定することと等しい。 \\[ \\begin{array}{l} H_0:~ \\beta_1=0\\\\ H_1:~ \\beta_1\\not=0 \\end{array} \\] また、ダミー変数の効果は以下のように定数項（切片）の違いとして表すことができる。 \\[ E(y|x=1)=\\beta_0+\\beta_1 \\] \\[ E(y|x=0)=\\beta_0 \\] 二元配置に関しては、以下のように、交差項 (interaction term) を含む回帰分析として分析を行えば良い。 \\[ y=\\beta_0 +\\beta_1x+\\beta_2z+\\beta_3(x\\times z)+e, \\] この場合、xがyに与える影響は以下のように表される。 \\[ \\frac{\\partial y}{\\partial x}=\\beta_1+\\beta_3z \\] 一方で、zがyに与える影響は以下のように表される。 \\[ \\frac{\\partial y}{\\partial z}=\\beta_2+\\beta_3x \\] そのため、xとzがyへ与える影響は互いがそれぞれに依存しており、\\(\\beta_3\\)によってそれぞれの影響に対する調整効果が表されている。 "],["pwr.html", "検定力分析とサンプルサイズ", " 検定力分析とサンプルサイズ 統計的検定を行う場合、サンプルサイズの多さが実務的含意の弊害になりうるという議論も存在する。例えば、平均の差の検定においてサンプルサイズが著しく多いと、実務的にはあまり意味を持たない極めて小さな差であっても、「統計的に有意な差」として判断される事がある。このような弊害を避けるために、検定力分析と呼ばれる分析枠組みからサンプルサイズを計算する方法が用いられる（Cohen, 1988）。 ここでは、以下のような帰無仮説と対立仮説を用いる平均の差の検定を例に取りこのアプローチによるサンプルサイズの検討を紹介する。 \\[ H_0:~~\\mu_1=\\mu_2\\\\ H_1:~~\\mu_1\\neq\\mu_2 \\] 検定力分析では、検定力と効果量という指標を用いる。検定力は第二種の誤りを犯す確率 \\(\\beta\\) を用いて \\(1-\\beta\\) と定義される。概念的に説明すると検定力とは、「母集団において 差があるとき，サンプルにおいて有意な結果が得られる確率」といえる（南風原, 2002, p.143）。一方で効果量は、サンプルサイズで変化することのない、標準化された効果の大きさについての指標である（水元・竹内, 2008）。先述の通り、サンプルサイズが大きいと、この効果量が小さくても統計的に有意な差があると判断されることもある。そのため、効果量を捉えることで実務的にどのような差があるのかを議論することが可能になる。なお、平均の差の検定においては、0.2、0.5、0.8をそれぞれ小さい、中くらい、大きい、に対応する効果量の目安として捉えられている（水本・竹内, 2008）。 検定力分析の枠組みでは、有意水準（\\(\\alpha\\)）、検定力（\\(1-\\beta\\)）、効果量（\\(d\\)）、サンプルサイズ（\\(n\\)）の4つの指標については、他の3つが定まると残りの1つの指標も自動的に求まるという関係を有する（水本・竹内, 2008）。検定力分析においては元々、分析における有意水準、効果量、サンプルサイズから分析の検定力を計算する方法として用いられてきた。しかしながら、上述の各指標同士の関係から、自身が想定する分析・検定の条件（\\(\\alpha\\)、\\(1-\\beta\\)、\\(d\\)）を指定することで、その条件を満たす \\(n\\) を求めることも可能である。この枠組みを採用することで、自身の求める大きさの効果量（例えば、\\(d=0.8\\)）を想定する条件（\\(\\alpha\\)、\\(1-\\beta\\)）で検定するために必要な \\(n\\)を逆算することが可能になる。 ここでは、検定力分析の詳細な計算方法について省略し、Rを用いた分析方法を紹介する。具体的な計算は、pwr::pwr.t.test() というパッケージおよび関数を用いる。なお、pwr には他にも様々な分析モデルに対応するサンプルサイズの計算が可能である。まずは、以下の通りパッケージをインストールし読み込む。 install.packages(&quot;pwr&quot;) library(pwr) ここでは、先行研究で推奨されている\\(\\alpha=0.05\\)（引数では sig.level）、\\(1-\\beta=0.8\\)（引数では power）という水準を用いて、平均の差の検定を行う場合のサンプルサイズを以下のように求める（Cohen, 1988）。 est_n &lt;- pwr.t.test(d=0.8, power=0.8, sig.level=0.05) est_n ## ## Two-sample t test power calculation ## ## n = 25.52458 ## d = 0.8 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 出力結果の通り、上記の条件に基づくサンプルサイズは各グループ26人ずつであるとわかる。そのため、2グループ間の比較を行う場合のサンプルサイズの合計は52となる。また、検定力とサンプルサイズの関係について、plot() 関数を用いて以下のように出力することも可能である。 plot(est_n) なお、pwr.t.test() 関数内の引数設定次第で、検定力を計算することも可能である。 例えば、\\(d=0.8\\), \\(n=30\\), \\(\\alpha=0.05\\) の場合の検定力は以下のように計算できる。 pwr.t.test(d=0.8, n=26, sig.level=0.05) ## ## Two-sample t test power calculation ## ## n = 26 ## d = 0.8 ## sig.level = 0.05 ## power = 0.8074866 ## alternative = two.sided ## ## NOTE: n is number in *each* group そのため、研究者が採用した検定の条件や、分析結果としての効果量が算出できれば、その検定における検定力も同じ関数から計算可能であることが伺えた。 "],["ch4reference.html", "参考文献", " 参考文献 浅野正彦・矢内勇生（2018）「Rによる計量政治学」，オーム社. 岩田暁一（1996）「経済分析のための統計的方法 第２版」，東洋経済新報社. 倉田博史・星野崇宏（2024）「入門統計解析」，新世社. 豊田秀樹（2017）「p値を使って学術論文を書くのは止めよう」,『心理学評論』,60(4), 379-390. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. 南風原(2002)「心理統計学の基礎 総合的理解のために」，有斐閣. 水本篤・竹内理(2008)「研究論文における効果量の報告のために 基礎概念と注意 点」 『英語教育研究』31, 57 66. 宮川公男（2002）「基本統計学」，有斐閣. Baker, M. Statisticians issue warning over misuse of P values. Nature 531, 151 (2016). Cohen J. (1988) Statistical Power Analysis for the Behavioral Sciences, 2nd edition, Lawrence Erlbaum Associates. "],["regintro.html", "回帰分析 ", " 回帰分析 "],["ch5Intro.html", "本章の概要", " 本章の概要 二変数間の関係を捉える分析手法として ?? 章では相関係数を紹介した。相関係数は二変数間の線形関係を表す -1から 1 の値を取る指標である。 しかしながら、相関係数は線形関係の強さ（どれだけデータが直線上に近く分布しているか）を表しているものの、示されている直線の切片や傾きといった線形関数の特徴は捉えられない。本章では、ビジネス領域で最も広く用いられる手法の一つである回帰分析を紹介する。回帰分析は、理論や産業的知識から導出された仮説を検証したり、変数間の関係について予測を行うために用いられる方法である。回帰分析の最も基本的な構造は以下のように線形関数の形で変数間の関係を捉えるモデルを定式化するものである： \\[ y_i=\\alpha+\\beta x_i+u_i \\] このとき、\\(y_i\\) は被説明変数（従属変数）、\\(x_i\\) は説明変数（独立変数）、\\(u_i\\) は誤差項と呼ぶ。\\(\\alpha\\) と \\(\\beta\\) はそれぞれ切片と傾きを表す係数である。経営学・マーケティング領域の研究では、これらの係数について推定・検定することが主たる目的になる事が多い。また、回帰分析では複数の説明変数を含むモデルの定式化も可能である。 本章では、Rを用いた分析手法および、分析結果の解釈について紹介する。回帰分析の原則や解釈上の注意については、別途テキストを参照してほしい。ここでは特に重要な注意点として簡単に以下の三つを紹介する。第一に回帰分析では、切片と傾きパラメータを用いた線形関数で被説明変数と説明変数の関係を示しているが、これはこれらの変数間の平均的な関係を捉えたものである。より具体的には、ある \\(x\\) の値が与えられたときの \\(y\\) の「平均値（期待値）」と \\(x\\) の間には線形の関係があることを示している。第二に、ソフトウェアで回帰分析を実行すると、回帰係数に対する検定を行ってくれるが、このような検定では、「係数がゼロか否か」を検定している。そのため、係数の検定結果（統計的に有意か否か）をもとに、\\(x\\) が \\(y\\) に与える影響の強さや程度について議論することはできないという点に注意が必要である。第三に、重回帰モデルにおける係数解釈とその重要性について強調する。重回帰モデルにおける説明変数の係数は、同モデル内の他の変数の影響をコントロールしたうえでの説明変数が被説明変数へ与える影響を表現している。これは、説明変数が持つ変動のうち他の説明変数とは無関係な変動だけを抽出し、被説明変数との関係を分析する構造になっているためである。この特性は分析におけるコントロール変数の採用や、より信頼性の高い効果検証を目的として広く活用されている。 "],["regpreparation.html", "分析準備", " 分析準備 本章では、変数間の関係を捉える回帰分析について、そのモデルの基礎と統計的推測に基づく解釈を説明する。回帰分析結果から得られる含意は、「予測」と「検証」の二つに大別することができる。その上で特に本書では、「検証」という側面、特に「研究上関心のある説明変数の係数の解釈」を重視する立場を取る。立場が異なれば、回帰分析において何を重視するかという観点も異なるため、注意してほしい。 なお本章では、?? 章でも利用した MktRes_firmdata.xlsxという企業データを用いた分析を行う。次節に移る前に以下の要領でデータを読み込んでほしい。 firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) 本章では主に、firmdata における2019年のデータを抽出し、クロスセクショナルデータとして用いる。以下の様に全データから2019年の情報を抽出してほしい。 library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.2 ✔ tibble 3.3.0 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors firmdata19 &lt;- firmdata %&gt;% filter(fyear == 2019) データを用いた分析を行う場合、取得したデータの記述統計や分布を確認する必要がある。本来であれば研究上重要な変数を対象にデータの特徴を整理するが、ここでは複数の変数の特徴を一括で整理、図示化する方法を提示する。この方法では、GGallyというパッケージのggpairs()という関数を用いるため、以下のようにパッケージをダウンロードしてほしい。 install.packages(&quot;GGally&quot;) firmdata19 データセットから、例として四つの変数を抽出して、ggpairsを実行する。これにより、各変数のヒストグラム（密度形式）と、それぞれの変数間の相関係数と散布図が同図内で示されている。また、ggpairs()内の引数設定によって様々な図示形式を指定できるため、興味のある人は調べてみてほしい。 firmdata19 %&gt;% select(sales, mkexp, emp, operating_profit) %&gt;% GGally::ggpairs()+ labs(title = &quot;ggpairs example&quot;) なお、記述統計については既出の summary()関数にデータフレームを指定することで、データセット全体の記述統計を出力する。ここでは例として先程と同じ変数の記述統計を以下のように出力してみる。 ds1 &lt;- firmdata19 %&gt;% select(sales, mkexp, emp, operating_profit) %&gt;% summary() knitr::kable(ds1, align = &quot;cccc&quot;) sales mkexp emp operating_profit Min. : 11333 Min. :0.01137 Min. : 163 Min. :-40469 1st Qu.: 183525 1st Qu.:0.16714 1st Qu.: 3454 1st Qu.: 7743 Median : 464450 Median :0.25448 Median : 7826 Median : 23904 Mean :1199403 Mean :0.29868 Mean : 20249 Mean : 81088 3rd Qu.:1164243 3rd Qu.:0.37506 3rd Qu.: 24464 3rd Qu.: 63068 Max. :9878866 Max. :0.75650 Max. :160227 Max. :656163 "],["prediction.html", "単回帰モデルと予測値", " 単回帰モデルと予測値 回帰分析絵では、二つの異なる変数 \\(y,~x\\) の関係を \\(\\small y=f(x)\\) のように\\(y\\)を\\(x\\)の関数（\\(f(x)\\)）で示すというアイディアで分析を行う。このとき \\(y\\) を「被説明変数（Explained variable）もしくは従属変数（dependent variable）」、\\(x\\) を「説明変数（Explaining variable）もしくは独立変数（independent variable）」という。そして、被説明変数と説明変数の関係を特定化した式のことを一般的に回帰モデルという。最も基本的な関数型の特定方法は以下のような一次関数による特定化である。 \\[ y=\\beta_0+\\beta_1x \\] このとき、\\(\\small \\beta_0\\) は切片、\\(\\small \\beta_1\\) は傾きを表す係数であり、回帰係数と呼ばれる。 回帰モデルは線形の関係を捉えているものの、実際にデータを入手し散布図を作成すると、以下のように、直線とは異なる結果を得る。そのため、上記のモデルは正確な表現でないことがわかる。 分析者がデータとして得る情報は、\\(y\\) や \\(x\\) の実現値であり、回帰モデルの切片や傾きの値は直接はわからない。そこで、モデルで捉えた直線と実現値のズレを考え、得たデータから回帰モデルのパラメータ（係数）を推定するという方針をとる。モデルで捉えた直線による（係数の推定値に基づく）\\(y\\) と\\(x\\) の関係は、\\(\\small x=x_i\\) のとき、\\(y_i\\) の予測値である\\(\\small \\hat{y}_i\\)（ワイハット）と、係数の推定値 \\(\\small \\hat{\\beta}_0\\)、 \\(\\small \\hat{\\beta}_1\\) を用いて以下のように定義できる。 \\[ \\hat{y}_i=\\hat{\\beta}_0+ \\hat{\\beta}_1 x_i \\] 係数の推定値 \\(\\small \\hat{\\beta}_0\\) と \\(\\small \\hat{\\beta}_1\\) を求めるための計算方法は、（最尤法や積率法など）いくつかあるものの、本書では最小二乗法（Ordinary least square: OLS）という方法に着目し紹介する。OLS推定量（OLS Estimator: OLSE）の求め方の直感は、以下の図の通り、観測値と回帰直線間の距離の合計（残差平方和）を最小にするように計算される。 OLSE概要 予測値のモデルで示されているのは、データを分析した結果求めたOLSEに基づく説明変数 \\(x_i\\) と、\\(\\small \\hat{y}_i\\) との関係である。\\(\\small \\hat{y}_i\\) は被説明変数 \\(y_i\\) の「予測値（predicted value）」や「理論値（fitted value）」と呼ばれるものであり、\\(y_i\\) の観測値とは異なる値であることに注意が必要である。 Rによる回帰分析は、lm()という関数（linear model）を用いて簡単に実行できる。この関数内では、lm(y ~ x, data = df) という要領で、説明変数と被説明変数を \\(\\sim\\)（チルダ）で繋いでモデルを指定する。例えば、先程の企業データにおける2019年の観測を用いて、従業員数と売上高の関係について分析するためには、以下のように分析を実行する。 reg1 &lt;- lm(sales ~ emp, data = firmdata19) coef(reg1) ## (Intercept) emp ## 22113.98050 58.14081 分析の結果、定数項（Intercept）は 22809.7 で、傾きは 58.1 であることがわかった。つまり、従業員数を一単位増やすと、売上高が58.1（百万円）増えることを示唆している。仮に従業員数が10人であれば、売上高の「予測値」は以下のように計算できる。 \\[ 22695.0=22114.0+58.1\\times 10 \\] 回帰分析による予測の精度を調べるために、分析したモデルがどの程度被説明変数全体の分散を説明しているか、という指標によってモデルの適合度を測る。一般的には、決定係数（\\(\\small R^2\\)）という指標によってモデル適合度が示される。\\(\\small R^2\\) は以下のように定義される。 \\[ R^2=1-\\frac{\\sum(y_i-\\hat{y}_i)^2}{\\sum(y_i-\\bar{y})^2}=\\frac{\\sum(\\hat{y}_i-\\bar{y})^2}{\\sum(y_i-\\bar{y})^2} \\] この指標は、被説明変数の分散を説明変数がどの程度説明するかの割合を表しており、0以上1以下の値を取る。例えば \\(\\small R^2\\) が0.80であるならば、被説明変数の変動の80%をモデルが説明しているということになる。そのため、\\(\\small R^2\\) は、回帰モデルの説明力として解釈される。しかしながら、予測という目的に対して近年は、機械学習などの発展的な手法が応用される事が多く、\\(\\small R^2\\) を軸に予測を行うことは少なくなってきている。 また、予測ではなく説明変数の効果（係数）についての検証や解釈に関心がある場合、回帰分析における \\(\\small R^2\\) の重要性は低くなる。特に、ビジネス分野における研究では、係数の推定や検定に焦点をあわせることが多い。本書においても、予測よりも係数に関する検証を重視する立場を取る。社会科学領域での分析では、\\(\\small R^2\\) が低くなることは珍しくない。そんな中で、「\\(\\small R^2\\) が低いからその回帰分析結果は意味がない」ということにはならない。研究者の目的が、関心のある変数同士（例、市場志向と企業成果）の関係性（有意性や影響の強さ）を検証したいというものである場合、仮に \\(\\small R^2\\) が低くても、きちんと両変数の関係を分析できる調査設計や分析を実行しているならば、その検証は有意義なものになる。つまりここで強調したいのは、係数の検証や解釈を重視して研究を行う場合、「\\(\\small R^2\\) がいくつ以上（以下）だから良い（ダメ）」という議論は目的と整合的ではなく、重要ではなくなるということである。 本節では、OLSを中心にデータから回帰係数を推定するプロセスに目を向け、予測値と決定係数について紹介した。しかしながら、先述の通り我々は多くの場合特定の変数が成果変数に与える影響の検証に関心がある。次節では確率的な視点から理論的に回帰分析を理解する事により、回帰分析の結果の解釈についてより詳しく学ぶ。 企業データを用いた回帰分析の実行 Rで回帰係数の検定結果を得るのは非常に簡単である。 lm() 関数の実行結果をストアしたオブジェクトに対して、summary() 関数を実行することで統計的検定結果を得ることができる。先程分析した reg1 を再度利用すると、以下のような結果を得る。 summary(reg1) ## ## Call: ## lm(formula = sales ~ emp, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1834902 -280228 -34549 136598 3292521 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22113.980 89224.761 0.248 0.805 ## emp 58.141 2.569 22.628 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 875800 on 144 degrees of freedom ## Multiple R-squared: 0.7805, Adjusted R-squared: 0.779 ## F-statistic: 512 on 1 and 144 DF, p-value: &lt; 2.2e-16 回帰係数の推定と検定に関する結果は Coefficients: の下に記載されている。推定・検定結果は表のような形式で表示されており、Estimate の列は回帰係数の推定結果、Std. Error は標準誤差（詳細は省略するが、誤差項の分散推定量の平方根）、 t valueはt値、 Pr(&gt;|t|)はp値をそれぞれ示している。そして、出力結果下欄には決定係数（R-squared）や自由度調整済み決定係数（Adjusted R-squared）、F検定結果、といったモデル適合度に関する結果が提示されている。 上記の結果を解釈するために、回帰分析における検定について説明する。ソフトウェアで自動的に出力される統計的仮説検定は、基本的には以下の帰無仮説と対立仮説を採用したものである（添字は省略）。 \\[ H_0:\\beta=0,~~H_1:\\beta\\neq0 \\] なお、R以外のソフトウェアを用いて回帰分析を実行しても係数に関する検定結果を返すが、通常はこの帰無仮説を採用した検定結果を出力する。 検定では、以下のような検定統計量を用いる。 \\[ t=\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})} \\] \\(H_0\\) が正しい（\\(\\small \\beta=0\\)）と仮定すると、検定統計量 t は計算可能であり、自由度（\\(\\small n-2\\)）のt分布に従う。検定の手順は ??節で紹介したのと同様、有意確率に基づく臨界値を定めた後、t 値を計算し、棄却域と採択域のどちらに入るのかを確認する。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(n-2) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(n-2)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] これを踏まえて分析結果を確認すると、emp が sales に与える影響（係数: 58.132）は有意に0とは異なると理解できる。また、切片の係数推定値（Intercept）は大きな値を取っているが、統計的には0ではないとは言えないことが示されている。この項は、従業員数が0のときの企業の売上を示しており、この結果が統計的に有意ではないということは、我々の直感とも整合的である。 上記の検定によって、どうやら emp の係数は0ではなさそうだということが示唆された。では、具体的にどのような値を取るのだろうか。おおよその値だけでも把握したい。そこで、信頼区間を求め、おおよその確率（95%など）で真のパラメータが含まれている区間を確認する。OLSEの漸近的性質と中心極限定理により、サンプルサイズが十分に大きいとき、先述の統計量 t は標準正規分布に近づくことが知られている（西山ほか,2019）。 そのため、標準正規分布に基づく信頼区間の推定を応用できる。信頼係数を \\(\\small \\alpha\\) とすると、以下の確率と区間の対応関係を得る。 \\[ P\\left(\\left|\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})}\\right|\\leq z_{\\alpha/2}\\right)=1-\\alpha \\] そして、上記を \\(\\small \\beta\\) に関する不等式に変換すると、以下の信頼区間を得る。 \\[ P(\\hat{\\beta}-se(\\hat{\\beta})\\cdot z_{\\alpha/2}\\leq\\beta\\leq\\hat{\\beta}+se(\\hat{\\beta})\\cdot z_{\\alpha/2})=1-\\alpha \\] したがって、\\(\\small [\\hat{\\beta}\\pm se(\\hat{\\beta})\\cdot z_{\\alpha/2}]\\) という観察可能な情報によって信頼区間が推定できる。Rによって信頼区間を得るには、回帰分析の結果に対して、confint() 関数を用いる（デフォルトで95%信頼係数が設定されている）。例えば、先程の reg1の結果を用いて、99%信頼区間を求めると、以下のような結果を得る。 confint(reg1,level = 0.99) ## 0.5 % 99.5 % ## (Intercept) -210798.52845 255026.48944 ## emp 51.43362 64.84799 したがって、emp の99%信頼区間が [51.4, 64.8] であることがわかった。すなわち、企業の従業員が一名多いと、売上高が 51から64 百万円高くなりそうだと解釈できる。一方で、(Intercept) の信頼区間には0を含んでいることが伺える。なお、confint() 関数によって計算される信頼区間の計算では上述の通り正規分布が仮定されており、詳しくはヘルプ（?confint）で確認できる。 "],["multiplereg.html", "重回帰モデル", " 重回帰モデル 本章のこれまでの内容では、回帰分析の概要や係数の検定・推定について説明した。回帰分析を実行することで得る情報は前節の内容がほとんどなのだが、モデルの特定化に関して、もう一つ重要な点が存在する。それが本節で扱う重回帰モデル（multiple regression model）の採用である。重回帰モデルとは、二つ以上の説明変数を含む回帰モデルのことである。一方で、前節で扱ったような説明変数が一つの回帰モデルのことを単回帰（simple regression model）という。回帰分析を用いた研究を行う際には、基本的に単回帰分析ではなく、重回帰分析を実行することが好ましい。通常の分析においては、ある被説明変数に対して考慮すべき説明変数は一つだけではなく、複数の説明変数を考慮すべき状況が多い。しかし、分析に不慣れな学生においては、複数の説明変数に関心がある場合であっても、複数の単回帰モデルを分析することで、それぞれの変数についての分析結果を得ようとすることが散見される（例えば、三つの説明変数の影響を捉えるために単回帰モデルを三本分析する等）。しかしながら本書では、基本的にはこのような分析アプローチは好ましくなく、複数の説明変数を含めた一本の重回帰分析を実施すべきだと主張する。 重回帰モデル概要 ある成果変数を説明するために、複数の説明変数が必要になることは、マーケティングリサーチにおいても珍しいことではない。例えば、ある製品のパフォーマンスを月次売上高で測るとする。マーケティング部門として、売上高に対してプロモーション施策がどれだけ貢献しているかを分析する際、プロモーションと売上高の関係を回帰分析で捉えるというアプローチが実現可能な分析方法として考えられる。 しかしながら、売上高を説明する変数はプロモーションだけで十分だろうか。マーケティング変数に着目するだけでも、価格や製品品質、流通網など、異なる変数が売上に関係していることが考えられる。例えば、一見プロモーションによる効果のような結果を得たとしても、実際にはその製品の価格による影響であり、プロモーションそのものにはあまり効果がないかもしれない。そのため、他の要素の影響を排除した上での純粋なプロモーション効果を明らかにすることは実務的に有意義な研究課題となりうる。そして、このような研究課題に対応する分析方法が、重回帰分析である。 重回帰分析においても単回帰同様、回帰モデルを記述することができる。k 個の説明変数を含む重回帰モデルは、以下のように示される。 \\[ y_i = \\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}+...+\\beta_kx_{ki}+u_i \\] 論文やレポート内に重回帰モデルを記載する際にも、多くの場合上記の誤差項を含む理論モデルを用いる。 回帰係数の解釈 ここからは、重回帰分析の係数の解釈について説明する。ここで説明する解釈は「なぜ基本的には重回帰モデルを採用すべきなのか」を理解するために重要な内容である。結論から述べると、重回帰分析における説明変数の係数は、「説明変数が持つ変動のうち他の説明変数とは無関係な変動だけを抽出し、被説明変数との関係を分析している」と解釈できる。この特徴が、同モデル内の「他の変数の影響をコントロールしたうえで」説明変数が被説明変数へ与える影響を捉える方法として、学術的にも実務的にも活用されている。 重回帰モデルにおける各説明変数の係数は、パーシャル効果として解釈できる。以下では、このパーシャル効果の直感について、Wooldridge（2012）を参考に説明する。まず、以下のような説明変数が二個である重回帰モデルを考える。 \\[ y_i=\\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}+u \\] そして、上のモデルにおける予測値は以下のように示すことができる。 \\[ \\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+\\hat{\\beta}_2x_{2i} \\] このとき、説明変数 \\(\\small x_1\\) と \\(\\small x_2\\) の変化をそれぞれ、 \\(\\small \\Delta x_{1i}\\) と \\(\\small \\Delta x_{2i}\\) とすると、予測値の変化（\\(\\small \\Delta \\hat{y}\\)）は以下のように表すことができる。 \\[ \\Delta\\hat{y}_i=\\hat{\\beta}_1\\Delta x_{1i}+\\hat{\\beta}_2\\Delta x_{2i} \\] ここで、\\(\\small x_2\\) を固定（\\(\\small \\Delta x_{2i}=0\\)）すると、以下を得る。 \\[ \\Delta\\hat{y}_i=\\hat{\\beta}_1\\Delta x_{1i} \\] つまり、重回帰モデルにおける \\(\\hat{\\beta}_1\\) は、別の説明変数を固定（\\(\\small \\Delta x_{2i}=0\\)）した上で、\\(\\small x_1\\) が \\(\\small \\hat{y}\\) に与える影響（\\(\\small x_1\\) が変化した際の \\(\\small \\hat{y}\\)の変化の程度）を捉えていると解釈できる。また、\\(\\small \\hat{\\beta}_2\\) についても同様に解釈できる。そしてこの特徴は、k個の説明変数を用いたモデルにも同様に適応できる。 "],["modelcomp.html", "単回帰モデルと重回帰モデルの比較", " 単回帰モデルと重回帰モデルの比較 先述のパーシャル効果という重回帰モデルの特徴は、どのように応用できるのだろうか。多くの実証研究では、重回帰モデルの特徴を利用し、「コントロール変数」を用いた分析を行っている。本節では、先程の企業データを用いて、「企業の広告支出が営業利益に与える影響を明らかにする」という問いを考える。まずは、学習的意図から以下のように単回帰分析を実施してみる（通常の論文・レポートであればこのような過程は必要ない）。 reg2 &lt;- lm(operating_profit ~ adv, data = firmdata19) summary(reg2) ## ## Call: ## lm(formula = operating_profit ~ adv, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -450526 -55552 -40672 -791 599084 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.708e+04 1.057e+04 5.401 2.68e-07 *** ## adv 1.257e+00 2.159e-01 5.823 3.61e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 117600 on 144 degrees of freedom ## Multiple R-squared: 0.1906, Adjusted R-squared: 0.185 ## F-statistic: 33.91 on 1 and 144 DF, p-value: 3.613e-08 confint(reg2) ## 2.5 % 97.5 % ## (Intercept) 36189.311039 77968.894422 ## adv 0.830362 1.683719 分析の結果、広告支出（adv）の係数は正に有意であり、その95%信頼区間は [0.83, 1.68] であることが確認できた。 しかしながら、このモデル化は不十分であり他の要素も考慮すべきである。営業利益に影響を与えうる要因は色々とあり、実際の研究においては先行研究を参照しつつ、コントロールすべき変数を含める形で回帰モデルを特定する必要がある。ここでは便宜上いくつかの要因にのみ焦点を合わせて簡単に特定化する。本データは主に小売・サービス産業の企業に焦点を合わせている。そのため、対人サービス水準は企業のパフォーマンスに影響を与えうる要因である。そのため、従業員に関する変数（従業員数: emp、パートタイム従業員数: temp）と人件費（labor_cost）をモデルに含める。また、資産合計（total_assets）、研究開発費（rd）もモデルに含める。今回の回帰モデルは以下のように示される。 \\[ \\text{opretating_profit}_i = \\beta_0 + \\beta_1 adv_i + \\beta_2emp_i+\\beta_3temp_i+\\beta_4\\text{labor_cost}_i+\\beta_5\\text{total_assets}_i+\\beta_6rd_i+u_i \\] Rにおいて重回帰分析を実行するのは簡単である。lm(y ~ x1 + x2 + x3) のように \\(+\\) 記号と変数を追加すれば、重回帰モデルとして分析を実行してくれる。 reg3 &lt;- lm(operating_profit ~ adv + temp + emp + labor_cost + total_assets + rd, data = firmdata19) summary(reg3) ## ## Call: ## lm(formula = operating_profit ~ adv + temp + emp + labor_cost + ## total_assets + rd, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -360544 -27618 -15279 3467 284094 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.209e+04 8.114e+03 2.723 0.00731 ** ## adv -1.431e+00 2.954e-01 -4.845 3.34e-06 *** ## temp -1.878e+00 6.313e-01 -2.975 0.00346 ** ## emp -1.510e+00 7.036e-01 -2.146 0.03358 * ## labor_cost 8.808e-01 1.692e-01 5.207 6.76e-07 *** ## total_assets 3.516e-02 5.840e-03 6.020 1.47e-08 *** ## rd 1.385e+00 5.268e-01 2.629 0.00953 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 78940 on 139 degrees of freedom ## Multiple R-squared: 0.6479, Adjusted R-squared: 0.6327 ## F-statistic: 42.63 on 6 and 139 DF, p-value: &lt; 2.2e-16 confint(reg3) ## 2.5 % 97.5 % ## (Intercept) 6048.51599480 3.813231e+04 ## adv -2.01511513 -8.470054e-01 ## temp -3.12602311 -6.298318e-01 ## emp -2.90144939 -1.190356e-01 ## labor_cost 0.54637427 1.215313e+00 ## total_assets 0.02361376 4.670793e-02 ## rd 0.34334525 2.426472e+00 見ての通り、結果の出力方式そのものは単回帰分析のものとほぼ同様である。回帰係数の結果の下にあるモデル適合度については前節を参照して欲しい。 分析の結果、広告支出の係数は「負」に有意であり、その信頼区間も [-2.01, -0.85] であった。したがって、本データの分析によると、労働や資産に加え研究開発といった側面を一定とすると、広告支出は営業利益に対して負の関係を持っていることがわかった。他の変数に着目すると、従業員数に関する変数はどちらも負に有意であった。一方で、人件費と総資産、研究開発費は正に有意な影響を与えることが示された。これらの結果から、単純に従業員数を増やしても営業利益には負の影響を与える一方で、従業員数を一定とした上で人件費を上げるほうが営業利益が高いことが示された。また、資産や研究開発費も営業利益につながることが示された。 このように、重回帰モデルを採用し複数の説明変数を含めることで、各係数の持つ含意が大きく変わることに注意して欲しい。また、reg2 と reg3の比較のように、特定の説明変数に対応する係数の符号が変わることも珍しくない。そのため、回帰モデルの定式化には非常に慎重になる必要があり、先述の通り、先行研究を参照して必要な変数をコントロールすることが求められる。 "],["参考文献.html", "参考文献", " 参考文献 秋山裕（2018）「Rによる計量経済学 第2版」，オーム社. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Nishikawa, H., Schreier, M., Fuchs, C., &amp; Ogawa, S. (2017). The Value of Marketing Crowdsourced New Products as Such: Evidence from Two Randomized Field Experiments. Journal of Marketing Research, 54(4), 525-539. Wooldridge, J. (2012) Introductory Econometrics A Modern Approach,Cengage Learning. "],["regtechnic.html", "回帰分析上の工夫紹介 ", " 回帰分析上の工夫紹介 "],["ch6Intro.html", "本章の概要", " 本章の概要 ?? 章では、回帰分析の基本的な構造と重回帰分析を中心に結果の解釈方法について説明した。しかしながら、回帰分析は単純な二変数間の直線関係（傾き）を調べる以上の複雑な分析にも応用できる。例えば、マーケティング領域の研究では、消費者の属性（例えば、性別やカテゴリなど）のように、カテゴリを表す変数が被説明変数へ与える影響に関心があることも多い。また、説明変数が被説明変数へ与える影響が、別の要因によって変化するか否かも分析することに関心がある場合も多い。着目する二つの異なる変数のうちどちらが被説明変数へ与える影響が強いかを比較したいという研究目的を持つこともあるかもしれない。もしくは収穫逓減（逓増）や弾力性といった非線形のモデルを推定したいと考えることもあるだろう。 これらの問題を回帰モデルのフォーマット上で表現できれば、我々がこれまでに学んだ手法で分析可能である。しかしながらこれらの発展的な手法については、その背後にあるモデル上の原則を理解していないと、不適切な手法の活用や結果の解釈に至ってしまう。そのため、本章ではこれらの目的を達成するためのモデル定式化や結果の解釈について、その背後にある統計的な理屈も含めて紹介する。具体的には、以下の内容について説明する。 第一に紹介する内容は、ダミー変数の活用についてである。ダミー変数とは、観測個体があるカテゴリに属するなら1を、そうでなければ0を取るような変数のことである。本章ではこのような二値変数を説明変数として用いる方法を紹介する。このような変数を用いた場合、係数の解釈が通常の回帰係数とは異なることに注意が必要である。具体的には、ダミー変数に対応する回帰係数は切片の差、つまり、被説明変数の値に関するグループ間での相対的な高低について示している。 第二に、交差項を用いた回帰モデルを紹介する。次に扱う方法は交差項を用いたモデルである。マーケティング領域ではよく「調整効果」という表現でこのモデルの定式化が用いられる。しかし、研究において誤まった用い方をしているケースも散見されるため、注意が必要である。調整効果という呼称からは、主に着目しているメインの説明変数とその効果を調整する副次的な変数があるかのようなニュアンスが読み取れる。しかしながらモデル化においてそのような扱いの差は存在せず、以下のように定式化される。 \\[ y_i = \\beta_0 + \\beta_1 x_i+\\beta_2z_i+\\beta_3(x_i\\times z_i)+u_i \\] このとき、仮に研究者が \\(x\\) を主たる研究の関心として持っていたとしても、交差項（掛け算の項）で用いる二つの変数についてはどちらも同様に単独項と交差項の両方を含めることが必要となる。なお、このような交差項を用いた定式化では、ダミー変数と連続変数どちらも利用することが可能である。そのうえで本章では、以下の点について注意を促す： 交差項には、条件付き効果に関する作業仮説を論じる必要がある。 例、XがYに与える影響は、Zの値に応じて変化する。 交差項を含むモデルには交差項を構成する二つの変数も含める。 推定モデル上、どちらか一方がメインかのような特定化は行わない。 交差項を構成する二つの変数の係数を従来の回帰分析結果と同じように解釈してはいけない。 説明変数独立項の係数の意味について注意が必要。 第三に、係数比較を意図した定式化を紹介する。マーケティング領域では稀に異なる説明変数のうちどちらの係数のほうが大きいのかを比較するような議論を行う研究が見受けられる。係数の大小関係を統計的な観点から比較するための現実的な方法のひとつは、説明変数の単位を統一した上で信頼区間を計測することである。第二の方法として本章では、大小比較に関する統計的検定を実行するためのモデルの定式化を紹介する。 第四に、マーケティングの枠組みと密接に関わる定式化の方法として、対数線形モデルを紹介する。マーケティングでは、需要の価格弾力性など変数間の非線形の関係を分析したいという目的を持つことも多い。回帰分析では、推定される係数について線形の形で扱うことで、非線形の関係も分析が可能になる。そのような定式化の方法として、説明変数と被説明変数の両方に対して、自然対数（\\(\\ln\\)）を用いた変数変換を行うことで、以下のようなモデルを設計できる。 \\[ \\ln y_i = \\ln \\alpha+\\beta\\ln x_i+u_i \\] このような定式化は一般的に線形対数モデルと呼ばれ、説明変数に対応する回帰係数は「弾力性」を表すことが知られている。より具体的には、このようなモデルの回帰係数は、説明変数が 1% 増加したとき、被説明変数が何%変化するか（変化率）を表していることに注意が必要である。 マーケティングに関する論文や研究レポートでは、これらの発展的な定式化を用いた回帰分析結果を報告することも多い。そのような結果を適切に解釈できるようにするためにも、これらの内容を理解することは重要である。また、これらの手法を実施するためにはその背後にある原則について理解することも重要になる。本章では、これらの内容について、その理屈も含めて紹介していく。 "],["dummy.html", "ダミー変数", " ダミー変数 本節では、説明変数としてカテゴリ変数を用いる場合の方法と、その結果の解釈について説明する。マーケティング領域の研究においては、あるカテゴリに属することが成果変数にどのような影響を与えるのかという問いに関心を持つことも多い。そのような場合には、「ダミー変数」と呼ばれる形にカテゴリ変数を定義し、分析を行う。ダミー変数とは、特定のカテゴリに属するならば1を、それ以外なら0を取るような変数を指す。例えば、女性ならば1、それ以外の性別であれば0を取るようなダミー変数を、「女性ダミー」として扱うことができる。ダミー変数 D を用いた回帰モデルは以下の様に表すことができる。 \\[ y_i=\\alpha+\\beta x_i + \\gamma D_i+u_i \\] ただし、\\(x_i\\) は連続尺度の説明変数である。ダミー変数は取りうる値が1か0に限定されているため、y の条件付期待値は以下のように解釈できる。 \\[ E(y_i|D_i=1)=\\alpha+\\beta x_i + \\gamma \\] \\[ E(y_i|D_i=0)=\\alpha+\\beta x_i \\] つまり、ダミー変数に対応する回帰係数はベースライン（\\(D_i=0\\)）グループとの「切片の差」を表しているということがわかる。例えば、\\(\\small \\alpha\\)、\\(\\small \\beta\\)と、\\(\\small \\gamma\\)が正の値を取るような場合、上記のダミー変数の関係は以下の図のように示す事ができる。例えば、このダミー変数が女性ダミーであるならば、女性はその他の性別に比べて、y の値が相対的に高い、と解釈できる。 ダミー変数のイメージ ダミー変数を説明変数に含む回帰分析をRで実行することはとても簡単である。lm() 関数内のモデル定義において、カテゴリ変数を含めば良いだけである。Characterという文字列情報のデータ型で示されているカテゴリ変数を用いると、自動でダミー変数化して、回帰分析を実行してくれる。 企業データを用いた分析の実行 本章で使っている firmdata19 を使ってダミー変数を用いた回帰分析を行ってみる。具体的には、営業利益率（営業利益/売上）が小売産業に属する企業とそれ以外で異なるかを分析する。しかし、本データセット上には、このような分類に対応するカテゴリー変数は存在しないため変数を作成する必要がある。具体的には、以下のように、“Retail Stores, NEC” もしくは “Supermarket Chains” のどちらかに含まれる企業であれば1、それ以外であれば0を取るカテゴリー変数（format）を作成する。%in% は二つのベクトル間の要素の一致を確認するための演算子で、下記mutateコマンドでは、ind_en 列に書かれている情報が retail リストに「含まれる」か否かをチェックしている。そのため、以下コマンドでは、事前に作成した retail を参照し、ind_en で観察されるカテゴリーが retail に当てはまれば、\"Retailを、そうでなければ、\"Others を返すように指示していることになる。 library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.2 ✔ tibble 3.3.0 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) firmdata19 &lt;- firmdata %&gt;% filter(fyear == 2019) #産業名を特定したオブジェクト&quot;Retail&quot;の作成 retail &lt;- c(&quot;Retail Stores, NEC&quot;, &quot;Supermarket Chains&quot;, &quot;Department Stores&quot;) # Retailを使ったカテゴリー変数の作成 firmdata19 &lt;- firmdata19 %&gt;% mutate(format = ifelse(ind_en %in% retail, &quot;Retail&quot;, &quot;Other&quot;)) #カテゴリーの頻度チェック with(firmdata19, table(format)) ## format ## Other Retail ## 90 56 そして、作成したカテゴリー変数も含めて、以下のような回帰モデルを分析する。 fit.d1 &lt;- lm(op ~ mkexp + format, data = firmdata19) summary(fit.d1) ## ## Call: ## lm(formula = op ~ mkexp + format, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.095926 -0.031427 -0.008661 0.014345 0.261068 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.101694 0.008833 11.514 &lt; 2e-16 *** ## mkexp -0.066003 0.024959 -2.644 0.009097 ** ## formatRetail -0.031770 0.009303 -3.415 0.000831 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05368 on 143 degrees of freedom ## Multiple R-squared: 0.1379, Adjusted R-squared: 0.1258 ## F-statistic: 11.44 on 2 and 143 DF, p-value: 2.469e-05 上記のように、カテゴリー変数（format）を回帰モデルに含めるだけで、自動的にダミー変数化して分析を実行してくれる。今回はたまたま我々の意図通り Othersがベースライングループに設定されているが、これは、指示した結果ではない。  もし確実に特定のカテゴリーを1と定義したい場合には、自身でダミー変数を作成して回帰分析を行えば良い。 firmdata19 &lt;- firmdata19 %&gt;% mutate(retail = ifelse(format == &quot;Retail&quot;, 1, 0)) #確認 with(firmdata19, table(retail, format)) ## format ## retail Other Retail ## 0 90 0 ## 1 0 56 fit.d2 &lt;- lm(op ~ mkexp + retail, data = firmdata19) summary(fit.d2) ## ## Call: ## lm(formula = op ~ mkexp + retail, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.095926 -0.031427 -0.008661 0.014345 0.261068 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.101694 0.008833 11.514 &lt; 2e-16 *** ## mkexp -0.066003 0.024959 -2.644 0.009097 ** ## retail -0.031770 0.009303 -3.415 0.000831 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05368 on 143 degrees of freedom ## Multiple R-squared: 0.1379, Adjusted R-squared: 0.1258 ## F-statistic: 11.44 on 2 and 143 DF, p-value: 2.469e-05 分析の結果、マーケティング支出と小売ダミーのどちらも営業利益率に対して負に有意な影響を持つことがわかった。したがって、小売企業はデータセット内の他の産業よりも利益率が低いといえる。 "],["interaction.html", "交差項", " 交差項 マーケティングに関する研究では、ある説明変数の被説明変数に与える影響が、別の説明変数に影響を受ける形で変化することを捉えることも多い。このような変数間の関係に影響するような効果を調整効果（moderating effect）もしくは相互作用効果（interaction effect）と言い、以下のような図で示されることが多い。 調整効果イメージ図 上の図では、メインの説明変数と調整変数が存在するかのように見えるが、分析に置いてはどちらか一方をメインと扱うことはなく、XとZをかけ合わせた交差項を含めたモデルを分析する。なお、調整効果もしくは相互作用効果の分析ではダミー変数もしくは量的変数のどちらも用いることができる。 交差項イメージ図 回帰モデルにおける交差項の利用には、以下の点に注意が必要である。 交差項には、条件付き効果に関する作業仮説を論じる必要がある。 例、XがYに与える影響は、Zの値に応じて変化する。 交差項を含むモデルには交差項を構成する二つの変数も含める。 推定モデル上、どちらか一方がメインかのような特定化は行わない。 交差項を構成する二つの変数の係数を従来の回帰分析結果と同じように解釈してはいけない。 説明変数独立項の係数の意味について注意が必要。 傾きダミー 先述の注意点について、具体例を見ながら確認していく。まず、量的変数とダミー変数の交差項について考える。ダミー変数と量的変数の交差項を作ることによって、傾きがグループによって変わるか否かを捉えることができる。交差項を含む回帰モデルは以下のように示される。 \\[ y_i = \\beta_0 + \\beta_1 x_i+\\beta_2D_i+\\beta_3(x_i\\times D_i)+u_i \\] \\(\\small x_i\\) が \\(\\small y_i\\) に与える影響を \\(\\small \\Delta y_i/\\Delta x_i\\) とすると、それぞれ以下のように示される。 \\[ \\begin{cases} D_i=1\\Rightarrow &amp; \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3\\\\ D_i=0\\Rightarrow &amp; \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1 \\end{cases} \\] つまり、\\(\\small x\\) が \\(\\small y\\) に与える影響（傾き）が、\\(\\small \\beta_3\\) の分だけ、ダミー変数のカテゴリーによって変化することが伺える。\\(\\small \\beta_1\\) は \\(\\small D=0\\) の際の x の効果であり、 \\(\\small D=1\\) の際の効果は、\\(\\small \\beta_1+\\beta_3\\) で表される。 そのため、このようなダミー変数の使い方を「傾きダミー（Slope dummy）」と呼ぶことも多い。 傾きダミーの実行{slopedummywithR} 先程の firmdara19 にこの分析モデルを適応してみる。Rにおける交差項の導入は、lm(y ~ x * d) のように指定すれば、xとdの交差項とそれぞれの独立項を自動でモデルに含めてくれる。 fit.d3 &lt;- lm(op ~ mkexp * retail, data = firmdata19) summary(fit.d3) ## ## Call: ## lm(formula = op ~ mkexp * retail, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.100914 -0.027907 -0.006023 0.020847 0.254339 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.112780 0.009123 12.362 &lt; 2e-16 *** ## mkexp -0.106801 0.026898 -3.971 0.000113 *** ## retail -0.099242 0.021752 -4.563 1.08e-05 *** ## mkexp:retail 0.205666 0.060393 3.405 0.000859 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0518 on 142 degrees of freedom ## Multiple R-squared: 0.203, Adjusted R-squared: 0.1862 ## F-statistic: 12.06 on 3 and 142 DF, p-value: 4.461e-07 分析の結果、マーケティング支出と小売ダミーは負に有意である一方で、両変数の交差項（`mkexp:retail）は正に有意であることが示された。このとき、mkexp の単独項は、retail が0のとき、つまりその他グループにおけるマーケティング支出と利益率の関係を示しており、それが負に有意であると理解できる。そして、mkexp:retail の項が正に有意であることから、その他グループの傾きと、小売グループの傾きは有意に異なることが伺える。もう少し詳細にこの結果を整理すると、本分析による利益率の予測値 \\(\\hat{y}\\)は以下のように示すことができる。 \\[ \\hat{y}_i = 0.108430 - 0.105662\\text{mkexp}_i - 0.088147\\text{retail}_i + 0.191432\\text{mkexp}_i * \\text{retail}_i \\] \\[ \\begin{cases} \\text{Retail}: &amp;\\hat{y}_i = 0.020283 + 0.08577\\text{mkexp}_i\\\\ \\text{Others}: &amp;\\hat{y}_i = 0.108430 - 0.105662\\text{mkexp}_i \\end{cases} \\] 交差項を用いた分析を実行する場合、分析結果をより詳細に理解するため、結果を図示化することが大切になる。具体的には以下のように、分析結果からカテゴリ別に予測値をそれぞれ計算し、図示化することで、含意を得ることができる。ここでは、sjPlotというパッケージの plot_model() 関数を用いて、分析結果に基づく被説明変数の予測値を図示化する。まず、パッケージをインストールして欲しい。 install.packages(&quot;sjPlot&quot;) 今回の図示化のために、plot_model() 関数内では、type = \"pred\" と予測値を計算するための引数を指定した上で、terms において着目する変数名を特定する。また、この関数では、ggplot2のように、図示化に関する凡例や軸ラベルなどの様々な情報を書き足していくことができる。 library(sjPlot) library(tidyverse) pred &lt;- plot_model(fit.d3, type = &quot;pred&quot;, terms = c(&quot;mkexp&quot;,&quot;retail&quot;), ci.lvl = .95) + labs(title = &quot;Slope Analysis&quot;, subtitle = &quot;(Predicted Values of Profitability with 95% Confdence Intervals)&quot;, x = &quot;Marketing Expense&quot;, y = &quot;Profitability&quot;) + scale_color_discrete(name = &quot;Retail Dummy&quot;) pred 分析の結果、小売企業においてはマーケティング支出と利益率の関係は右上がりである一方で、その他の企業では右下がりである。回帰分析における交差項の係数がこの傾きの違いを示している。また、実線を比較すると マーケティング支出が 0.5 辺りを境に小売企業の利益率の予測値のほうがその他グループよりも高くなっているように見える9。しかしながら、両直線の95%信頼区間は重なっており、統計的に有意な差があるとは言えない。そのため、マーケティング支出が高いとき、小売企業の利益率（の予測値）のほうが有意に高いとは言えない。 量的変数同士の交差項 交差項を用いた分析は量的変数同士にも応用できる。しかしながら、交差項を用いた分析結果の解釈には注意が必要であり、分析の実行においても工夫が求められる。量的変数同士の交差項モデルとして、以下の回帰式を考える。 \\[ y_i = \\beta_0 + \\beta_1 x_i+\\beta_2z_i+\\beta_3(x_i\\times z_i)+u_i \\] 上記における x と z はどちらも量的変数であり、x が y に与えるパーシャル効果は、以下のように表すことができる。 \\[ \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3z_i \\] 同様に、z が y に与えるパーシャル効果は、 \\[ \\frac{\\Delta y_i}{\\Delta z_i}=\\beta_2+\\beta_3x_i \\] となる。したがって、x と z が y に与える影響は、互いに依存しあっていることがわかる。上式の \\(\\small \\beta_3\\) は調整効果や相互作用効果と呼ばれる。 交差項を用いた回帰モデルでは、説明変数の独立項に関する解釈に注意が必要となる。例えば上式の \\(\\small \\beta_1\\)（\\(\\small \\beta_2\\)）はどのような条件で x（z）が y に与える影響だと解釈できるだろうか。例えば、上記のモデルにおける y が体重、 x と z がそれぞれ筋肉量と身長だったとする。このとき、筋肉量が体重に与える影響は、 \\[ \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3z_i \\] であり、\\(\\small \\beta_1\\) は「身長（z）が0」という条件下で筋肉量が体重に与える影響を表している。しかしながら、身長が0のときという非現実的な条件下での結果は、我々にとって解釈が難しく、また情報としても有益でないかもしれない。この問題への対策のひとつが「平均値での中心化（mean-centering）」である。これは、交差項に用いる説明変数に関して、平均からの偏差を用いる方法である。中心化された変数による交差項モデルは以下のように示される。 \\[ y_i = \\beta_0 + \\beta_1 (x_i-\\bar{x})+\\beta_2(z_i-\\bar{z})+\\beta_3(x_i-\\bar{x})\\times (z_i-\\bar{z})+u_i \\] これにより、x の効果は以下のように示される。 \\[ \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3(z_i-\\bar{z}) \\] そのため、中心化されたモデルにおける\\(\\small \\beta_1\\) は、\\(\\small z_i-\\bar{z}=0\\)、つまり「z が平均値」の際の、x 効果だと解釈できる。また、平均値以外の値を用いた中心化も当然可能であるため、研究・実務上関心の強い値（何らかの閾値など）がある場合には、それを基準とした中心化もできる。 交差項モデルの実行 ここで、量的変数を用いた交差項モデルをRで分析する。分析の実行方法は、前節の傾きダミーと同様である。ここでは、“headphone07.csv” という、ある年のヘッドフォン製品の売上を捉えたデータセット（人工架空データ）を用いる。このデータは、以下の変数を含んでいる。 売上（百万円） プロモーション投資額（百万円） R&amp;D投資額（百万円） このデータセットに対して、本分析ではR&amp;Dによって向上する（と仮定する）製品品質とプロモーションの相互作用が売上に与える影響を分析する。具体的には、たとえ良いものを作っても、きちんとその情報を消費者に伝達しないといけないのではないかという課題を捉える。そのため本分析を通じて、製品品質への投資（R&amp;D投資）が売上に与える影響が、プロモーション量に応じて変化するのではないかという研究課題に答える。まず以下のようにデータを読み込み、データフレームを確認する。 Headphone07 &lt;- readr::read_csv(&quot;data/headphone07.csv&quot;, na = &quot;.&quot;) #データフレームの確認 glimpse(Headphone07) ## Rows: 221 ## Columns: 4 ## $ ID &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… ## $ sales &lt;dbl&gt; 118.8377, 548.6312, 197.3075, 104.2657, 748.8251, 947.8850, … ## $ rd &lt;dbl&gt; 404.0893, 252.1270, 444.3374, 407.5876, 841.7605, 336.8744, … ## $ promotion &lt;dbl&gt; 75.63163, 102.74572, 97.98040, 83.46613, 105.69250, 80.17476… このデータに対して、まずは以下の通り中心化していない変数を用いて分析を行う。 fit_int &lt;- lm(sales ~ rd*promotion, data = Headphone07) summary(fit_int) ## ## Call: ## lm(formula = sales ~ rd * promotion, data = Headphone07) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.522 -12.861 -0.638 13.578 70.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.033e+04 1.090e+02 186.5 &lt;2e-16 *** ## rd -5.187e+01 2.844e-01 -182.4 &lt;2e-16 *** ## promotion -1.914e+02 1.052e+00 -181.9 &lt;2e-16 *** ## rd:promotion 4.979e-01 2.730e-03 182.4 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.55 on 217 degrees of freedom ## Multiple R-squared: 0.9935, Adjusted R-squared: 0.9934 ## F-statistic: 1.111e+04 on 3 and 217 DF, p-value: &lt; 2.2e-16 分析の結果、rdとpromotion の交差項は正に有意だが、どちらの独立項も負に有意であった。また、先述の通りこれらの独立項の係数は、もう一方の変数が0のときのそれぞれの効果を表しており、現実的には解釈が難しい結果になっている。したがって、以下の様に中心化変数を作成し、回帰分析を実行する。 Headphone07 &lt;- Headphone07 %&gt;% mutate(promotion_c = promotion - mean(promotion, na.rm = TRUE), rd_c = rd - mean(rd, na.rm = TRUE)) fit_int_c &lt;- lm(sales ~ rd_c*promotion_c , data = Headphone07) summary(fit_int_c) ## ## Call: ## lm(formula = sales ~ rd_c * promotion_c, data = Headphone07) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.522 -12.861 -0.638 13.578 70.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 382.27812 1.51459 252.40 &lt;2e-16 *** ## rd_c -0.91767 0.01196 -76.75 &lt;2e-16 *** ## promotion_c 0.69039 0.03972 17.38 &lt;2e-16 *** ## rd_c:promotion_c 0.49792 0.00273 182.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.55 on 217 degrees of freedom ## Multiple R-squared: 0.9935, Adjusted R-squared: 0.9934 ## F-statistic: 1.111e+04 on 3 and 217 DF, p-value: &lt; 2.2e-16 分析の結果、交差項の結果については非中心化モデルと同じであることが伺える。一方で独立項については、rdは負に、promotionは正に有意であることが明らかになった。つまり、プロモーションが平均値である場合、rdにコストを掛けても売上には繋がらない一方で、rdが平均値の場合、プロモーションによって売上が伸びることが伺える。これをまとめると、品質が平均的ならプロモーションで売上は上がるが、プロモーションが平均的で品質投資をしても売上に逆効果となることが伺える。R&amp;D投資による負の効果については価格との関係もあるかもしれない。品質が向上すると通常価格も上がるため、プロモーションがあまり高くない状態においては、価格の向上によって売上を損ねるかもしれない。この点については、価格も含めたさらなる調査、分析が必要になる。 分析結果の図示化 量的変数同士の交差項分析結果の図示化においては、二種類の方法がある。一つ目は調整変数として捉える変数を高水準（例、平均 \\(+\\) 1標準偏差）と低水準（例、平均 \\(-\\) 1標準偏差）に二分し、前節の傾きダミーの図示化のようにそれぞれの場合でメイン変数による被説明変数への傾きを描画する方法である。これは、前節で使用したsjPlotパッケージで実行できる。 二つ目は、メインの変数が被説明変数へ与える影響を縦軸に、調整変数の値を横軸に取ることによって、メイン変数が持つ効果が調整変数によって変化する様子を連続的に描画する方法である。これは、marginaleffectsというパッケージを用いる。 まず、一つ目の方法は、以下の通り実行することができる。分析の結果、やはりプロモーション水準が低い場合にはR&amp;D投資が売上に与える影響は負であるものの、プロモーション水準が高い場合には正に転じることが伺える。 leg = c(&quot;Mean - 1sd&quot;, &quot;Mean&quot;, &quot;Mean + 1sd&quot;) int_fig1 &lt;- plot_model(fit_int_c, type = &quot;int&quot;, mdrt.values = &quot;meansd&quot;, ci.lvl = .9999999999) + labs(title = &quot;Predicted values of Sales (R&amp;D * Promotion)&quot;, x = &quot;R&amp;D Investment&quot;, y = &quot;Sales&quot;)+ scale_color_discrete(name = &quot;Promotion level&quot;, labels=leg) ## Scale for colour is already present. ## Adding another scale for colour, which will replace the existing scale. int_fig1 続いて、二つ目の方法を実行するために、以下の要領でパッケージをインストールしてほしい。marginaleffects は、margins や emtrends の機能を引き継ぎ、限界効果に関する計算結果を図示化するためのパッケージである10。 install.packages(&quot;marginaleffects&quot;) R&amp;D が 売上に与える影響（傾き）がどのように変化するかは、plot_slopes() という関数を用いる。以下は、傾きの変化に関する図の出力結果である。 library(marginaleffects) int_fig2 &lt;- plot_slopes(fit_int_c, variables = &quot;rd_c&quot;, condition = &quot;promotion_c&quot;, conf_level = .99999999) + labs(title = &quot;Marginal effects of R&amp;D on Sales&quot;, x = &quot;Promotion&quot;, y = &quot;Slope of R&amp;D on Sales&quot;) + geom_hline(aes(yintercept=0), linetype = &quot;dashed&quot;) int_fig2 分析の結果、promotionが0（平均）である条件を基準に、R&amp;Dの傾きが正に転じていることが伺える。なお、よく見ると、直線の周りに灰色の影が描画されていることがわかる。これは、99%信頼区間を示している。今回は人工的に作成したデータなので、非常に当てはまりがよく信頼区間がとても狭くなっているが、実際のデータを用いて同様の図示化をすればもう少し明確に信頼区間を視認できる。このように、交差項を利用した回帰モデルを分析する際は、事後的な図示化を行うことを心がけると良い。この作業により実務的・学術的により有益な含意につながることがある。 以下は、plot_slopes() に関するおまけである。図示化の確認のために、firmdata19を使って、マーケティング支出と総資産との交差項により、利益率を説明する回帰モデルを考える。以下では、中心化と回帰分析の実行、図の出力を実施している（回帰分析結果は省略）。 firmdata19 &lt;- firmdata19 %&gt;% mutate(mkexp_c = mkexp - mean(mkexp), asset_c = total_assets - mean(total_assets)) fit_int2 &lt;- lm(op ~ mkexp_c * asset_c, data = firmdata19) int_fig3 &lt;- plot_slopes(fit_int2, variables = &quot;mkexp_c&quot;, condition = &quot;asset_c&quot;, conf_level = 0.99) + geom_hline(aes(yintercept=0), linetype = &quot;dashed&quot;) int_fig3 図を見ると、Headphoneデータよりもはっきりと信頼区間が視認できる。マーケティング支出の効果は資産によって低下するように見えるが、信頼区間を考慮すると、資産額が平均値（asset_c \\(=0\\)）付近の値を取るときのみマーケティング支出は負に有意の影響を持つが、それ以外の区間では有意でない（信頼区間に0を含む）ことが伺える。このように、信頼区間の出力は結果の解釈を有意義なものにしてくれる。 試しに、plot_model 内の信頼区間に関する引数を ci.lvl = NAとし、実線のみの比較を確認して見て欲しい↩︎ ここでは、詳細は割愛するが以下から詳細を確認できる https://vincentarelbundock.github.io/marginaleffects/dev/↩︎ "],["coefComparison.html", "係数比較", " 係数比較 マーケティング領域においては、異なる説明変数のうちどちらの係数のほうが大きいのかを比較するような議論を行う研究が稀に見られる。しかし、その多くの場合において、（1）係数の推定値をそのまま比較することや、（2） 片方の検定結果が有意でありもう一方は有意でないというような検定結果の比較をもとに大小関係を論じている。しかしながら、たとえ説明変数の単位を（標準化などで）統一していたとしても、これら二つのような比較によって大小関係を結論づけるのは不十分である。係数の大小比較に関する現実的な方法のひとつは、説明変数の単位を統一した上で信頼区間を計測することである。以下では、もうひとつの方法として、大小比較に関する統計的検定を実行するための工夫を紹介する。 まず、以下のような被説明変数を\\(Y\\)、説明変数を \\(X_1,~X_2\\)とする重回帰モデルを考える。 \\[ Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + u \\] 係数の大小比較において重要となるのは、上式内の \\(\\alpha_1\\) と \\(\\alpha_2\\) の差である。つまり、\\(\\alpha_1 - \\alpha_2 = 0\\) であれば両者に差がないことになる。ここで、\\(\\theta = \\alpha_1 - \\alpha_2\\) と定義し、回帰モデルを以下のような修正版モデルに書き換える。 \\[ Y = \\alpha_0 +(\\theta - \\alpha_2) X_1 + \\alpha_2 X_2 + u \\\\ = \\alpha_0 +\\theta X_1 + \\alpha_2 (X_1 + X_2) + u \\] つまり、この式のように \\(X_1\\) と \\((X_1 + X_2)\\) という二つの説明変数を用いた重回帰モデルを推定すると、修正版における\\(X_1\\)の係数は \\(\\theta = \\alpha_1 - \\alpha_2\\) として解釈する事が可能になる。そしてこの修正版モデルにおける \\(X_1\\)の係数を検定することで、\\(\\alpha_1 - \\alpha_2=0\\)を帰無仮説とした検定と同義の結果を得ることができ、大小関係に関する含意を得ることができる。 係数比較の実行 ここでは例として、企業の広告投資とR&amp;Dへの投資が売上に与える影響について比較する。分析にはfirmdata19を用いる。このデータの広告とR&amp;D変数の単位はともに百万円であり揃っているが、本書では教育的意図から変数を標準化するプロセスを提示し、標準化した変数を用いる。そのため、分析結果の係数解釈には注意が必要になる。本書ではまず、以下の通り変数を作成した後、通常の重回帰モデルを実行する。 firmdata19 &lt;- firmdata19 %&gt;% mutate(adv = (adv -mean(adv))/sd(adv), rd = (rd -mean(rd))/sd(rd), ad_rd = adv +rd) fit_linear &lt;- lm(sales ~ adv + rd, data = firmdata19) summary(fit_linear) ## ## Call: ## lm(formula = sales ~ adv + rd, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4357995 -463348 -236824 123663 2692251 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1199403 74285 16.146 &lt;2e-16 *** ## adv 1632326 74996 21.766 &lt;2e-16 *** ## rd 29915 74996 0.399 0.691 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 897600 on 143 degrees of freedom ## Multiple R-squared: 0.7711, Adjusted R-squared: 0.7679 ## F-statistic: 240.8 on 2 and 143 DF, p-value: &lt; 2.2e-16 これを見ると、一見広告投資のほうが係数が大きそうである。では次に、ad+rd を用いた係数比較モデルを分析することでこの差が統計的に有意かを検討する。 fit_comp &lt;- lm(sales ~ adv + ad_rd, data = firmdata19) summary(fit_comp) ## ## Call: ## lm(formula = sales ~ adv + ad_rd, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4357995 -463348 -236824 123663 2692251 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1199403 74285 16.146 &lt;2e-16 *** ## adv 1602411 111739 14.341 &lt;2e-16 *** ## ad_rd 29915 74996 0.399 0.691 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 897600 on 143 degrees of freedom ## Multiple R-squared: 0.7711, Adjusted R-squared: 0.7679 ## F-statistic: 240.8 on 2 and 143 DF, p-value: &lt; 2.2e-16 分析の結果、adv の独立項の係数が有意であったため、二つの係数は有意に異なると理解できる。しかしながら、このような係数の比較を行う際には、その背景にある社会的実務的比較可能性について考慮する必要がある。例えば、特定の産業において、広告への投資もしくはR&amp;Dへの投資をしづらい状況はないだろうか。反対に多くの企業が広告への投資を行っているが、R&amp;Dには投資がされていないという状況は無いだろうか。仮に多くの企業がすでに広告への投資を十分に行っているならば、その中で広告支出額が1単位（1標準偏差）増やすことの意味は非常に大きいはずである。したがって、たとえ分析上変数間の比較が可能であったとしても、その比較がどのような意味をもつのか、もしくはその比較はフェアなものなのかという点については慎重に議論・検討する必要がある。 "],["logLinearModel.html", "対数線形", " 対数線形 ここまでの回帰モデルの特定化では、一次関数を利用していた。しかしながら、以下の図のように傾きの大きさが途中で変化するような非線形の関係には一次関数のモデルは上手くフィットしない。 非線形例 非線形の関係として例えば、以下のような関数形がある。 \\[ Y=AL^\\alpha K^\\beta \\] ここで、Yは生産量、Lは労働投入量、K は資産を仮定すると、上記の式は、経済学で使用され、経営学分野にも応用されるコブダグラス型の生産関数として知られている。しかしながら、このような関数をそのまま線形回帰分析に当てはめることはできない。そこで、右辺と左辺両側の変数に対して、自然対数を用いた変数変換を行うことで、回帰分析で推定可能なモデルを構築する。このような変換された説明変数（例、\\(\\small \\ln K_i\\)）を用いる定式化であれば、推定される係数については線形の形で扱うことができる。対数による変数変換を伴う線形回帰モデルは、一般に対数線形モデルと呼ばれる。 \\[ \\ln Y_i = \\ln A + \\alpha\\ln K_i+\\beta\\ln L_i + u_i \\] 対数線形モデルの分析実行 このような回帰式をR上で構築することは難しくない。lm() 関数内のモデル定式化において、log() を用いれば良い。例えば、上の式を firmdata19 に当てはめて、Yを売上、Lを人件費、K を有形固定資産とおいて、以下のようにモデルを推定してみる11。 fit_prod &lt;- lm(log(sales) ~ log(labor_cost) + log(ppent), data = firmdata19) summary(fit_prod) ## ## Call: ## lm(formula = log(sales) ~ log(labor_cost) + log(ppent), data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.40821 -0.36129 -0.00933 0.38591 1.66673 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.17129 0.31733 9.994 &lt;2e-16 *** ## log(labor_cost) 0.53739 0.03679 14.605 &lt;2e-16 *** ## log(ppent) 0.34588 0.02797 12.366 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5169 on 143 degrees of freedom ## Multiple R-squared: 0.8747, Adjusted R-squared: 0.8729 ## F-statistic: 499.1 on 2 and 143 DF, p-value: &lt; 2.2e-16 このような対数線形モデルでは、これまでのような一次関数モデルとは異なる係数の解釈を行う必要がある。両側対数の回帰モデルにおける回帰係数は、「弾力性」を表している。つまり係数は、説明変数が 1% 増加したとき、被説明変数が何%変化するか（変化率）を表している。また、この係数解釈の特徴から、マーケティング分野においても対数線形モデルはよく用いられる。具体的には、対数線形モデルによって需要の価格弾力性（価格が1%増えたときに、需要量が何%減るか）を分析することができる。 需要の価格弾力性（\\(\\eta\\): イータ）は、消費者による価格感度を捉える指標である。通常、他の要素を一定とした場合、価格が下がると需要量は上がる。では、価格の変化がどの程度需要量に影響を与えるのかという問いは、実務的にも学術的にも重要になる。その際に、単位に依存しない価格感度の尺度として最も広く使われているものが、需要の価格弾力性である。また弾力性は、1を重要な閾値として実務的に重要な含意を与える。弾力性値に応じた含意は以下のように整理することができる。 \\(\\small \\eta= 1\\) の場合: 価格が1%下がった時に需要量が1%上がることを意味する。価格と需要の変化率がちょうど釣り合った状態。 \\(\\small \\eta &gt; 1\\) の場合: 価格の上昇によって需要量が著しく減る（価格に敏感に反応する）。 価格の上昇によって売上を損ねる（需要量の損失が価格上昇の便益を上回っている） 。 \\(\\small \\eta &lt; 1\\) の場合: 価格の上昇によって需要量があまり減らない（価格に敏感に反応しない） 価格の上昇によって売上が伸びる（価格上昇の便益が需要量の損失を上回っている） 。以下では、対数線形モデルと需要の価格弾力性との関係に関する補足的な説明を追加している。 需要の価格弾力性について*{#elasticity} ここでは、需要の価格弾力性についてもう少し詳細な説明を加える。需要の価格弾力性は、需要（q(p)）と価格（p）とし、以下の \\(\\eta\\)のように定義できる。 \\[ \\eta=-\\frac{dq}{dp}\\times \\frac{p}{q} \\] この価格弾力性の定義に基づき、なぜ１が重要な閾値になるのかを説明する。 価格の変化による売上の変化 \\(dpq/dp\\) の関係を変形することで \\(\\small \\eta\\) について以下を得る。 \\[ \\frac{dp\\cdot q}{dp}=q+p\\cdot\\frac{dq}{dp}=q\\left[1-\\left(-\\frac{dq}{dp}\\cdot\\frac{p}{q}\\right)\\right]=q(1-\\eta) \\] そのため、\\(\\small \\eta\\) が１より小さい時、価格の増加による売上の変化が正（\\(dpq/dp&gt;0\\)）であることが伺える。 対数線形の推定と検定 ここで、需要の価格弾力性に関する分析を実行するために、“price_data.csv” という人工データを用いる。このデータはある製品の（架空の）価格と需要量の情報を含んでいる。以下のようにデータを読み込み、分析を実行してほしい。 price &lt;- readr::read_csv(&quot;data/price_data.csv&quot;) fit_q &lt;- lm(log(q) ~ log(p), data = price) summary(fit_q) ## ## Call: ## lm(formula = log(q) ~ log(p), data = price) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.11220 -0.13668 0.02804 0.16166 0.51039 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.47781 0.08135 116.5 &lt;2e-16 *** ## log(p) -0.18008 0.01354 -13.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2309 on 998 degrees of freedom ## Multiple R-squared: 0.1506, Adjusted R-squared: 0.1497 ## F-statistic: 176.9 on 1 and 998 DF, p-value: &lt; 2.2e-16 分析の結果、log(p) の係数は負に有意であった。しかしここで、デフォルトで出力される検定結果の「帰無仮説」を思い出して欲しい。これは、係数が0か否かの検定を行っている。しかしながら、需要の価格弾力性について議論する際に我々は、係数が 1 より高いか否かに興味がある。そのため、\\(\\small H_0:\\beta = 1\\)という帰無仮説による検定を採用すべきである。このような検定の実行には、色々とやり方はあるが、本書では carというパッケージのlinearHypothesis() 関数 を用いる方法を紹介する。まず、以下のようにパッケージをインストールして欲しい。 install.packages(&quot;car&quot;) そして、linearHypothesis()において、参照する分析結果と、着目する変数（の係数）とその値について入力することで、検定を行う。 library(car) linearHypothesis(fit_q, c(&quot;log(p) = -1&quot;)) ## ## Linear hypothesis test: ## log(p) = - 1 ## ## Model 1: restricted model ## Model 2: log(q) ~ log(p) ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 999 248.811 ## 2 998 53.225 1 195.59 3667.3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 検定の結果、fit.q による係数の推定値は、有意に -1 とは異なることがわかった。つまり、弾力性は 1 より低く、非弾力的である（価格にあまり敏感でない）ことが伺えた。なお、一つの説明変数の有意性検定にF検定を用いる場合のF分布は対応するt分布の二乗値に等しい。そのため、car による検定では、F値を返しているが、それ自体に問題はない。 最後に、回帰モデルにおいて対数化を右辺（説明変数）と左辺（被説明変数）のどちらに適用するかによって、係数の解釈が変わることを説明する。モデルの特定化と係数解釈の対応関係は、以下のように整理できる。 両側対数（\\(\\ln y_i = \\alpha + \\beta\\ln x_i + u_i\\)）: \\(\\beta\\) は x が 1%変化したときの y の変化率を表している。 被説明変数のみ対数（\\(\\ln y_i = \\alpha + \\beta x_i + u_i\\)）: \\(\\beta\\) は x が 1単位変化したときの y の変化率を表している。 説明変数のみ対数（\\(y_i = \\alpha + \\beta\\ln x_i + u_i\\)）: \\(\\beta\\) は x が 1%変化したときの y の変化量を表している。 なお、実際に研究において生産関数を推定する場合には、変数の特定化に関して、先行研究に基づきより慎重に検討する必要があるため注意して欲しい↩︎ "],["ch6Reference.html", "参考文献", " 参考文献 秋山裕（2018）「Rによる計量経済学 第2版」，オーム社. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Arel-Bundock V (2023). marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests. R package version 0.13.0.9002, https://vincentarelbundock.github.io/marginaleffects/. Wooldridge, J. (2013) Introductory Econometrics A Modern Approach,Cengage Learning. "],["choice.html", "消費者の選択と離散選択モデル ", " 消費者の選択と離散選択モデル "],["Ch7Intro.html", "本章の概要", " 本章の概要 マーケティング施策の導入したり、戦術要素（例えば、4Ps）の変更はマーケティング意思決定において重要な問題である。その際、どのようにしてその変更によってどの程度消費者の需要が変化を分析、予測すればよいのだろうか？マーケティングリサーチでは伝統的に、「製品を購買したか否か」や、複数ある製品の候補から「どのブランドを選んだか」というような観察可能な選択行動についてのデータを収集、活用することで、上記の問いに答えてきた。 消費者の選択行動は例えば、ある製品を購買していれば 1 を、していなければ 0 を取るような離散データとして扱われる。マーケティングリサーチでは、このようなデータに対して「購入をした人は何%か？」というような円グラフを描くだけではなく、消費者の選択行動を被説明変数、マーケティング戦術要素などを説明変数とする分析モデルを構築し分析する。このような分析モデルは離散選択モデルと呼ばれ、マーケティングのみならず、交通や都市計画分野などにおいても頻繁に用いられている。 Rでは、プロビットモデルやロジットモデルを実行するための関数を用いてこれらの分析が可能である。しかしながら、分析の実行が容易であることに反して、これらの分析を適切に実行し、結果を解釈するためにはモデルの理論的前提についても注意が必要である。そのため、理論的な説明については別途テキストを参照してほしい。ここでは、分析の実行に着目し、主に以下の3つの内容を扱う。 第一に、離散変数を被説明変数とする回帰モデルを最小二乗法で推定する場合について説明する。このような方法は線形確率モデルと呼ばれ、変数間の関係を検証することに着目する際には意図的に採用されることもある。線形確率モデルによって推定された係数は、説明変数が変化したときに被説明変数（ダミー変数）が1を取る確率（反応確率）が、どのように変化するかを示す。しかしながら線形確率モデルの最も大きな問題として、推定結果に基づく予測値が論理的整合性を満たさないというものがある。具体的には、線形確率モデルによって推定された予測値は反応確率を示しているはずであるにも関わらず、予測値が1を上回ったり、負の値を取ることがある。そのため、予測を重視する研究を行う場合や、効果の程度や推定値についての議論が重要である場合には線形確率モデルは適さない。 第二に、線形確率モデルではない推定方法としてプロビットモデルとロジットモデルを紹介する。本章では主にプロビットモデルを中心に説明を行っているが、どちらのアプローチでも非線形の（回帰直線ではなく、回帰曲線を引く）分析を行うことで、推定結果に基づいて計算される反応確率が 0 から 1 の範囲を越えないように調整される。この統計的なモデルの定式化について説明する。その後、潜在変数アプローチと呼ばれる、観察可能な離散変数の背後に観察できない連続的な変数が存在するという視点に基づく定式化を紹介する。マーケティングにおける研究群では、多様な背景を持った研究者が論文を書いているため、同じ離散選択モデルを用いた論文でもその定式化の説明方法が異なる場合がある。その際の手助けになるように、本章では上記の二つの異なる定式化アプローチを紹介する。その後、最尤法を用いてプロビットモデルを推定する方法を紹介する。ここではRでの分析実行方法と結果の解釈についても組み合わせて説明する。 第三にに離散選択モデルの消費者理論への応用について説明する。具体的には効用最大化という理論枠組みから効用関数の構造についてモデル化し、それを分析する手順や考え方について説明する。効用の大小関係に基づきプロビットやロジットモデルといった離散選択モデルを定式化する考え方自体は照井・佐藤（2022）などのマーケティングリサーチのテキストでも紹介されているが、これらの議論を巡る理論的な理解については別途テキストを参照してほしい。 本章後半には、選択肢が三つ以上の場合の選択モデルとして、多項ロジットモデルの概要とRを用いた分析方法も追加的にを紹介している。また、最後には離散選択モデルの推定から実務的含意を得るための具体例も紹介しているため、こちらも合わせて確認してほしい。 "],["discretechoice.html", "離散選択モデル", " 離散選択モデル 線形確率モデル 選択肢の持つ属性によって消費者の選択行動がどのように変化するかを、統計的に分析・説明するためには、被説明変数に選択結果をとり、説明変数に選択肢属性を取る以下のような回帰モデルを考える。なお、簡単化のために単回帰モデルで本アプローチを紹介しているが、以下の議論は任意の \\(k\\) 個の説明変数を含む重回帰モデルに応用することができる。 \\[ y_i=\\beta_0+\\beta_1x_i+e_i, \\] ただし、\\(y\\) は特定の製品を購入したか（\\(y_i=1\\)）、購入していないか（\\(y_i=0\\)）を表しており、\\(x_i\\) は価格を表しているとする。分析においては \\(y_i\\) と \\(x_i\\)についての情報を含むデータセットを用いて、パラメータ \\(\\beta_0\\) と \\(\\beta_1\\) を推定する。通常、（他の要素が一定である場合）価格が上がると製品を購入（選択）する確率は下がると考えられるので、\\(\\beta_1&lt;0\\) が予測される。このように消費者の選択を捉えた分析においては、被説明変数がダミー変数である回帰モデルを考える必要がある。しかしながら、ダミー変数のような離散変数を被説明変数に用いる場合には注意も必要である。 被説明変数がダミー変数であるOLS回帰モデルは線形確率モデル（Linear Probability Model: LPM）と呼ばれる。ただし、\\(y\\) が2値しか取らないため、\\(\\beta_1\\) は、「\\(x_i\\) が1単位変化した際の \\(y\\) の変化」として解釈することはできない。 ここで、誤差項の条件付き期待値について 0 であるという仮定（\\(E(e_i|x_i)=0\\)）に基づくと、\\(y_i\\) の条件付き期待値について以下を得る。 \\[ E(y_i|x_i)=\\beta_0+\\beta_1x_i \\] ここで、\\(y\\) は 0 か 1 を取るダミー変数なので、\\(P(y_i = 1|x_i)= E(y_i|x_i)\\) と示すことができる（Wooldridge, 2012）。したがってLPMでは、以下のように線形モデル化したものだと理解できる。 \\[ P(y_i = 1|x_i)=\\beta_0+\\beta_1x_i \\] このとき、\\(P(y_i = 1|x_i)\\) は反応確率（response probability）や成功確率（probability of success）と呼ばれる。また、確率の合計は 1 になるため、\\(P(y_i = 0|x_i)=1-P(y_i = 1|x_i)\\) もまた \\(x_i\\) に関する線形の関数になる。LPMにおいて定数項（\\(\\beta_0\\)）は \\(x\\) が0のときの反応確率を表しており、傾きの係数 \\(\\beta_1\\) は \\(x_i\\) の変化に伴う反応確率の変化を表していると解釈できる。より具体的には、\\(\\beta_1\\) は以下のように表現できる。 \\[ \\Delta P(y_i = 1|x_i)=\\Delta\\beta_1x_i \\] このことから、\\(y\\) の予測値（\\(\\hat{y_i} = \\hat{\\beta}_0+\\hat{\\beta}_1x_i\\)）も線形回帰モデルと同様に示すことができる。したがって、係数の解釈について注意が必要ではあるものの、LPMによる推定結果も線形回帰モデル同様の含意を提供する。特に、複数の説明変数を採用することで、他の変数を固定したうえで説明変数の変化に伴う反応確率の変化を捉えることができる。そのため、LPMによって着目する変数が反応確率へ与える影響を検証することができる。 しかしながら、LPMの推定結果には注意も必要である。具体的には、以下の2点について問題が生じる。 誤差項の分散が不均一になる。 予測値が論理的整合性を満たさなくなる。 第一の問題点については、LPMで推定してしまうことで、OLS推定量が好ましい性質を持つための仮定（均一分散）を満たさないことにつながる。分散が均一であるとは、誤差項の分散がどの観測個体 \\(i\\) に対しても同じ大きさであることを指す。そのため、特定の主体だけ誤差項の分散が大きい場合や、\\(x\\) の値の変化に伴って誤差項の分散が大きくなるような状態ではこの仮定は満たされず、分散不均一であると言われる。 ここで、上記のモデルの誤差項は \\(e_i= y_i - (\\beta_0+\\beta_1x_i)\\) である。ここで、\\(y_i\\) の条件付き期待値を \\(P_i=E(y_i|x_i)=P(y_i = 1|x_i)=\\beta_0+\\beta_1x_i\\) と定義すると、誤差項の分散は \\(Var(e_i|x_i)=P_i(1-P_i)\\) となることが知られている（西山ほか,2019）。\\(P_i\\) の大きさはその定義より各主体によって \\(x_i\\) に依存する形で変化することが伺える。そのため、LPMによる推定では、分散不均一の問題が生じるといえる。しかしながらこの問題は通常、分散不均一に対して頑健な標準誤差（例えば、ホワイトの標準誤差）を用いた分析を用いることで対応される。 第二に予測値の論理的整合性について説明する。LPM推定によって得た \\(y\\) の予測値は、反応確率の予測値を表すため、 \\(y\\) の予測値は 0 から 1 の間に収まらないといけない。しかし、LPMでは予測値が負の値を取ったり、1 を上回ることもある。言い換えると確率の定義に反するような、論理的に整合的ではない予測値を返してしまう。ここで、以下のような簡単な人工データを用いて、LPMによる分析を実行してみる。下記df1の成果変数（Y）はダミー変数であり、説明変数（X）は連続変数だとする。これを線形モデルで回帰し、予測値を出力してみる。すると、1つ目と4つ目の観測は負の値、7つ目と8つ目の観測は1を越える予測値を得たことが伺える。 df1 &lt;- data.frame(Y = c(0, 0, 0, 0, 0, 1, 1, 1, 1), X = c(3.4, 5.22, 7.06, 2.81, 4.11, 10.34, 13.67, 15.99, 9.09)) lpm1 &lt;- lm(Y ~ X, data = df1) pred_lpm1 &lt;- predict(lpm1) pred_lpm1 ## 1 2 3 4 5 6 ## -0.006342894 0.173357681 0.355032986 -0.064597476 0.063760077 0.678888967 ## 7 8 9 ## 1.007681776 1.236750640 0.555468243 図 2 は、このデータのプロットとLPMで推定した際の回帰直線との関係の例を示している。この結果からも、いくつかの観測主体によって確率の範囲を越えた予測値を得ていることが伺える。 Figure 2: LPM と回帰直線 そのため、予測を重視する研究を行う場合には、LPMは適さないことが多い。その一方で変数間の関係を検証することを目的とする場合には、大きな問題にはならないとする主張もある（西山ほか, 2019）。言い換えると、ある説明変数が選択確率に与える影響を統計的に検証するという目的のもとでは、LPMでも対応可能である。操作変数法や固定効果推定といった発展的な手法が線形モデルでは開発されており、LPMではこれらの手法を応用することができることもこの考え方に影響を与えている。 プロビットモデルとロジットモデル*{#logitprobit} 任意の \\(k\\) 個の説明変数を用いたLPMの反応確率は、 \\[ P_i=E(y_i|x_{1i},...,x_{ki})=P(y_i = 1|x_{1i},...,x_{ki})=\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki} \\] となる。このモデルを分析するうえで線形確率モデル（LPM）が有する問題のひとつは、この確率が 0 から 1 の範囲を越えてしまうことにあった。この問題は、\\(y\\)（選択）と \\(x\\)（説明変数）との関係を線形で捉えることが原因となっている。そのため、0 から 1 の範囲を超えないように、何らかの累積分布関数を用いて非線形で分析を行う（回帰直線ではなく、回帰曲線を引く）ことで、この問題を克服する事ができる。 非線形なモデルとして分析する場合、回帰モデルによる反応確率は以下のように示すことができる。 \\[ P(y_i = 1|x_{1i},...,x_{ki})=F\\left(\\beta_0+\\beta_1x_i+...+\\beta_kx_{ki}\\right) \\] ただし、\\(F(\\cdot)\\)は選択（反応確率）と説明変数を非線形な形で結びつけるためのなんらかの関数である。ここで採用する\\(F(\\cdot)\\)によって回帰曲線の形状が決まる。 例えば、図 3は図2との対比として（説明変数が一つのモデルを）非線形でのモデル化を示したものである。図 3のように、非線形の近似曲線で回帰分析を行うことで、確率を意味する予測値が0から1の範囲に収まる。 Figure 3: 非線形での近似イメージ このときに用いる関数形として一般的なものが、標準正規分布の累積分布関数である。累積分布関数は、確率密度関数を積分していくことで得ることができ、0 を下限、1 を上限とする分布である。 標準正規分布の累積分布関数を\\(\\Phi\\)（ファイ）で示すと、反応確率は以下のように表すことができる12。 \\[ P(y_i=1|x_{1i},...,x_{ki})=\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})=\\int^{\\beta_0+\\sum^k_{j =1}\\beta_jx_{ji}}_{-\\infty}\\phi(z)~dz \\] 一方で、\\(P(y_i=0|x_{1i},...,x_{ki})\\)については、以下が成り立つ。 \\[ P(y_i=0|x_{1i},...,x_{ki})=1-P(y_i=1|x_{1i},...,x_{ki})=1-\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\] これにより、選択肢を選ぶことへの条件付き確率と選ばないことへの条件付き確率を、回帰モデルと標準正規分布の累積分布関数との関係から定義する事ができた。これを \\(y_i=1\\) か \\(y_i=0\\) という値を取る事象に対応させるため、ベルヌーイ確率関数13に代入すると、以下のような確率密度関数を考えることができる。 \\[\\begin{equation} P(y_i|x_{1i},...,x_{ki})=\\\\ \\Big(\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\Big)^{y_i}\\times \\Big(1-\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\Big)^{1-y_i} \\tag{1} \\end{equation}\\] このように、選択行動に対して標準正規分布の累積分布関数を仮定して定式化するモデルを「プロビットモデル」と呼ぶ。また、選択モデルでは標準正規分布ではなく、ロジスティック分布の累積分布関数を用いた定式化を行う「ロジットモデル」を用いることも多い。ロジットモデルでは、ロジスティック分布の累積分布関数（\\(\\Lambda(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})\\)）を用いて、以下のような密度関数を用いる。 \\[ P(y_i|x_{1i},...,x_{ki}) =\\Lambda(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})=\\frac{\\exp(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})}{1+\\exp(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})} \\] ロジットモデルでは、回帰モデル（\\(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}\\)）の値が大きくなると反応確率が 1 に近づき、小さくなると 0 に近づくという性質を持っており、取りうる区間も 0 から 1 の間に限定されている。この点において、ロジットモデルはプロビットモデルと同様の特徴を持っている。そのうえでロジットモデルは分析における数値計算がプロビットモデルよりも容易であり、これまで広く使われてきたという経緯がある。コンピュータの性能が高まった近年ではプロビットモデルの使用が増えてきたものの、過去の研究との比較やこれまでの研究蓄積、慣習と言った側面を重視し、引き続きロジットモデルが使われることも多い。 潜在変数アプローチによる説明*{#latentApproach} これまでは、「選択」という点に着目し、ベルヌーイ確率関数を用いてプロビットモデルを導出する方法を紹介した。一方で、選択という離散的な変数の背後に、観察できない連続的な変数（潜在変数: latent variable）が存在するという視点からモデル化を説明することも可能である。このような考え方を潜在変数アプローチと呼ぶ。マーケティング領域の論文やテキストでは、この潜在変数アプローチに基づく選択モデルの記述や紹介も行われるため、本書では、先述のプロビットモデルと潜在変数アプローチにより導出されたモデルが一致することを示す。 ここで、\\(y_i^*\\) という（観察可能な説明変数とは異なる）連続的な潜在変数を考え、以下のような回帰モデルを考える。 \\[ y_i^*=\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}+e_i \\] ただし、この誤差項 \\(e_i\\) は標準正規分布に従い、\\(e_i|x_{1i},...,x_{ki}~\\sim N(0,1)\\)を満たすと考える。 この時、\\(y_i^*\\) がある閾値を越えたならば（観察可能な）被説明変数は 1 をとり、越えない場合は被説明変数が 0 をとる、というような潜在変数と被説明変数との関係を考える。プロビットモデルでは具体的に、以下のような関係を仮定する。 \\[ \\begin{aligned} y_i= \\left\\{ \\begin{array}{ll} 0 &amp; ~\\text{if}~~y_i^*\\leq0 \\\\ 1 &amp; ~\\text{if}~~y_i^*&gt;0 \\end{array} \\right. \\end{aligned} \\] そのため、\\(P(y_i=1|x_{1i},...,x_{ki})=P(y_i^*&gt;0|x_{1i},...,x_{ki})\\)や、\\(P(y_i=0|x_{1i},...,x_{ki})=P(y_i^*\\leq0|x_{1i},...,x_{ki})\\) と示せることがわかる。この性質を利用して、選択しないことへの条件付き確率 \\(P(y_i=0|x_{1i},...,x_{ki})\\) は以下のように標準正規分布の累積分布関数を用いて表現できる（西山ほか, 2019, p.339）。 \\[ \\begin{align} P(y_i=0|x_{1i},...,x_{ki})&amp;=P(y_i^*\\leq0|x_{1i},...,x_{ki})\\\\ &amp;= P(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}+e_i\\leq0|x_{1i},...,x_{ki})\\\\ &amp;=P(e_i\\leq -\\beta_0-\\beta_1x_{1i}-...-\\beta_kx_{ki}|x_{1i},...,x_{ki})\\\\ &amp;=\\Phi(-\\beta_0-\\beta_1x_{1i}-...-\\beta_kx_{ki}) \\end{align} \\] 標準正規分布はその性質より、0を中心として左右対称な確率密度関数を持つ。そのため、\\(e_i\\) が \\((-\\beta_0-\\beta_1x_{1i}-...-\\beta_kx_{ki})\\) 以下の値を取る確率と、\\((\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})\\) 以上の値を取る確率は等しくなるため、以下を得る。 \\[ P(y_i=0|x_{1i},...,x_{ki})=1-\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\] また、\\(P(y_i=1|x_{1i},...,x_{ki})=1-P(y_i=0|x_{1i},...,x_{ki})\\) であるため、反応確率は以下のように示すことができる。 \\[ P(y_i=1|x_{1i},...,x_{ki})=\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\] したがって、潜在変数アプローチでも通常のプロビットモデルと一致するモデルを得ることができた。 ただし、\\(\\Phi&#39;(z)=\\phi(z)=\\frac{1}{\\sqrt{2\\pi}}\\exp(\\frac{-z^2}{2})\\)とする。↩︎ \\(f(y_i)=P_i^{y_i}\\times\\big(1-P_i)^{1-y_i}\\)↩︎ "],["probitwithR.html", "プロビットモデルの推定と解釈", " プロビットモデルの推定と解釈 分析の実行と結果のまとめ プロビットモデルを構築できたら、次はモデル内のパラメータに関する推定値（\\(\\hat{\\beta}_0, \\hat{\\beta}_1,...,\\hat{\\beta}_k\\)）を求め、選択確率に関する含意を得る。プロビットモデルのパラメータ推定においては、最小二乗法ではなく、最尤（Maximum Likelihood: ML）法を用いるが、詳細については補足（??）を参照してほしい。 プロビットモデルを推定する場合のデータセットには、説明変数に関する列（\\(x_{1i},.., x_{ki}\\)）と、それに対応する個人 \\(i\\) の選択結果 \\(y_i\\)（選択していれば1、選択していなければ 0）が記録されている。choice_data.xlsx は、二つの製品に対する消費者の選択結果を捉えた人工データセットである。データには製品1の価格 \\(p1\\)、製品2の価格 \\(p2\\)（千円）、製品1のクーポン広告を受け取ったかのダミー変数 \\(a1\\)、製品2のクーポン広告ダミー変数 \\(a2\\) と、製品1の選択結果 \\(y1\\)、製品2の選択結果 \\(y2\\) が含まれている。 choice_df &lt;- readxl::read_xlsx(&quot;data/choice_data.xlsx&quot;) head(choice_df) ## # A tibble: 6 × 6 ## y1 y2 p1 p2 a1 a2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 10.7 16.8 0 1 ## 2 0 1 11.6 9.15 1 0 ## 3 1 0 8.97 9.31 1 0 ## 4 1 0 4.62 8.03 0 0 ## 5 1 0 7.81 19.2 1 1 ## 6 1 0 7.49 8.84 0 0 このchoice_data.xlsxを用いて製品1の選択に関するプロビットモデルを推定してみる。Rではglm()関数において、family = binomial(link = probit) という引数を指定することで実行が可能である14。以下では分析の実行と結果の出力を行う。 probit1 &lt;- glm(y1 ~ p1 + a1, family = binomial(link = probit), data = choice_df) summary(probit1) ## ## Call: ## glm(formula = y1 ~ p1 + a1, family = binomial(link = probit), ## data = choice_df) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.04117 0.22350 9.133 &lt; 2e-16 *** ## p1 -0.24720 0.02213 -11.171 &lt; 2e-16 *** ## a1 0.62763 0.08656 7.251 4.13e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1384.5 on 999 degrees of freedom ## Residual deviance: 1205.5 on 997 degrees of freedom ## AIC: 1211.5 ## ## Number of Fisher Scoring iterations: 3 ここまで、我々は線形確率、プロビット、ロジットという3種類のモデルでの推定方法を概観してきた。以下では、これらの異なる推定方法を用いた分析結果を併記する。 library(modelsummary) ols1 &lt;- lm(y1 ~ p1 + a1, data = choice_df) logit1 &lt;- glm(y1 ~ p1 + a1, family = binomial(link = logit), data = choice_df) models &lt;- list() models[[&quot;Linear probability model&quot;]] &lt;- ols1 models[[&quot;Probit model&quot;]] &lt;- probit1 models[[&quot;Logit model&quot;]] &lt;- logit1 modelsummary(models, title = &quot;モデル比較&quot;, notes = &quot;Values in [ ] show robust standard errors&quot;, stars = TRUE, statistic = &quot;std.error&quot;, vcov = &quot;robust&quot;, gof_map = &quot;nobs&quot;) #適合度指標において、サンプルサイズ（&quot;nobs&quot;）のみ表示するという指示 /* tinytable css entries after */ .table td.tinytable_css_tj85cuepflj3e4wpb130, .table th.tinytable_css_tj85cuepflj3e4wpb130 { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_izqd5l5620pvep1kz51p, .table th.tinytable_css_izqd5l5620pvep1kz51p { text-align: center; border-bottom: solid black 0.05em; } .table td.tinytable_css_k0le9pcpaiqdgnv8witb, .table th.tinytable_css_k0le9pcpaiqdgnv8witb { text-align: center; } .table td.tinytable_css_jqhrqjkjpxiz71ifhzpt, .table th.tinytable_css_jqhrqjkjpxiz71ifhzpt { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_78kqby0lbvpm1jil2zqy, .table th.tinytable_css_78kqby0lbvpm1jil2zqy { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_9m97w0wfum5edyyrgadx, .table th.tinytable_css_9m97w0wfum5edyyrgadx { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_mpwuxgrf7hviu6mig11r, .table th.tinytable_css_mpwuxgrf7hviu6mig11r { text-align: left; } .table td.tinytable_css_g17hdk462kclif48qmcr, .table th.tinytable_css_g17hdk462kclif48qmcr { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } モデル比較 Linear probability model Probit model Logit model + p Values in [ ] show robust standard errors (Intercept) 1.196*** 2.041*** 3.332*** (0.065) (0.212) (0.361) p1 -0.085*** -0.247*** -0.404*** (0.006) (0.021) (0.036) a1 0.221*** 0.628*** 1.035*** (0.029) (0.087) (0.145) Num.Obs. 1000 1000 1000 分析の結果、推定値の違いはあるものの、どのアプローチでも p1 は被説明変数に対して負に、a1 は正に有意な影響を与えることが示されている。しかしながら、詳しくは次項にて説明するが、係数の解釈については注意が必要である。前章で紹介した modelsummary パッケージを使えば、複数の分析アプローチを用いた結果をまとめ、併記することも簡単である。結果の頑健性チェックや、様々な比較検討のために、このような表が用いられることも多い。 限界効果の計算 プロビットモデルの分析には成功したが、非線形モデルで推定された係数の解釈には注意が必要である。特に、プロビットモデルによって推定されたある変数の係数値は、「他の変数の影響をコントロールした場合にその変数が選択確率に与える影響」を意味しない。そのため、OLSの分析結果のように、係数の推定値のみを見て変数の影響の程度を議論することはできず、そのような解釈を行うためには特定の変数が持つ「限界効果」を追加的に分析する必要がある。限界効果の必要性や、平均的な限界効果、個別限界効果の平均値については次節で説明を加えている。 限界効果の計算には、mfx::probitmfx() を用いる15ため、以下のようにパッケージをインストールしてほしい。 install.packages(&quot;mfx&quot;) mfx パッケージを用いた限界効果の計算では、線形モデルやプロビットモデルの推定同様、以下のようにモデルを指定し、分析を行う。平均的な限界効果の計算方法については、atmean という引数で指定する。atmean = FALSE とすることで、個別限界効果の平均値を計算できる。個別限界効果の平均値とは、データ内の個人の個別限界効果を計算し、その平均値求める方法である。一方で、平均値における限界効果はatmean = TRUE と指示することで計算できる。平均値における限界効果は、モデル内の説明変数の値について、それぞれ平均値を計算し、そのうえで限界効果を求める方法である。 library(mfx) #Average Marginal Effects (限界効果の平均) probit1_ame &lt;- probitmfx(y1 ~ p1 + a1, data = choice_df, atmean = FALSE) probit1_ame ## Call: ## probitmfx(formula = y1 ~ p1 + a1, data = choice_df, atmean = FALSE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## p1 -0.0847786 0.0060787 -13.9469 &lt; 2.2e-16 *** ## a1 0.2188866 0.0289816 7.5526 4.266e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;a1&quot; #Marginal Effects at Mean（平均値における限界効果） probit1_mem &lt;- probitmfx(y1 ~ p1 + a1, data = choice_df, atmean = TRUE) probit1_mem ## Call: ## probitmfx(formula = y1 ~ p1 + a1, data = choice_df, atmean = TRUE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## p1 -0.0984235 0.0088074 -11.1750 &lt; 2.2e-16 *** ## a1 0.2446609 0.0324366 7.5427 4.602e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;a1&quot; 分析結果を比較すると、多少計算結果は異なるものの、どちらの計算方法においても、価格は製品の選択確率に負に、クーポンは正に有意な影響を与えることが伺える。また影響の程度として、限界効果の平均値に着目すると、価格が1単位（千円）上昇すると、選択確率が約\\(8.5\\%\\)下がること、クーポンを受け取った人は受け取っていない人と比べて約\\(21.9\\%\\)選択確率が高いことが伺える。なお、詳細については省略するが限界効果の標準誤差はデルタ法と呼ばれる方法で計算されている（Fernihough, 2019）。 また、mfx による結果も以下のように整理して出力する事ができる。下表の(1) は限界効果の平均（Average Mean Effect）、(2) は平均的個人における限界効果（Marginal Effect at Mean）をそれぞれあらわしている。 marginal &lt;- c( &quot;p1 marginal&quot; = &quot;p1&quot;, &quot;a1 marginal&quot; = &quot;a1&quot; ) marg_model &lt;- list( &quot;(1) Probit_AME&quot; &lt;- probit1_ame, &quot;(2) Probit_MEM&quot; &lt;- probit1_mem ) marg_sum &lt;- modelsummary( # models to summarize side-by-side marg_model, # S.E. in parentheses statistic = &quot;std.error&quot;, # rename and select the coefficients coef_map = marginal, # significance stars stars = TRUE, # term and component columns are combined shape = term:component ~ model, add_rows = data.frame(&quot;Marginal effect type&quot;, &quot;Average Marginal Effect&quot;, &quot;Marginal Effect at Mean&quot;), title = &quot;限界効果サマリー&quot;, # omit all goodness-of-fit statisitcs except # of observations gof_map = &quot;nobs&quot;) marg_sum /* tinytable css entries after */ .table td.tinytable_css_yelne0lcir1sutp3bbno, .table th.tinytable_css_yelne0lcir1sutp3bbno { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_yokglx63b0x6ls3phg1e, .table th.tinytable_css_yokglx63b0x6ls3phg1e { text-align: center; border-bottom: solid black 0.05em; } .table td.tinytable_css_uvth36cm3k07kvbq3o1r, .table th.tinytable_css_uvth36cm3k07kvbq3o1r { text-align: center; } .table td.tinytable_css_n9jvwxp7k94v053idglp, .table th.tinytable_css_n9jvwxp7k94v053idglp { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_6q7mbmydjmondiukvelc, .table th.tinytable_css_6q7mbmydjmondiukvelc { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_u0s0ag5tevlf6xxpw152, .table th.tinytable_css_u0s0ag5tevlf6xxpw152 { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_kij97j4dlfkjejq7245i, .table th.tinytable_css_kij97j4dlfkjejq7245i { text-align: left; } .table td.tinytable_css_s44182h41acyy5ngfike, .table th.tinytable_css_s44182h41acyy5ngfike { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; } 限界効果サマリー (1) (2) + p p1 -0.085*** -0.098*** (0.006) (0.009) a1 0.219*** 0.245*** (0.029) (0.032) Num.Obs. 1000 1000 Marginal effect type Average Marginal Effect Marginal Effect at Mean 最後に、プロビットモデルのモデル評価指標について紹介する。プロビットモデルを最尤法で推定した場合、OLSにおける決定係数 \\(R^2\\) を用いてモデルの当てはまりの良さを用いることはできない。そこで、通常、疑似決定係数（Pseudo \\(R^2\\)）を用いる事が多い16。疑似決定係数は0から1の間の値を取り、自身の立てたモデルの当てはまりが良いほど1に近づくという特徴を持っている。Rにおいては、DescTools::PseudoR2() を用いて計算可能である。install.packages(\"DescTools\") でインストールしてほしい。 DescTools::PseudoR2(probit1) ## McFadden ## 0.1293345 限界効果がなぜ必要か？*{#whymarginal} ここでは、プロビットモデルによって得た係数の推定値を直接的に解釈できない理由と限界効果についての詳細を説明する。パラメータの推定結果と、\\(x_{1i},...,x_{ki}\\) の値を得たときに \\(y_i\\) が 1 を取る確率は以下の様に示される。 \\[ \\hat{P}(y_i=1|x_{1i},...,x_{ki})=\\Phi(\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+...+\\hat{\\beta}_kx_{ki}) \\] ここで、説明変数 \\(x_{1i}\\) の限界効果は、\\(x_{1i}\\) が変化したときに反応確率がどのように変化するのかを表す。これは偏微分という計算方法を用いて以下の様に示すことができる。なお、以下で示す内容は \\(x_{1i}\\) 以外の任意の説明変数を用いても成り立つ。 \\[ \\frac{\\partial \\hat{P}(y_i=1|x_{1i},...,x_{ki})}{\\partial x_{1i}}=\\phi(\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+...+\\hat{\\beta}_kx_{ki})\\hat{\\beta}_1 \\] ただし、\\(\\phi\\) は標準正規分布の確率密度関数であり、累積分布関数を微分することで得られる。このように、OLSのような線形モデルと異なり、\\(\\hat{\\beta}_1\\) が直接的に限界効果を示しているわけではないことが伺える17。 また同式より、ある変数の変化が反応確率へ与える限界効果は、個人が持つ全ての説明変数（\\(x_{1i},...,x_{ki}\\)）の値によって変化することもわかる。そのため、限界効果の報告においては「平均的な」限界効果を報告することが一般的である。ただし、「何の平均を取るか」という点において、2種類の計算方法が存在する。第1に、個人の個別限界効果を計算し、その平均値求めるという以下のような方法である。 \\[\\begin{equation} \\frac{1}{n}\\sum^n_{i=1}\\Big[\\phi(\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+...+\\hat{\\beta}_kx_{ki})\\hat{\\beta}_1 \\Big] \\tag{2} \\end{equation}\\] この計算方法は、因果推論における平均処置効果の議論に対応しているという好ましい性質を持っている（西山ほか, 2019）。 第2の方法は、説明変数の値について、それぞれ平均値を計算し、限界効果を求めるという以下のような方法である。 \\[\\begin{equation} \\phi(\\hat{\\beta_0}+\\hat{\\beta}_1\\bar{x}_1+...+\\hat{\\beta}_k\\bar{x}_k)\\hat{\\beta}_1 \\tag{3} \\end{equation}\\] これは、それぞれの説明変数について平均値を取るような「平均的個人」における限界効果として解釈可能な計算方法である。 着目する説明変数が上のデータ分析結果における a1 のように離散変数（ダミー変数）の場合、その限界効果についてこれまでのような偏微分の議論は使えない。そのため、ダミー変数が 0 の場合の反応確率と、1 の場合の反応確率の差を取る形で限界効果を捉える。例えば、クーポンを受け取っていない場合（\\(a_1=0\\)）と受け取った場合（\\(a_1=1\\)）の反応確率の差は以下の様に示すことができる。 \\[\\begin{equation} \\Phi(\\hat{\\beta}_0+\\hat{\\beta}_1p_{1i}+\\hat{\\beta}_2) - \\Phi(\\hat{\\beta}_0+\\hat{\\beta}_1p_{1i}) \\tag{4} \\end{equation}\\] この限界効果も、他の変数の値（例えば、\\(p_{1i}\\)）によって変化するため、連続変数の場合と同様に平均値を報告することが一般的である。 ロジットモデルの場合は、family = binomial(link = logit) で計算可能である。↩︎ ロジットモデルの場合は、mfx::logitmfx() で計算可能である。↩︎ この指標は、最尤推定で用いる対数尤度関数 (5) に実際の観測値と最尤推定値を代入して求まる対数尤度の和（\\(L_{full}\\)）と、定数項だけを含むプロビットモデルを推定したときの対数尤度の和（\\(L_0\\)）を用いて、\\(\\Big(1-(L_{full}/L_0) \\Big)\\) と定義される↩︎ \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta_1}x_{1i}+...+\\hat{\\beta}_kx_{ki}\\) という線形モデルの限界効果は、以下のようになる。 \\[\\frac{\\partial \\hat{y}_i}{\\partial x_{1i}}=\\hat{\\beta}_1 \\]↩︎ "],["ml.html", "補足：最尤推定法の紹介", " 補足：最尤推定法の紹介 最尤法では、あるデータが与えられたときに、そのような情報が得られる確率が最も高くなる（最も尤もらしくなる）ようにパラメータの値を求める方法である。この尤もらしさを尤度と呼び、得られたデータを尤度が最大になる（最もうまく説明できる）ようにパラメータを推定する。 一般的に、パラメータ \\(\\theta\\) を含む確率密度 \\(f(x,\\theta)\\) からの無作為標本\\({x_1,..., x_n}\\) を得た時、これに対する以下のような同時確率密度関数をパラメータ \\(\\theta\\) に関する尤度関数と見做す。 \\[ L(\\theta)=f(x_1,\\theta)\\times f(x_2,\\theta)\\times...\\times f(x_n,\\theta) \\] そして、これを最大にするように \\(\\theta\\) の推定値を求める方法が最尤法であり、ここで得る推定量を最尤推定量（Maximum Likelihood Estimator: MLE）と呼ぶ。 実際の分析においては、尤度関数の自然対数を取った以下のような対数尤度関数を用いる事が多い。 \\[ LL(\\theta)=\\sum^n_{i=1}\\ln f(x_i,\\theta) \\] 対数尤度と尤度を最大にする \\(\\theta\\) は数学的には等しく、対数尤度を用いたほうが、計算が容易であることから、対数尤度が用いられる。 前節で確認したプロビットモデル（(1)）の対数尤度関数は、以下のように求まる。 \\[\\begin{equation} LL_i(\\theta)=y_i\\Big(\\ln[\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})] \\Big)+ (1-y_i)\\Big(\\ln[1-\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})]\\Big) \\tag{5} \\end{equation}\\] 式 (5) は個人 \\(i\\) に関する対数尤度であるため、データ全体をうまく説明するパラメータを推定するためには、個別対数尤度の和（\\(\\sum^n_{i=1}LL_i\\)）を最大化するような係数パラメータ（\\(\\beta_0,\\beta_1,..., \\beta_k\\)）の推定値（\\(\\hat{\\beta}_0,\\hat{\\beta}_1,..., \\hat{\\beta}_k\\)）を計算する。 "],["Mcfadden.html", "離散選択モデルへの合理的消費者像の応用", " 離散選択モデルへの合理的消費者像の応用 離散選択モデルを用いた消費者需要の推定に関する先駆的な研究としてMcFadden (1974) による交通網に関する研究が挙げられる。この研究から続く離散選択モデルを用いた消費者需要の分析アプローチでは、顕示選好に関する理論的仮定を置いて分析を行っている。これらの議論では、首尾一貫性を満たす選択行動においては（実際に消費者が何を考えているかに関わらず）消費者が自身の効用が最大になる選択肢を選んでいるとみなせることを学んだ。これを活用し、「AとBという任意の2つの選択肢があり、ある個人がAを選んだ場合、その個人のAに対する効用はBに対する効用よりも高い」という、顕示選好に関する仮定に基づき考えることとする。 ここで、ある個人 \\(i\\) が選択肢 A と B の中から Aを選んだ、という状況を考える。選択肢 A の効用を \\(U_{iA}\\)、B の効用を \\(U_{iB}\\) とすると、\\(U_{iA}&gt;U_{iB}\\) という関係として表現することができる。消費者の離散選択モデルでは選択肢に関する観察可能な特徴（説明変数）と観察できない要素（誤差項）を使って、選択肢 \\(j\\) に対する効用を以下のように定式化する18。 \\[ U_{ij}=\\beta_{0}+\\beta_{1}x_{ij}+e_{ij} \\] このとき、\\(x_{ij}\\) は個人 \\(i\\)が直面する選択肢 \\(j\\)についての説明変数、\\(e_{ij}\\) は効用のランダム項を表している。例えば、\\(x_{ij}\\)が製品の価格である場合、\\(\\beta_{1}\\) は負の値を取ると考えられるが、それだけで消費者の効用を説明できず、確率的に変動するかもしれない。そのような確率的に変動する誤差を \\(e_{ij}\\) というランダム項で捉えていると考えられる。このように離散選択モデルは、顕示選好の仮定と観察可能な選択結果によって、データに含まれる個人の選好・効用について類推するアプローチである。 ここで改めて、ある個人 \\(i\\) が選択肢 A と B の中から Aを選んだという状況について考える。誤差項を含むモデルを用いると、\\(U_{iA}&gt;U_{iB}\\) という関係を以下のようなランダム項に関する式に変換することができる。 \\[ U_{iA}&gt;U_{iB}\\\\ \\beta_0+\\beta_1x_{iA}+e_{iA}&gt;\\beta_0+\\beta_1x_{iB}+e_{iB}\\\\ e_{iB}-e_{iA}&lt;(\\beta_0+\\beta_1x_{iA})-(\\beta_0+\\beta_1x_{iB})\\\\ e_{iB}-e_{iA}&lt;\\beta_1(x_{iA}-x_{iB}) \\] ここで、効用のランダム項の差（\\(e_{iB}-e_{iA}=e_i\\)）が標準正規分布に従うことを仮定すると、以下のように表すことができる (Adams, 2021)。 \\[\\begin{equation} P(y_i=A|x_{iA},x_{iB})=P(e_{iB}-e_{iA}&lt;\\beta_1(x_{iA}-x_{iB}))\\\\ = P(e_i&lt;\\beta_1(x_{iA}-x_{iB}))\\\\ =\\Phi(\\beta_1(x_{iA}-x_{iB})) \\tag{6} \\end{equation}\\] つまりこれは、\\(P(y_i=A|x_{iA},x_{iB})\\) という選択確率に関する回帰モデル\\(\\beta_1(x_{iA}-x_{iB})\\) を標準正規分布の累積分布関数（\\(\\Phi\\)）で表現したものであるとみなすことができる。 さらに、このモデルは選択肢のいずれかを選ぶ合理的行動を捉えているため、何も選ばない個人がいないとすれば、製品Bの選択確率は以下のように表現される。 \\[ P(y_i=B|x_{iA},x_{iB})= P(U_{iA}&lt;U_{iB})=1-\\Phi(\\beta_1(x_{iA}-x_{iB})) \\] このように、個人の選択結果を捉えた理論モデル（\\(U_{iA}&gt;U_{iB}\\)）と整合的な形でプロビットモデルを定式化できることが示された。ここまでの内容で学んだ理論的議論にプロビットモデルを応用することによって、本来観察できない個人の選択肢に対する効用を類推する方法が離散選択モデルという分析アプローチである。 なお、上記のモデルでは、定数項のパラメータが消される形で定式化されていた。しかしながら、選択肢ごとに異なるブランド価値があると想定するモデルを構築することで、定数項を含むモデルとして定式化することも可能である。具体的には、選択肢ごとに固有の定数項（\\(\\beta_{0j}\\)、ただし \\(j = A, B\\)）用いて、以下のような効用関数を想定する。 \\[ U_{ij}=\\beta_{0j}+\\beta_{1}x_{ij}+e_{ij} \\] これを、式 (6)と同様の定式化を行うことで、以下を得る。 \\[ \\begin{align} P(y_i=A|x_{iA},x_{iB})&amp;=P(e_{iB}-e_{iA}&lt;(\\beta_{0A}-\\beta_{0B})+\\beta_1(x_{iA}-x_{iB}))\\\\ &amp;= P(e_i&lt;\\tilde{\\beta}_{0}+\\beta_1(x_{iA}-x_{iB}))\\\\ &amp;=\\Phi(\\tilde{\\beta}_{0}+\\beta_1(x_{iA}-x_{iB})) \\end{align} \\] ただし、\\(\\tilde{\\beta}_0\\)は、\\(\\beta_{0A}-\\beta_{0B}\\) であり、選択肢間の定数項の差を表している。そのため、このような定式化によって分析されるモデルの定数項は、「選択肢固有の定数項の差」として推定される。これは、選択肢に対するマーケティング戦略変数（説明変数）以外の平均的な影響として理解できる。そのため、このような効用モデルの定数項を、その選択肢がそもそも持っている相対的価値として、製品の「ベースライン価値」や「ブランド価値」として解釈することがある（照井・佐藤, 2022）。通常は、どちらか一方の選択肢の定数項を基準にし（0とおく）、そこからの差を捉える形で分析する事が多い。 以下では、前節でも用いた choice_df を用いて上記の消費者の離散選択モデルを推定する。具体的には、製品1と2の価格差（p_ratio=p1-p2）と製品1のクーポン広告受取り有無（a1）と製品2のクーポン広告（a2）を用いて以下のようなモデルを分析する。 \\[ U_{1i}=\\tilde{\\beta}_{0}-\\beta_1(p_1-p_2)+\\beta_2a_1-\\beta_3a_2+e_{1i} \\] ただし、R上のコードでは、\\(a_2\\) の係数について、負に推定されることを想定しつつ、+ 記号を使って定式化する。 library(dplyr) #価格差変数作成 choice_df &lt;- choice_df %&gt;% mutate(p_ratio = p1 -p2) probit2 &lt;- glm(y1 ~ p_ratio + a1 + a2, family = binomial(link = probit), data = choice_df) summary(probit2) ## ## Call: ## glm(formula = y1 ~ p_ratio + a1 + a2, family = binomial(link = probit), ## data = choice_df) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.54634 0.07756 -7.044 1.87e-12 *** ## p_ratio -0.22180 0.01524 -14.559 &lt; 2e-16 *** ## a1 0.66314 0.09162 7.238 4.55e-13 *** ## a2 -0.32291 0.09884 -3.267 0.00109 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1384.5 on 999 degrees of freedom ## Residual deviance: 1066.9 on 996 degrees of freedom ## AIC: 1074.9 ## ## Number of Fisher Scoring iterations: 4 分析の結果、価格差については負に有意であり、p1 が相対的に高いほど選択確率が下がる傾向にあることを示唆している。また、自社のクーポン（a1）は正に、他社のクーポン（a2）は負に有意であることも伺えた。加えて、定数項（(Intercept)）は負に有意であるため、製品2を基準（製品2の定数項を 0 ）とした場合と比べ、製品 1 の定数項（ブランド価値）は低いと解釈する事ができる。本モデルの解釈を行うために、限界効果と擬似決定係数を以下のように分析する。 DescTools::PseudoR2(probit2) ## McFadden ## 0.229384 probit2_ame &lt;- probitmfx(y1 ~ p_ratio + a1 + a2, data = choice_df, atmean = FALSE) probit2_ame ## Call: ## probitmfx(formula = y1 ~ p_ratio + a1 + a2, data = choice_df, ## atmean = FALSE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## p_ratio -0.0669199 0.0029405 -22.7581 &lt; 2.2e-16 *** ## a1 0.2036848 0.0270475 7.5306 5.049e-14 *** ## a2 -0.0969891 0.0291848 -3.3233 0.0008897 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;a1&quot; &quot;a2&quot; 分析の結果、価格差が1単位（千円）大きくなると約 \\(6.7\\%\\) 製品1の選択確率が下がることが伺える。一方で、製品 1 のクーポンを受け取っている消費者はそうでない消費者に比べて約 \\(20\\%\\) 購買確率が高く、反対に他社クーポンは約 \\(9.7\\%\\) の購買確率低下につながる。 ただし、行列の記法を使えば、同じ議論を \\(x_{1ij},...,x_{kij}\\)の説明変数に応用できる。↩︎ "],["mlogit.html", "多項選択モデル紹介", " 多項選択モデル紹介 本章のこれまでの内容では、2 つの選択肢から 1 つを選ぶ 2 項選択モデルに着目していた。しかし、複数の選択肢から 1 つを選ぶという多項選択モデルにもこれまでの議論を拡張することができる。任意の \\(m\\) 個の選択肢の中からある選択肢 \\(j\\) を選ぶ場合、選択肢 \\(j\\) の効用は、\\(j\\) を含む 1 から \\(m\\) までの選択肢の中で最も大きいといえる。 詳細は省略するが、(1) でベルヌーイ試行に着目していたものをカテゴリカル分布に拡張し、選択確率についての確率密度関数を考えることで、多項選択モデルを得る。なお、計算の複雑性から、多項選択の分析では、多項ロジットモデルが用いられることが多い。多項離散選択モデルの実行においては、解釈やデータ準備においていくつか注意が必要である。 多項選択モデルの推定では、mlogit パッケージを用いる。そのため、install.packages(\"mlogit\") によってパッケージをインストールしてほしい。分析においては、?? 章でも紹介した、wide型とLong型のデータ構造に注意が必要である。ここで、 mlogit パッケージに含まれる Cracker データを用いて多項ロジットモデルを実行する。このデータは3293件のクラッカーの選択について扱ったデータである。このデータには以下の変数が含まれている。 id: 個人を特定する番号 choice: sunshine, keebler, nabisco, private,のうちどれを選んだか disp.z: ブランドz (各ブランド) が特別な陳列をされていたか feat.z: ブランドz (各ブランド) が新聞広告を掲載していたか price.z: ブランドz (各ブランド) の価格 library(mlogit) data(Cracker, package = &quot;mlogit&quot;) head(Cracker, n=20) ## id disp.sunshine disp.keebler disp.nabisco disp.private feat.sunshine ## 1 1 0 0 0 0 0 ## 2 1 0 0 0 0 0 ## 3 1 1 0 0 0 0 ## 4 1 0 0 0 0 0 ## 5 1 0 0 0 0 0 ## 6 1 0 0 0 0 0 ## 7 1 0 0 1 0 0 ## 8 1 0 0 1 0 0 ## 9 1 0 0 1 0 0 ## 10 1 1 0 1 0 0 ## 11 1 0 0 1 0 0 ## 12 1 0 0 0 0 0 ## 13 1 1 0 0 0 0 ## 14 1 0 1 1 0 0 ## 15 1 0 0 0 0 0 ## 16 1 0 0 1 0 0 ## 17 2 0 0 0 0 0 ## 18 2 1 0 1 0 1 ## 19 2 1 0 0 0 1 ## 20 2 1 0 0 0 0 ## feat.keebler feat.nabisco feat.private price.sunshine price.keebler ## 1 0 0 0 98 88 ## 2 0 0 0 99 109 ## 3 0 0 0 49 109 ## 4 0 0 0 103 109 ## 5 0 0 0 109 109 ## 6 0 0 0 89 109 ## 7 0 0 0 109 109 ## 8 0 0 0 109 119 ## 9 0 0 0 109 121 ## 10 0 0 0 79 121 ## 11 0 0 0 109 113 ## 12 0 0 0 109 121 ## 13 0 0 0 89 121 ## 14 0 0 0 109 109 ## 15 0 0 0 109 109 ## 16 0 0 0 129 104 ## 17 0 0 0 79 99 ## 18 0 0 0 69 105 ## 19 0 0 0 79 125 ## 20 0 0 0 79 125 ## price.nabisco price.private choice ## 1 120 71 nabisco ## 2 99 71 nabisco ## 3 109 78 sunshine ## 4 89 78 nabisco ## 5 119 64 nabisco ## 6 119 84 nabisco ## 7 129 78 sunshine ## 8 129 78 nabisco ## 9 109 78 nabisco ## 10 109 78 nabisco ## 11 109 96 nabisco ## 12 99 86 nabisco ## 13 99 86 nabisco ## 14 129 96 nabisco ## 15 129 79 nabisco ## 16 129 96 nabisco ## 17 69 69 nabisco ## 18 89 65 sunshine ## 19 106 69 sunshine ## 20 106 69 sunshine このデータ構造を、long型に変更する19。 cracker &lt;- mlogit.data(Cracker, choice = &quot;choice&quot;, shape = &quot;wide&quot;, varying=c(2:13)) head(cracker, n=20) ## ~~~~~~~ ## first 20 observations out of 13168 ## ~~~~~~~ ## id choice alt disp feat price chid idx ## 1 1 FALSE keebler 0 0 88 1 1:bler ## 2 1 TRUE nabisco 0 0 120 1 1:isco ## 3 1 FALSE private 0 0 71 1 1:vate ## 4 1 FALSE sunshine 0 0 98 1 1:hine ## 5 1 FALSE keebler 0 0 109 2 2:bler ## 6 1 TRUE nabisco 0 0 99 2 2:isco ## 7 1 FALSE private 0 0 71 2 2:vate ## 8 1 FALSE sunshine 0 0 99 2 2:hine ## 9 1 FALSE keebler 0 0 109 3 3:bler ## 10 1 FALSE nabisco 0 0 109 3 3:isco ## 11 1 FALSE private 0 0 78 3 3:vate ## 12 1 TRUE sunshine 1 0 49 3 3:hine ## 13 1 FALSE keebler 0 0 109 4 4:bler ## 14 1 TRUE nabisco 0 0 89 4 4:isco ## 15 1 FALSE private 0 0 78 4 4:vate ## 16 1 FALSE sunshine 0 0 103 4 4:hine ## 17 1 FALSE keebler 0 0 109 5 5:bler ## 18 1 TRUE nabisco 0 0 119 5 5:isco ## 19 1 FALSE private 0 0 64 5 5:vate ## 20 1 FALSE sunshine 0 0 109 5 5:hine ## ## ~~~ indexes ~~~~ ## chid alt ## 1 1 keebler ## 2 1 nabisco ## 3 1 private ## 4 1 sunshine ## 5 2 keebler ## 6 2 nabisco ## 7 2 private ## 8 2 sunshine ## 9 3 keebler ## 10 3 nabisco ## 11 3 private ## 12 3 sunshine ## 13 4 keebler ## 14 4 nabisco ## 15 4 private ## 16 4 sunshine ## 17 5 keebler ## 18 5 nabisco ## 19 5 private ## 20 5 sunshine ## indexes: 1, 2 変換後のデータセットでは、各選択肢の特徴と、それに対する個人の選択結果（TRUE or FALSE）を含める形で行が構成されている。このようにデータの構造を修正することで、分析を行う。 分析についても mlogitパッケージを利用して実行するのだが、分析結果の解釈を確かなものにするため、多項ロジットモデルについての説明を加える。多項ロジットモデルでは、\\(m\\) 個の選択肢の中からある選択肢 \\(j\\) を選ぶ行為を分析対象としている。分析モデルにおいては、説明変数 \\(x_{1i},...,x_{ki}\\) が与えられたときにある選択肢 \\(j\\) を選ぶ確率は 0 から 1 の間の値を取る必要がある。そのため、各選択肢に対する選択確率の合計が1になるように調整しなければならない。これに対して多項ロジットモデルでは、ある特定の選択肢（仮に選択肢 1 とする）を基準とし、その選択肢に対応する回帰係数パラメータ（\\(\\beta_{10},\\beta_{11},...,\\beta_{1k},\\)）をすべて 0 に固定する20。多項ロジットモデルは以下のように表すことができる。 \\[ \\begin{aligned} P(y_i=j|x_{1i},...,x_{ki})= \\left\\{ \\begin{array}{ll} \\frac{1}{1+\\sum^J_{j=2}[\\exp(\\beta_{j0}+\\beta_{j1}x_{1i}+...+\\beta_{jk}x_{ki})]} &amp; ~(y_i=1) \\\\ \\frac{\\exp(\\beta_{j0}+\\beta_{j1}x_{1i}+...+\\beta_{jk}x_{ki})}{1+\\sum^J_{j=2}[\\exp(\\beta_{j0}+\\beta_{j1}x_{1i}+...+\\beta_{jk}x_{ki})]} &amp; ~(y_i=2,..,m) \\end{array} \\right. \\end{aligned} \\] 多項ロジットの係数の解釈にも注意が必要である。多項ロジットモデルの係数（\\(\\beta\\)）は、対応する説明変数が微小に変化したときの、\\(y_i=j\\) と \\(y_i=1\\)（基準となる選択肢）との相対確率がどのように変化するかを示している。一方で限界効果は、説明変数が変化した時に \\(y_i=j\\) を選ぶ確率がどのように変化するかを示している。そのため、推定される係数そのものと限界効果との意味が大きく異なる点も、多項ロジットモデルの特徴である（西山ほか, 2019）。 また、多項ロジットモデルでは任意の 2 つの選択肢の相対的な選択確率は他の選択肢からは独立であるという、無関係な選択肢からの独立（independence from irrelevant alternatives: IIA）という仮定を置く。この仮定が満たされない場合には、データセット内の選択肢を統合（ひとまとめに）するなどして、IIAの仮定が適切だとみなせる状況を作るよう工夫が必要となる。 これらを踏まえ、多項ロジットモデルを実行する。分析においては mlogit::mlogit() を用いる。多項ロジットモデルの推定では、用いる変数に対する係数をどの細かさで分析するかを分析者が判断することが重要になる。例えば、選択肢ごとに異なる係数を推定すべきなのか、それとも各選択肢に共通で同一の係数が推定されるべきなのか、という点については分析者が指定することになる。Rコードでのモデルの定式化においては、以下の 3 つの変数カテゴリを区別する形で、変数と推定される係数の関係を記述する必要がある（ただし、\\(j\\) はある選択肢を示す）。 データ上選択肢固有の変数であり、複数の選択肢に共通の一般的な係数\\(\\beta\\)を得るための変数：\\(x_{ij}\\) データ上個人固有の変数であり、各選択肢ごとに異なる係数 \\(\\gamma_{j}\\) を得るための変数：\\(z_i\\) データ上選択肢固有の変数であり、各選択肢ごとに異なる係数 \\(\\delta_j\\)を得るための変数：\\(w_{ij}\\) これらの変数と係数のカテゴリを踏まえ、個人 \\(i\\)、選択肢 \\(j\\) に関する効用の確定項（モデルにおける確率的誤差項以外の部分） \\(V_{ij}\\)は以下のように示すことができる。 \\[ V_{ij}=\\alpha_j+\\beta x_{ij}+\\gamma_j z_i+\\delta_j w_{ij} \\] そのため、どの変数が選択肢ごとの係数を推定すべきなのかについて、分析者側が、既存の理論やドメイン知識（実務・社会的知識）を用いて決定する必要がある。例えば、価格と効用の関係についてはブランド間で統一だと仮定しても良さそうだが、特殊なマーケティング方策を行っている企業・ブランドがある場合、選択肢ごとに推定できるモデルも検討したほうが良いかもしれない。しかしながら、消費者一般に成立する価格への反応を推定したい場合には、ブランドごとに異なる価格反応を前提としたモデル化は適切ではないかもしれない。このように、研究目的や採用する理論、着目するデータの背景などに注意してモデルを構築する必要がある。 Rでのコード作成では、formula = choice ~ x|z|w という形でモデルを定義することができる。以下では、価格に対する消費者の反応はブランドごとに異なるものではないが、広告や陳列については各ブランドの具体的な戦術の特徴によって影響の程度が変わるかもしれないと考え、以下のように分析を行う（プロビットモデルを使うときには、probit=TRUEとする）。なお、このような選択肢に関する特徴を説明変数に含む多項ロジットモデルは、条件付きロジットモデル（Conditional logit model）と呼ばれることもある。 ml_cracker &lt;- mlogit(choice ~ price|1|disp + feat, probit = FALSE, data = cracker) summary(ml_cracker) ## ## Call: ## mlogit(formula = choice ~ price | 1 | disp + feat, data = cracker, ## probit = FALSE, method = &quot;nr&quot;) ## ## Frequencies of alternatives:choice ## keebler nabisco private sunshine ## 0.068651 0.544350 0.314399 0.072600 ## ## nr method ## 5 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 0.000258 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept):nabisco 2.0011226 0.0831336 24.0712 &lt; 2.2e-16 *** ## (Intercept):private 0.3193793 0.1238052 2.5797 0.0098888 ** ## (Intercept):sunshine -0.5435987 0.1139407 -4.7709 1.834e-06 *** ## price -0.0300966 0.0021082 -14.2761 &lt; 2.2e-16 *** ## disp:keebler 0.2999316 0.2070369 1.4487 0.1474251 ## disp:nabisco 0.1011111 0.0773633 1.3070 0.1912249 ## disp:private -0.2244555 0.1495236 -1.5011 0.1333199 ## disp:sunshine 0.4818670 0.1672400 2.8813 0.0039605 ** ## feat:keebler 0.6678542 0.2581732 2.5868 0.0096859 ** ## feat:nabisco 0.6047773 0.1404077 4.3073 1.653e-05 *** ## feat:private 0.1726981 0.2004446 0.8616 0.3889213 ## feat:sunshine 0.8304895 0.2340938 3.5477 0.0003886 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -3333.8 ## McFadden R^2: 0.052798 ## Likelihood ratio test : chisq = 371.66 (p.value = &lt; 2.22e-16) この分析モデルでは、priceはブランドごとに係数が推定されない変数（\\(x_{ij}\\)）、disp と feat はブランドごとに係数が異なる変数（\\(w_{ij}\\)）として定義している。そして、ブランドごとに係数を分析するが、その特徴自体は同個人に共通（同一のid内では各ブランドに共通の値が与えられる）である変数（\\(z_i\\)）には、1 という値が記入されている。これは、他の変数から独立して、各ブランドが持つ特徴である「定数項」として分析するための指示である。これにより、各説明変数から独立して、消費者が各選択肢に対してどのような選択確率を（相対的に）有しているのかを表している。なお、ここで出力されている選択肢ごとの係数の推定値は基準となる選択肢（keebler）との相対的な選択確率の変化を表していることに、改めて注意が必要である。 なお、mlogit.data() 関数の引数の設定については、元々のデータ構造によって変わるので、[Cranにあるソフトウェア](https://cran.r-project.org/package=mlogit）の説明などを確認し、応用することを進める。↩︎ ただし。\\(\\exp(0)=1\\) 。↩︎ "],["choiceAndPrediction.html", "需要の予測と本章のまとめ", " 需要の予測と本章のまとめ ここまでは、離散選択モデルを用いて消費者の選択に関わるパラメータを推定する方法について紹介してきた。これを用いることで、モデルに基づく各選択肢の選択確率を計算することが可能になる。例えば以下では、先ほど推定した2項選択モデルである probit2 を用いて、推定結果から選択確率の予測値を計算する。 #パラメータの推定値(beta)の抽出 beta &lt;- probit2$coefficients #パラメータ betaと説明変数の線形結合を作成（beta_0は定数項なので1をかける） est_probit2 &lt;- cbind(1, choice_df$p_ratio, choice_df$a1, choice_df$a2) %*% beta #pnormによって標準正規分布の分布関数による計算を実行する。 #製品1(c1) を選ぶ確率の予測値 pred_probit_c1 &lt;- pnorm(est_probit2) mean(pred_probit_c1) ## [1] 0.4789384 #ロジットモデルの場合 logit2 &lt;- glm(y1 ~ p_ratio + a1 + a2, family = binomial(link = logit), data = choice_df) beta_logit &lt;- logit2$coefficients est_logit2 &lt;- cbind(1, choice_df$p_ratio, choice_df$a1, choice_df$a2) %*% beta_logit pred_logit_c1 &lt;- (exp(est_logit2))/(1+exp(est_logit2)) mean(pred_logit_c1) ## [1] 0.479 probit2 モデルにおける製品1の選択確率は約 \\(47.9\\%\\) であり、製品2とほぼ半分ずつ分け合っているが、わずかに製品2の選択確率のほうが高いことが伺える。ここで、製品1の企業が値下げを行ったら、選択確率はどの様に変化するのだろうか。このような架空の状況について、今回の分析結果（パラメータの推定値）を用いて予測してみる。choice_df における p1 の平均はおよそ10（千円）である。仮に、製品2の価格を変えず、製品1の価格を平均価格の \\(10\\%\\) （1000円）値引き（\\(p1-1\\)）した場合の価格差を考える。 choice_df_v &lt;- choice_df %&gt;% mutate(p1_v = p1 - 1, p_ratio_v = p1_v - p2) est_probit_v &lt;- cbind(1, choice_df_v$p_ratio_v, choice_df_v$a1, choice_df_v$a2) %*% beta #pnormによって標準正規分布の分布関数による計算を実行する。 #製品1(c1) を選ぶ確率の予測値 pred_probit_v &lt;- pnorm(est_probit_v) mean(pred_probit_v) ## [1] 0.5459071 分析の結果、1000円の値下げによって企業1は製品の選択確率を約\\(54.6\\%\\)まで上昇することが示された。この値下げ幅によって生じる損失や費用と、上昇する選択確率による便益をどう捉えるのか、そしてどのような意思決定を行うのかは、企業の意思決定者に委ねられるべき問題である。しかしながら、既存の消費者行動データをきちんと蓄積、分析することで、実際に値下げを実行しなくても消費者の選択がどう変わりうるかを予測できることは実務的にも非常に有効な手段であるだろう。 離散選択モデルは、プロビットモデルやロジットモデルを実行するための関数を用いて分析が可能である。しかしながら、引数の設定や出力される結果の解釈、ひいてはモデルそのものに内包されている理論的仮定について最低限の理論的知識が必要になる。そのため、本章では理論に関する説明も含め離散選択モデルについての紹介を行った。 離散選択モデルには、ここで紹介した以外にも様々なタイプが存在する。たとえば、被説明変数が離散的であり大小関係を持つような変数（ランキング、アンケート尺度など）の場合、順序プロビット（Ordered probit）や順序ロジットモデルを用いることが多い。また、被説明変数が打ち切りデータである場合、トービットモデルを用いる。打ち切りデータとは、データの上限や下限があるようなデータであり、例えば株の保有額は保有していなければ 0（下限）で保有している際はその評価額をとるため、打ち切りデータだと考えられる。また、打ち切りデータであり、 0 の観測が著しく多い被説明変数の場合には、ポアソンモデルや負の二項分布モデルが用いられる。例えば、ある製品の購買頻度や購買量について（アンケートなどを用いて）広く消費者から情報を得る場合など、多くの消費者においては購買頻度が 0 であると考えられる。このようなデータを被説明変数とする場合にはポアソン分布や負の二項分布を想定したモデルを利用することが多い。紙幅の都合上これらの詳細は割愛するが、関心のある読者は計量経済学やマーケティング・サイエンスのテキストを参照してほしい。 また、本章の前半に紹介した通り、このアプローチは「自身の好みを理解し、首尾一貫した選択を行う」ような消費者像を前提としている。この前提は多くの状況に当てはまるものだと思われるが、そうではない状況もあるだろう。ここで想定している前提が崩れるような状況や個人的特性に着目する場合、異なる世界観（理論や学術領域）に基づく研究が必要になる。そのため、自身の捉えている問いや状況がどのようなものであるかを明確化し、それに整合的な理論と手法を選んで研究を進めることが重要になる。 "],["Ch7Reference.html", "参考文献", " 参考文献 神取道宏（2014）「ミクロ経済学の力」，日本評論社. デビッドクレプス（2008）「MBAのためのミクロ経済学入門I価格と市場 中泉真樹他訳」，東洋経済新報社. 照井信彦・佐藤忠彦（2022）「現代マーケティング・リサーチ 新版」, 有斐閣. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Adams, C., P. (2021) “Learning Microeconometrics with R,” CRC Press. Fernihough, A. (2019) “Marginal Effects for Generalized Linear Models: The mfx Package for R.” McFadden, D. (1974) “The Measurement of Urban Travel Demand,” Journal of Public Economics, 3, 303-328. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
